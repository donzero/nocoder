<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据处理]]></title>
    <url>%2F2018%2F12%2F01%2FML_normalize%2F</url>
    <content type="text"><![CDATA[归一化：一种是把数变为(0,1)之间的小数;一种是把有量纲表达式变为无量纲表达式。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。 标准化：标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1。 中心化：平均值为0，对标准差无要求 sklearn.preprocessing模块见：sklearn_preprocessing.md 几个区别与概念： 归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。 标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。 无量纲：通过某种方法能去掉实际过程中的单位，从而简化计算。 归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成’失效’，反而能提高数据的表现。比如梯度下降(GD)中求解最优化问题，归一化/标准化后可以加快梯度下降求解速度，提示模型收敛速度 区别(参考：https://www.zhihu.com/question/20467170)： 归一化：对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。 好处：提高迭代求解的收敛速度；提高迭代求解的精度。 归一化后会改变数据的原始距离、分布、信息 标准化：对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。 好处：使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上；不改变原始数据的分布； eg:1.7米 变换成 170厘米~！：标准化就是一种对样本数据在不同维度上进行一个伸缩变化（而不改变数据的几何距离），也就是不改变原始数据的信息（分布） 数据预处理归一化 名词网络上不统一(Rescaling)：x&#39; = (x - X_min) / (X_max - X_min) 均值归一化(Mean normalization)：x&#39; = (x - μ) / (MaxValue - MinValue) 新数据加入时导致max与min变化，需要重新计算 非线性归一化：对数函数转换y=log10(x)，反余弦函数转换y=atan(x)*2/π，包括log、指数、正切等 标准化 标准化(tandardization):x&#39; = (x - μ) / σ就是Z-score规范化(标准差标准化/零均值标准化) 中心化 x&#39; = X - μ 使用参考 如果对输出结果范围有要求，用归一化 如果数据较为稳定，不存在极端的最大最小值，用归一化。 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。 必须归一化/标准化：SVM,KNN,NN(神经网络) 参考https://www.jianshu.com/p/95a8f035c86c]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model.LogisticRegression API]]></title>
    <url>%2F2018%2F12%2F01%2Fsklearn_LogisticRegression%2F</url>
    <content type="text"><![CDATA[sklearn.linear_model.LogisticRegression API逻辑回归分类器(sklearn.linear_model.LogisticRegression)不是回归类算法，算法名称叫逻辑回归LogisticRegression(penalty=&#39;l2&#39;, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=&#39;liblinear&#39;, max_iter=100, multi_class=&#39;warn&#39;, verbose=0, warm_start=False, n_jobs=None) penalty：惩罚项，str类型(&#39;l1&#39;,&#39;l2&#39;)。用于指定惩罚项中使用的规范,L1规范假设的是模型的参数满足拉普拉斯分布，L2假设的模型参数满足高斯分布，所谓的范式就是加上对参数的约束，使得模型更不会过拟合(overfit) dual：对偶或原始方法，bool类型。对偶方法只用在求解线性多核(liblinear)的L2惩罚项上。当样本数量&gt;样本特征的时候，dual通常设置为False。 tol：停止求解的标准，float类型。求解到多少的时候，停止，认为已经求出最优解。 C：正则化系数λ的倒数，float类型。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化 fit_intercept：是否存在截距或偏差，bool类型。 intercept_scaling：仅在正则化项为liblinear且fit_intercept=True时有用。float类型 class_weight：用于标示分类模型中各种类型的权重，可以是一个字典或者balanced字符串，默认为不输入，也就是不考虑权重 ‘balanced’让类库自己计算类型权重，或者自己输入各个类型的权重。举个例子，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9,1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。当class_weight为’balanced’时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))。n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]。 solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。solver参数决定了我们对逻辑回归损失函数的优化方 liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。适用于小数据集 lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。只能用L2 newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。只能用L2、适合于样本数据多 sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度。只能用L2、适合于样本数据多 saga：线性收敛的随机优化算法的的变重。 max_iter：算法收敛最大迭代次数，int类型。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数 multi_class：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。ovr即前面提到的one-vs-rest(OvR)，而multinomial即前面提到的many-vs-many(MvM) OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。多数情况下分类效果相对略差，速度快 MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类。分类相对精确，速度慢，且只能选择newton-cg、lbfgs、sag verbose：日志冗长度，int类型。默认为0。就是不输出训练过程，1的时候偶尔输出结果，大于1，对于每个子模型都输出 warm_start：热启动参数，bool类型。默认为False。如果为True，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化） n_jobs：并行数。int类型。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序 实例化后LogisticRegression常用的属性与方法： coef_：训练后的输入端模型系数，如果label有两个，即y值有两列。那么是一个2D的array intercept_：截距 n_iter_：所有类的实际迭代次数 get_params()：获取LogisticRegression所有参数设置值 densify()：将系数矩阵转化成密集矩阵的格式 sparsify()：将系数矩阵转换成稀疏矩阵格式 fit(X, y, sample_weight=None)：训练模型 X：训练集 y：训练集对应的标签集 sample_weight：权重array predict(X)：预测数据的结果 predict_log_proba(X)：对数概率估计预测 predict_proba(X)：概率估计预测 score(X, y, sample_weight=None):：评估 总结 优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低。 缺点：容易欠拟合，分类精度可能不高。 其他： Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。 改进的一些最优化算法，比如sag。它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批量处理。 机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。 我们需要根据数据的情况，这是Sklearn的参数，以期达到更好的分类效果。 参考https://blog.csdn.net/jark_/article/details/78342644https://blog.csdn.net/weixin_39541558/article/details/80613457]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.preprocessing API]]></title>
    <url>%2F2018%2F12%2F01%2Fsklearn_preprocessing%2F</url>
    <content type="text"><![CDATA[sklearn.preprocessing主要包括14大类，为训练集数据的预处理提供接口，每个类都提供了fit（填充数据，获取数据上的特征信息并保存），transform（将fit保存的信息应用到其它数据集上，对其它数据集进行转换），fit_transform（填充数据并对数据进行转换）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161# -*- coding: utf-8 -*-"""Created on Sun Nov 25 11:01:53 2018@author: nocoder"""#%%import numpy as np"""sklearn.preprocessing主要包括14大类，为训练集数据的预处理提供接口，每个类都提供了fit（填充数据，获取数据上的特征信息并保存），transform（将fit保存的信息应用到其它数据集上，对其它数据集进行转换），fit_transform（填充数据并对数据进行转换）"""X = np.array([[1,2,3,4],[2,5,6,7]], dtype=np.float32)#%%'''Standardization标准化:将特征数据的分布调整成标准正太分布，也叫高斯分布，也就是使得数据的均值维0，方差为1.标准化的原因在于如果有些特征的方差过大，则会主导目标函数从而使参数估计器无法正确地去学习其他特征。标准化的过程为两步：去均值的中心化（均值变为0）；方差的规模化（方差变为1）。'''from sklearn.preprocessing import scale'''scale(X, axis=0, with_mean=True, with_std=True, copy=True)'''X_scale_ax0 = scale(X, axis=0, with_mean=True, with_std=True, copy=True)print('scale列标准化后数据:\n',X_scale_ax0)print('scale列标准化后列均值:',X_scale_ax0.mean(axis=0))print('scale列标准化后列标准差:',X_scale_ax0.std(axis=0))print('scale列标准化后行均值:',X_scale_ax0.mean(axis=1))print('scale列标准化后行标准差:',X_scale_ax0.std(axis=1))print('scale列标准化后均值:',X_scale_ax0.mean())print('scale列标准化后标准差:',X_scale_ax0.std())X_scale_ax1 = scale(X, axis=1, with_mean=True, with_std=True, copy=True)print('scale行标准化后数据:\n',X_scale_ax1)print('scale行标准化后均值:\n',X_scale_ax1.mean())print('scale行标准化后标准差:\n',X_scale_ax1.std())'''scale列标准化后数据: [[-1. -1. -1. -1.] [ 1. 1. 1. 1.]]scale列标准化后列均值: [0. 0. 0. 0.]scale列标准化后列标准差: [1. 1. 1. 1.]scale列标准化后行均值: [-1. 1.]scale列标准化后行标准差: [0. 0.]scale列标准化后均值: 0.0scale列标准化后标准差: 1.0scale行标准化后数据: [[-1.3416407e+00 -4.4721359e-01 4.4721359e-01 1.3416407e+00] [-1.6035674e+00 -2.9802322e-08 5.3452241e-01 1.0690449e+00]]scale行标准化后均值: 0.0scale行标准化后标准差: 0.99999994'''#%%from sklearn.preprocessing import StandardScaler'''StandardScaler(copy=True, with_mean=True, with_std=True)'''standarScaler = StandardScaler(with_mean=True, with_std=True)X_standarScaler = standarScaler.fit_transform(X)print('StandardScaler标准化后数据:\n',X_standarScaler)print('StandardScaler标准化后均值:',X_standarScaler.mean())'''StandardScaler标准化后数据: [[-1. -1. -1. -1.] [ 1. 1. 1. 1.]]StandardScaler标准化后均值: 0.0StandardScaler标准化后标准差: 1.0'''#%%from sklearn.preprocessing import Normalizer'''归一化正则化Normalizer(norm='l2', copy=True)'''# X行/X行的l1或l2？？l1_normzer = Normalizer(norm='l1')X_l1 = l1_normzer.fit_transform(X)print("Normalizer('l1'):\n", X_l1)print("Normalizer('l1')均值:", X_l1.mean())print("Normalizer('l1')标准差:", X_l1.std())'''# x_i/||xi||_1Normalizer('l1'): [[0.1 0.2 0.3 0.4 ] [0.1 0.25 0.3 0.35]]Normalizer('l1')均值: 0.25Normalizer('l1')标准差: 0.10307764'''l2_normzer = Normalizer(norm='l2')X_l2 = l2_normzer.fit_transform(X)print("Normalizer('l2'):\n", X_l2)print("Normalizer('l2')均值:", X_l2.mean())print("Normalizer('l2')标准差:", X_l2.std())'''# x_i/||xi||_2Normalizer('l2'): [[0.18257418 0.36514837 0.5477225 0.73029673] [0.18731716 0.4682929 0.56195146 0.6556101 ]]Normalizer('l2')均值: 0.46236417Normalizer('l2')标准差: 0.19031385'''#%%from sklearn.preprocessing import MinMaxScaler'''(x-列最小值)/(列最大值-列最小值),value在0-1:MinMaxScaler(feature_range=(0, 1), copy=True)'''minMaxScaler = MinMaxScaler()X_minmax = minMaxScaler.fit_transform(X)print('MinMaxScaler区间缩放[0,1]后数据：\n', X_minmax)'''MinMaxScaler区间缩放[0,1]后数据： [[0. 0. 0. 0. ] [1. 1. 1. 1.0000001]]'''from sklearn.preprocessing import MaxAbsScaler'''x / 列最大值的绝对值,value在[-1,1],(稀疏矩阵推荐使用):MaxAbsScaler(copy=True)'''maxAbsScaler = MaxAbsScaler()X_maxabs = maxAbsScaler.fit_transform(X)print('MaxAbsScaler最大值绝对值后数据：\n', X_maxabs)'''X_maxabs最大值绝对值后数据： [[0.5 0.4 0.5 0.5714286] [1. 1. 1. 1. ]]'''#%%from sklearn.preprocessing import Binarizer'''特征二值化:Binarizer(threshold=0.0, copy=True)'''binarizer = Binarizer(threshold=2)X_binarizer = binarizer.fit_transform(X)print('Binarizer特征二值化后数据：\n', X_binarizer)'''Binarizer特征二值化后数据： [[0. 0. 1. 1.] [0. 1. 1. 1.]]'''#%%from sklearn.preprocessing import OneHotEncoder'''onehot编码:OneHotEncoder(n_values=None, categorical_features=None, categories=None, sparse=True, dtype=np.float64, handle_unknown='error')'''oneHotEncoder = OneHotEncoder()X_oneHotEncoder = oneHotEncoder.fit_transform(X)print('OneHotEncoder onehot编码后数据：\n', X_oneHotEncoder.toarray())'''OneHotEncoder onehot编码后数据： [[1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 0. 1. 0. 1. 0. 1.]]'''#%%from sklearn.preprocessing import Imputer'''缺失值处理:Imputer(missing_values="NaN", strategy="mean", axis=0, verbose=0, copy=True)'''X = [[1,2,3],[np.nan,2,4]]imputer = Imputer(strategy='mean',axis=1)X_impute = imputer.fit_transform(X)print('Imputer缺失值处理，行均值后数据：\n', X_impute)'''Imputer缺失值处理，行均值后数据： [[1. 2. 3.] [3. 2. 4.]]''' 参考https://blog.csdn.net/sinat_33761963/article/details/53433799http://blog.sina.com.cn/s/blog_73ff91640102xdl4.html]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model.SGDClassifier API]]></title>
    <url>%2F2018%2F12%2F01%2Fsklearn_SGDClassifier%2F</url>
    <content type="text"><![CDATA[一般来说，逻辑回归用梯度下降算法来求解参数比较常见，所以这也导致一开始误以为LogisticRegression模型就是用梯度下降算法来实现的，当遇到SGDClassifier(Stochastic Gradient Descent)随机梯度下降分类器的时候，就有点蒙了。梯度下降明明是一个求解算法，怎么就和分类器扯上关系了。原来SGDClassifier是一系列采用了梯度下降来求解参数的算法的集合，例如SVM, logistic regression等。 而sklearn中，LogisticRegression的实现方法是基于liblinear、newton-cg、lbfgs、sag这些库来实现的，当数据集特别大的时候，推荐使用SGDClassifier中的逻辑回归。 SGDClassifierSGDClassifier是一个用随机梯度下降算法训练的线性分类器的集合。默认情况下是一个线性(软间隔)支持向量机分类器 疑惑：大多数SVM的求解不都是用的SMO算法么？怎么这儿又跑来一个SGD算法。原因是因为支持向量机的另一个解释就是最小化合页损失函数，因此，该损失函数同样可以通过梯度下降算法来求解参数。 SGDClassifier(loss=&quot;hinge&quot;, penalty=&#39;l2&#39;, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, n_jobs=None, random_state=None, learning_rate=&quot;optimal&quot;, eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False, n_iter=None) loss：损失函数选择项，字符串型；默认为hinge即SVM；log为逻辑回归。更多见API描述 penalty：惩罚方式,字符串型；默认为l2;其余有none,l1,elasticnet：l1和elasticnet可能会给l2无法实现的模型(特征选择)带来稀疏性 alpha：常数乘以正则化项。缺省值为0.0001也用于在设置为最优时计算学习率。 l1_ratio：弹性网络混合参数，0 &lt;= l1_ratio &lt;= 1.1,l1_ratio = 0对应于L2惩罚，l1_ratio = 1对应于L1。 默认为0.15。 fit_intercept：截距开关 max_iter：训练数据(也称为epochs)的最大次数。 它只会影响fit方法中的行为，而不会影响局部拟合(partial_fit)。 默认为5，如果不是None的话为0.21-1000 tol：停止标准。 如果它不是None，则迭代将在(当前loss&gt; 上一个previous_loss - tol)时停止。 默认为无。 V0.21默认为1e-3。 shuffle：洗牌。数据训练前打乱排序 verbose：没用过？int类型，冗长的水平 epsilon：没用过？Epsilon对epsilon不敏感的损失函数; 只有当损失是huber,epsilon_insensitive或squared_epsilon_insensitive时。 对于huber，确定使得预测完全正确变得不那么重要的阈值。 对于epsilon不敏感，如果它们小于此阈值，则忽略当前预测和正确标签之间的任何差异。 n_jobs：并行数。int类型。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序 random_state：机数的种子。在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的，但填0或不填，每次都会不一样。 learning_rate：学习速率，字符串型；默认值为optimal，根据alpha计算得到。 constant：eta = eta0 optimal：eta = 1.0 / (alpha * (t + t0)) invscaling：eta = eta0 / pow(t, power_t) adaptive：自适应？ eta0：constant，invscaling或adaptive的初始学习率。 默认值为0.0，因为默认计划optimal未使用eta0。 power_t：反比例缩放学习率的指数，默认为0.5。 early_stopping：当验证得分在参数n_iter_no_change期间没有提升时，提前终止训练。 validation_fraction：将训练数据的比例留作早期停止的验证集。 必须介于0和1之间。仅在early_stopping=True时使用。 n_iter_no_change：在提前停止之前等待没有改进的迭代次数。 class_weight：用于标示分类模型中各种类型的权重，可以是一个字典或者balanced字符串，默认为不输入，也就是不考虑权重 warm_start：没用过？设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。 请参阅词汇表。 当warm_start=True时，反复调用fit或partial_fit可能会导致与调用fit一次时不同的解决方案，因为数据被洗牌的方式。 如果使用动态学习速率，则根据已经看到的样本数量来调整学习速率。 调用适合重置此计数器，而partial_fit将导致增加现有计数器 average：没用过？设置为True时，计算平均SGD权重并将结果存储在coef_属性中。 如果设置为大于1的int，则一旦看到的样本总数达到平均值，就会开始平均。 因此，在看到10个样本后，平均值=10将开始平均。 n_iter：迭代次数，不推荐使用，V0.21中将删除 实例化后SGDClassifier常用的属性与方法：详见sklearn.linear_model.LogisticRegression API 参考https://blog.csdn.net/The_lastest/article/details/79100463]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2018%2F11%2F25%2FML_Ensemble%2F</url>
    <content type="text"><![CDATA[集成学习(Ensemble Learning)：顾名思义，通过将多个单个学习器集成/组合在一起，使它们共同完成学习任务。 也被称为多分类器系统(multi-classifier system)、基于委员会的学习(Committee-based learning)。 个体学习器通常是用一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。此时集成中只包含同种类型的个体学习器，例如决策树集成中的个体学习器全是决策树，神经网络集成中就全是神经网络，这样的集成是同质(homogeneous)的，同质集成中的个体学习器也称为基学习器(base learner)，相应的学习算法称为基学习算法(base learning algorithm。有同质就有异质(heterogeneous)，若集成包含不同类型的个体学习器，例如同时包含决策树和神经网络，那么这时个体学习器一般不称为基学习器，而称作组件学习器(component leaner)或直接称为个体学习器。 要获得好的集成，个体学习器应好而不同，即个体学习器要有一定的准确性，即学习器不能太坏，并且要有多样性(diversity)，即学习器间具有差异 目前集成学习方法大致可分为两大类: Boosting：个体学习器间存在强依赖关系、必须串行生成的序列化方法 Bagging和随机森林(Random Forest)：个体学习器间不存在强依赖关系、可同时生成的并行化方法 BoostingBoosting是一族可将弱学习器提升为强学习器的算法。 先从初始训练集训练出一个基学习器 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注 然后基于调整后的样本分布来训练下一个基学习器 如此重复进行，直至基学习器数目达到事先指定的值T 最终将这T个基学习器进行加权结合 AdaBoost。 Boosting算法要求基学习器对特定的数据分布进行学习，这一点是通过重赋权法(re-weighting)实现的，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重，对无法接受代全样本的基学习算法，则可通过重采样法(re-sampling)来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得到的样本集对基学习器进行训练。 从偏差/方差分解的角度看，Boosting主要关注降低偏差(避免欠拟合) Bagging与随机森林BaggingBagging基于前面提到过的自助采样法(bootstrap sampling)。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m此随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。于是，我们可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再集成，这就是Bagging的基本流程 随机森林随机森林(Random Forest，简称RF)是Bagging的一个扩展变体。其在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。 集成学习简单描述Bagging(套袋法)过程： 从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集(k个训练集之间相互独立，元素可以有重复) 对于k个训练集，我们训练k个模型(这k个模型可以根据具体问题而定，比如决策树，knn等) 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果(所有模型的重要性相同) Boosting(提升法)过程： 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。 进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型(例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大) Bagging与Boosting的主要区别: 样本选择上 Bagging采用的是Bootstrap随机有放回抽样 Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重 样本权重 Bagging使用的是均匀取样，每个样本权重相等 Boosting根据错误率调整样本权重，错误率越大的样本权重越大 预测函数 Bagging所有的预测函数的权重相等 Boosting中误差越小的预测函数其权重越大 并行计算 Bagging各个预测函数可以并行生成 Boosting各个预测函数必须按顺序迭代生成 下面是将决策树与这些算法框架进行结合所得到的新的算法： Bagging + 决策树 = 随机森林 AdaBoost + 决策树 = 提升树 Gradient Boosting + 决策树 = GBDT 决策树常用的决策树算法有ID3，C4.5，CART三种 ID3，C4.5决策树的生成:输入：训练集D，特征集A，阈值eps –&gt; 输出：决策树T 若D中所有样本属于同一类Ck，则T为单节点树，将类Ck作为该结点的类标记，返回T 若A为空集，即没有特征作为划分依据，则T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T 否则，计算A中各特征对D的信息增益(ID3)/信息增益比(C4.5)，选择信息增益最大的特征Ag 若Ag的信息增益（比）小于阈值eps，则置T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T 否则，依照特征Ag将D划分为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T 对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归地调用1~5，得到子树Ti，返回Ti CART决策树的生成，与ID3，C4.5区别: CART树是二叉树，而ID3和C4.5可以是多叉树 CART在生成子树时，是选择一个特征一个取值作为切分点，生成两个子树 选择特征和切分点的依据是基尼指数，选择基尼指数最小的特征及切分点生成子树 决策树的剪枝决策树的剪枝主要是为了预防过拟合，过程就不详细介绍了。 主要思路是从叶节点向上回溯，尝试对某个节点进行剪枝，比较剪枝前后的决策树的损失函数值。最后我们通过动态规划（树形dp，acmer应该懂）就可以得到全局最优的剪枝方案。 参考https://blog.csdn.net/qq_32690999/article/details/78759463https://blog.csdn.net/qq547276542/article/details/78304454]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林算法]]></title>
    <url>%2F2018%2F11%2F25%2FML_RandomForest%2F</url>
    <content type="text"><![CDATA[随机森林(RandomForest)属于集成学习(Ensemble Learning)中的bagging算法，可以用来做分类、回归等问题 优点： 具有极高的准确率 随机性的引入，使得随机森林不容易过拟合 随机性的引入，使得随机森林有很好的抗噪声能力 能处理很高维度的数据，并且不用做特征选择 既能处理离散型数据，也能处理连续型数据，数据集无需规范化 训练速度快，可以得到变量重要性排序 容易实现并行化 缺点： 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大 随机森林模型还有许多不好解释的地方，有点算个黑盒模型 过程： 从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集 对于n_tree个训练集，我们分别训练n_tree个决策树模型 对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂 每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝 将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F11%2F24%2FML_Normalization%2F</url>
    <content type="text"><![CDATA[过于追求准确率，容易导致过拟合问题，使用正则化技术可以在一定程度上防止过拟合(1.减少特征数量;2.使用正则化) 正则化是结构风险最小化策略的实现，在经验风险上加一个正则项或罚项。 奥卡姆剃刀(Occam’s razor)原理：如无必要，勿增实体。 即’简单有效原理’ 概念机器学习中正则化或范式需添加λ超参数(通常会取一个较大的数，有一个恰当的λ使得模型处于既不过拟合又不欠拟合的状态)。 F0L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说，就是让参数W是稀疏的。 F1 λ ||w||_1L1范数是指向量中各个元素绝对值之和，也有个美称叫’稀疏规则算子(Lasso regularization)’。 L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 F2 λ/2 ||w||_2L2范数也被称为权重衰减(weight decay)。 为了最小化整个代价函数，λ，那么就要减小权重的值。这就是使得权重衰减。让L2范数的正则项||W||_2最小，可以使得W的每个元素都很小，都接近于0。(L1范数让W等于0),而越小的参数说明模型越简单，越简单的模型越不容易产生过拟合的现象 一般λ要乘以1/2，为了后向传播求导。 L1与L2 下降速度：在代价函数中，最小化的L1与L2的坡度不一致，L1按绝对值下坡，L2按二次函数下坡 L1范数：L1范数在正则化的过程中会趋向于产生少量的特征，而其他的特征都是0（L1会使得参数矩阵变得稀疏）。因此L1不仅可以起到正则化的作用，还可以起到特征选择的作用。 L2范数：L2范数是通过使权重衰减，进而使得特征对于总体的影响减小而起到防止过拟合的作用的。L2的优点在于求解稳定、快速。 稀疏好处 特征选择，稀疏的矩阵可以起到特征选择的作用。 模型更容易解释]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉验证Cross-validation]]></title>
    <url>%2F2018%2F11%2F23%2FML_cross%2F</url>
    <content type="text"><![CDATA[百度百科思想引用： 在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。 目的：客观的判断模型(参数)对训练集之外(未知)的数据的符合程度(泛化能力)(表现效果)，避免过拟合。 样本准确度会依赖不同的测试集，其表现效果不尽相同。 交叉验证形式 留出验证(Holdout) K折交叉验证(k-fold) 留一验证(Leave one out,LOOCV):只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于和K-fold交叉验证是一样的，其中K为原本样本个数。 留出验证一种简单交叉验证，将数据集随机分成训练集，其他作为测试集。数据都只被所用了一次，没有被充分利用 train_test_split(*arrays, test_size=0.25, train_size=None, random_state=None, shuffle=True, stratify=None) *arrays：多个数据集，一般传入X,Y列表。 test_size:测试集尺寸。如果是浮点数，在0-1之间，表示测试集占比；如果是整数的话就是测试集的数量。当train_size=None时默认值为0.25，否则作为训练集补码 train_size：训练集尺寸。同test_size不过为训练集尺寸。当train_size=None时自动设置为test_size的补码 random_state：随机数的种子。在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的，但填0或不填，每次都会不一样。 shuffle：洗牌。是否洗牌数据后再进行分割。如果shuffle=False，那么分层一定是None stratify：分层。如果不是None，则以分层的方式分割数据，使用它作为类标签。 返回：返回切割后的list，返回个数length = 2 * len(arrays) 12345678910111213import numpy as npfrom sklearn.model_selection import train_test_splitX = np.array(np.arange(10))Y = np.array(np.arange(10)) + 10X_train,X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)print('训练集(x,y):', X_train, Y_train)print('测试集(x,y)', X_test, Y_test)'''训练集(x,y): [1 2 9 6 7 0 3 8] [11 12 19 16 17 10 13 18]测试集(x,y) [5 4] [15 14]''' K折交叉验证步骤： 将数据集平均分割成K个等份 使用1份数据作为测试集，其余作为训练集用于模型训练 模型训练后，相应测试集测试并保存模型评估指标 使用不同的测试集，重复2、3步骤 将K组的评估指标计算平均值为模型精度估计，当作对未知数据预测准确率的估计 KFoldKFold(n_splits=3, shuffle=False, random_state=None) n_splits：K折验证的K值。至少大于2，V0.22中更改为5 shuffle：洗牌。是否洗牌数据后再进行分割。 random_state：随机数的种子。 方法： get_n_splits(X=None, y=None, groups=None):返回交叉验证器中拆分迭代的次数 参数总忽略 split(X, y=None, groups=None):返回生成索引以将数据分割为训练集和测试集。 X：训练集 y：目标集 groups：没用过，类似于数组，具有形状(n_samples,)，可选的将数据集拆分为训练/测试集时使用的样本的组标签。 12345678910111213141516171819202122232425262728import numpy as npfrom sklearn.model_selection import KFold,StratifiedKFoldX = np.array(np.arange(10))Y = np.array(np.arange(10)) + 10# 5折，不洗牌,洗牌后分布式随机的kf = KFold(5, shuffle=False, random_state=None)print(kf.get_n_splits())#5k_i = 0#当前轮数for train_idx, test_idx in kf.split(X, Y): k_i += 1 print('第%d轮:' % (k_i)) X_train,X_test,Y_train,Y_test = [X[idx] for idx in train_idx],[X[idx] for idx in test_idx],[Y[idx] for idx in train_idx],[Y[idx] for idx in test_idx] print('训练集(x,y):', X_train, Y_train) print('测试集(x,y)', X_test, Y_test)'''第1轮:训练集(x,y): [2, 3, 4, 5, 6, 7, 8, 9] [12, 13, 14, 15, 16, 17, 18, 19]测试集(x,y) [0, 1] [10, 11]第2轮:训练集(x,y): [0, 1, 4, 5, 6, 7, 8, 9] [10, 11, 14, 15, 16, 17, 18, 19]测试集(x,y) [2, 3] [12, 13]...第5轮:训练集(x,y): [0, 1, 2, 3, 4, 5, 6, 7] [10, 11, 12, 13, 14, 15, 16, 17]测试集(x,y) [8, 9] [18, 19]''' StratifiedKFold同KFold使用(测试需要更多数据，不然不好分层)。分层采样交叉切分，确保训练集，测试集中各类别样本的比例与原始数据集中相同。 留一验证(等同于K折交叉验证中K=整个数据集个数)只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料 1234567891011121314151617181920212223242526import numpy as npfrom sklearn.model_selection import LeaveOneOutX = np.array(np.arange(10))Y = np.array(np.arange(10)) + 10loo = LeaveOneOut()k_i = 0#当前轮数，总轮数等于样本个数len(X)for train_idx, test_idx in loo.split(X, Y): k_i += 1 print('第%d轮:' % (k_i)) X_train,X_test,Y_train,Y_test = [X[idx] for idx in train_idx],[X[idx] for idx in test_idx],[Y[idx] for idx in train_idx],[Y[idx] for idx in test_idx] print('训练集(x,y):', X_train, Y_train) print('测试集(x,y)', X_test, Y_test)'''第1轮:训练集(x,y): [1, 2, 3, 4, 5, 6, 7, 8, 9] [11, 12, 13, 14, 15, 16, 17, 18, 19]测试集(x,y) [0] [10]第2轮:训练集(x,y): [0, 2, 3, 4, 5, 6, 7, 8, 9] [10, 12, 13, 14, 15, 16, 17, 18, 19]测试集(x,y) [1] [11]...第10轮:训练集(x,y): [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 11, 12, 13, 14, 15, 16, 17, 18]测试集(x,y) [9] [19]''' cross_val_scorecross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=3, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=&#39;2*n_jobs&#39;, error_score=&#39;raise-deprecating&#39;) estimator：估算器对象实现’fit’用于拟合数据的对象，一般为sklearn的算法，例如：sklearn.linear_model.LogisticRegression X：训练集 Y：目标集 groups：类似于数组，具有形状(n_samples,)，可选的将数据集拆分为训练/测试集时使用的样本的组标签。 scoring：准确率的算法，可以通过score_func参数指定；如果不指定的话，是用estimator默认自带的准确率算法。有:&#39;accuracy&#39;,&#39;average_precision&#39;,&#39;f1&#39;,&#39;f1_micro&#39;,&#39;f1_weighted&#39;,&#39;f1_samples&#39;,&#39;roc_auc&#39;.. cv：代表不同的交叉验证的方法。默认3折交叉分类器(0.22更改为5折)，如果cv是一个int值，指定（分层）KFold中的折叠数。对于整数/无输入，如果估计器是分类器并且y是二进制或多类，则使用StratifiedKFold。 在所有其他情况下，使用KFold。 n_jobs：用于执行计算的CPU数。 除非在joblib.parallel_backend上下文中，否则表示1。 -1表示使用所有处理器。 verbose：没用过?冗长的水平 fit_params：没用过?参数传递到估计器的拟合方法。 pre_dispatch：没用过?控制在并行执行期间调度的作业数。 减少此数量可有助于避免在分配的作业多于CPU可处理的内容时消耗内存消耗。 此参数可以是：None，在这种情况下，将立即创建和生成所有作业。 将此用于轻量级和快速运行的作业，以避免因按需生成作业而导致的延迟一个int，给出生成的总作业的确切数量一个字符串，给出一个表达式作为n_jobs的函数，如’ 2 * n_jobs’ error_score：没用过?如果估算器拟合中发生错误，则分配给分数的值。 如果设置为“raise”，则会引发错误。 如果设置为’raise-deprecating’，则会在引发错误之前打印FutureWarning。 如果给出了数值，则引发FitFailedWarning。 此参数不会影响重新启动步骤，这将始终引发错误。 默认为’raise-deprecating’，但从版本0.22开始，它将更改为np.nan。 代码参考：https://blog.csdn.net/qq_36523839/article/details/8070767812345678910111213141516171819202122232425from sklearn import datasets #自带数据集from sklearn.model_selection import train_test_split,cross_val_score #划分数据 交叉验证from sklearn.neighbors import KNeighborsClassifier #一个简单的模型，只有K一个参数，类似K-meansimport matplotlib.pyplot as pltiris = datasets.load_iris() #加载sklearn自带的数据集X = iris.data #这是数据y = iris.target #这是每个数据所对应的标签train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=1/3,random_state=3) #这里划分数据以1/3的来划分 训练集训练结果 测试集测试结果k_range = range(1,31)cv_scores = [] #用来放每个模型的结果值for n in k_range: knn = KNeighborsClassifier(n) #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV scores = cross_val_score(knn,train_X,train_y,cv=10,scoring='accuracy') #cv：选择每次测试折数 accuracy：评价指标是准确度,可以省略使用默认值 cv_scores.append(scores.mean())plt.plot(k_range,cv_scores)plt.xlabel('K')plt.ylabel('Accuracy') #通过图像选择最好的参数plt.show()best_knn = KNeighborsClassifier(n_neighbors=3) # 选择最优的K=3传入模型best_knn.fit(train_X,train_y) #训练模型print(best_knn.score(test_X,test_y)) #看看评分'''0.94''']]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jieba包中文分词]]></title>
    <url>%2F2018%2F11%2F16%2FNLP_jieba%2F</url>
    <content type="text"><![CDATA[jieba包中文分词‘结巴’中文分词：做最好的 Python 中文分词组件 特点与算法特点 支持三种分词模式： 精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 支持繁体分词 支持自定义词典 MIT 授权协议 算法 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-"""Created on Fri Nov 16 21:33:09 2018@author: nocoder"""#%%import jiebaimport jieba.analysewithWeight = True#FalsetopK = 5short_strs = '我来到北京清华大学'long_strs = '随着政府对新能源汽车的大力扶植以及智能联网汽车兴起都预示着未来几年汽车行业的多元化发展及转变。汽车厂商需要了解自身产品是否能够满足消费者的需求，但传统的调研手段因为样本量小、效率低等缺陷已经无法满足当前快速发展的市场环境。因此，汽车厂商需要一种快速、准确的方式来了解消费者需求。'#用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径jieba.analyse.set_idf_path("jieba_data/jieba_idf.txt.big");# 关键词提取所使用停止词(Stop Words)文本语料库可以切换成自定义语料库的路径jieba.analyse.set_stop_words("jieba_data/stop_words.txt")#%%"""分词"""# 待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8# jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用# jieba.lcut 以及 jieba.lcut_for_search 直接返回 list# jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。# 全模式# jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型seg_list = jieba.cut(short_strs, cut_all=True)print("Full Mode: " + "/ ".join(seg_list))#我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学# 精确模式，默认是精确模式seg_list = jieba.cut(short_strs, cut_all=False)print("Default Mode: " + "/ ".join(seg_list))#我/ 来到/ 北京/ 清华大学# 搜索引擎模式# jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细seg_list = jieba.cut_for_search(short_strs)print(", ".join(seg_list))#我, 来到, 北京, 清华, 华大, 大学, 清华大学#%%"""添加自定义词典"""# 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率# 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径# 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。# 词频省略时使用自动计算的能保证分出该词的词频。jieba.load_userdict("jieba_data/word_dict.txt")# 手动添加# 使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。# 使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。# 注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。jieba.add_word('石墨烯')jieba.add_word('台中')jieba.add_word('创新办', freq=5)jieba.add_word('八一双鹿', freq=3, tag = 'nz')test_sent = ("李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\n""例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\n""「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。")words = jieba.cut(test_sent)print('/'.join(words))print("="*40)print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))#如果/放到/post/中将/出错/。print(jieba.suggest_freq(('中', '将'), True))#494print(jieba.get_FREQ('中将'))#494??print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))#如果/放到/post/中/将/出错/。#%%print('TF-IDF', 26*'-')"""基于 TF-IDF 算法的关键词抽取"""# sentence 为待提取的文本# topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20# withWeight 为是否一并返回关键词权重值，默认值为 False# allowPOS 仅包括指定词性的词，默认值为空，即不筛选TF_IDF_tags = jieba.analyse.extract_tags(long_strs, topK=topK, withWeight=withWeight)if withWeight is True: for tag in TF_IDF_tags: print("TF_IDF_tags: %s\t weight: %f" % (tag[0],tag[1]))else: print(",".join(TF_IDF_tags))#%%print('TextRank', 26*'-')"""基于 TextRank 算法的关键词抽取"""TextRank_tags = jieba.analyse.textrank(long_strs, topK=topK, withWeight=withWeight, allowPOS=('ns', 'n', 'vn', 'v'))for x, w in TextRank_tags: print('TF_IDF_tags: %s\t weight: %f' % (x, w))#%%print('词性标注', 26*'-')"""词性标注"""#jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。#标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。import jieba.posseg as psegwords = pseg.cut(short_strs)print(words)for word, flag in words: print('%s %s' % (word, flag))#%%print('Tokenize', 26*'-')""" Tokenize：返回词语在原文的起止位置"""#注意，输入参数只接受 unicoderesult = jieba.tokenize(u'我来到北京清华大学')for tk in result: print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))#%%print('搜索模式', 26*'-')"""搜索模式"""result = jieba.tokenize(u'我来到北京清华大学', mode='search')for tk in result: print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))#%%"""延迟加载机制"""#jieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。import jiebajieba.initialize() # 手动初始化（可选）"""其他词典"""# 占用内存较小的词典文件https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small# 支持繁体分词更好的词典文件https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big# 下载你所需要的词典，然后覆盖 jieba/dict.txt 即可；或者用 jieba.set_dictionary('data/dict.txt.big') 参考github-jieba]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF]]></title>
    <url>%2F2018%2F11%2F15%2FNLP_TFIDF%2F</url>
    <content type="text"><![CDATA[TF-IDF提取关键字，用余弦相似度找相似文章，通过词频对文章摘要 词频：如果某个词很重要，它应该在这篇文章中多次出现。我们进行”词频”（Term Frequency，缩写为TF）统计。 词频（TF） = 某个词在文章中的出现次数。文章有长短之分，为了便于不同文章的比较,做”词频”标准化。 词频（TF） = 某个词在文章中的出现次数 / 文章总词数 词频（TF） = 某个词在文章中的出现次数 / 拥有最高词频的词的次数 停用词：结果你肯定猜到了，出现次数最多的词是—-“的”、”是”、”在”—-这一类最常用的词。它们叫做”停用词”（stop words），表示对找到结果毫无帮助、必须过滤掉的词。 如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词发现”中国”、”蜜蜂”、”养殖”这三个词的出现次数一样多。因为”中国”是很常见的词，相对而言，”蜜蜂”和”养殖”不那么常见，”蜜蜂”和”养殖”的重要程度要大于”中国” IDF :最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。 需要一个语料库（corpus），用来模拟语言的使用环境。逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1） TF-IDF：”词频”（TF）和”逆文档频率”（IDF）以后，两个值相乘，得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。 TF-IDF = 词频（TF) * 逆文档频率（IDF），可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。 总结TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） TF-IDF与与余弦相似的应用：找相似文章除了找到关键词，还希望找到与原文章相似的其他文章，需要用到余弦相似性。 eg: 分词 句子A：我/喜欢/看/电视，不/喜欢/看/电影。 句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。 列出所有值 我，喜欢，看，电视，电影，不，也。 计算词频 句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。 句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1 写出词频向量。 句子A：[1, 2, 2, 1, 1, 1, 0] 句子B：[1, 2, 2, 1, 1, 2, 1] 我们可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。假定a向量是[x1, y1]，b向量是[x2, y2]，那么可以将余弦定理改写成下面的形式: cosθ = (x1x2 + y1y2)/(根号(x1^2 + y1^2) + 根号(x2^2 + y2^2)) 通过词频，对文章进行自动摘要信息都包含在句子中，有些句子包含的信息多，有些句子包含的信息少。”自动摘要”就是要找出那些包含信息最多的句子。句子的信息量用”关键词”来衡量。如果包含的关键词越多，就说明这个句子越重要。 Luhn提出用”簇”（cluster）表示关键词的聚集。所谓”簇”就是包含多个关键词的句子片段。 只要关键词之间的距离小于”门槛值”，它们就被认为处于同一个簇之中。Luhn建议的门槛值是4或5。也就是说，如果两个关键词之间有5个以上的其他词，就可以把这两个关键词分在两个簇。 簇的重要性 = (包含的关键词数量)^2 / 簇的长度 其中的簇一共有7个词，其中4个是关键词。因此，它的重要性分值等于 ( 4 x 4 ) / 7 = 2.3。 然后，找出包含分值最高的簇的句子（比如5句），把它们合在一起，就构成了这篇文章的自动摘要 潜在语义索引(LSI)潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic Analysis（LSA）,是一种简单实用的主题模型,LSI是基于奇异值分解（SVD）的方法来得到文本的主题的 LSI用于文本相似度计算通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。 LSI主题模型总结LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。 主要的问题有： SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。 主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。 主题值的选取对结果的影响非常大，很难选择合适的k值。 这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。 LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。 牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。 回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。 文本主题模型之LDA隐含狄利克雷分布(Latent Dirichlet Allocation，以下简称LDA)。注意机器学习还有一个LDA，即线性判别分析，主要是用于降维和分类的。 基础LDA贝叶斯模型LDA是基于贝叶斯模型的，涉及到贝叶斯模型离不开“先验分布”，“数据（似然）”和”后验分布”三块。在朴素贝叶斯算法原理小结中我们也已经讲到了这套贝叶斯理论。在贝叶斯学派这里： 先验分布 + 数据（似然）= 后验分布比如你对好人和坏人的认知，先验分布为：100个好人和100个的坏人，即你认为好人坏人各占一半，现在你被2个好人（数据）帮助了和1个坏人骗了，于是你得到了新的后验分布为：102个好人和101个的坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成你的新的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布为：103个好人和104个的坏人。依次继续更新下去 二项分布与Beta分布多项分布与Dirichlet 分布LDA主题模型]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法评估]]></title>
    <url>%2F2018%2F11%2F14%2FML_alg_eval%2F</url>
    <content type="text"><![CDATA[不同算法有不同特点，在不同数据集上有不同的表现效果，根据特定的任务选择不同的算法。 评价指标 正确率(accuracy)是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。 错误率(error rate)与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以 accuracy =1 - error rate。 灵敏度(sensitive)，sensitive = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。 特效度(specificity)，specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。 精确度(precision)是精确性的度量，表示被分为正例的示例中实际为正例的比例，precision=TP/ (TP+FP)。 召回率(recall)是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitive，可以看到召回率与灵敏度是一样的 其他评价指标 计算速度：分类器训练和预测需要的时间 鲁棒性：处理缺失值和异常值的能力 可扩展性：处理大数据集的能力 可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。 查准率和查全率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标 F1 测试值，也称为综合分类率 F1 = 2/(1/P + 1/R)=2PR/(P+R) –&gt;Fβ = (1+β)PR/(β^2P + R)]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>指标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow常见函数]]></title>
    <url>%2F2018%2F11%2F12%2FML_TensorFlow_FUN%2F</url>
    <content type="text"><![CDATA[tensorflow常见函数: tf.truncated_normal(shape, mean, stddev)这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差 shape表示生成张量的维度 mean是均值 stddev是标准差。 tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)可以将一个张量中的数值限制在一个范围之内。(可以避免一些运算错误) t(A Tensor) clip_value_min、clip_value_max是对数据的限制。 return 一个限量后的tensor tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None)这个函数的功能就是计算labels和logits之间的交叉熵(cross entropy) _sentinel基本不用 labels一个分类标签，所不同的是，这个label是分类的概率，比如说[0.2,0.3,0.5]，labels的每一行必须是一个概率分布 logits:它把某个概率p从[0,1]映射到[-inf,+inf](即正负无穷区间)。这个函数的形式化描述为：logit=ln(p/(1-p)) 如果labels的每一行是one-hot表示，也就是只有一个地方为1(或者说100%)，其他地方为0(或者说0%)，还可以使用tf.sparse_softmax_cross_entropy_with_logits()。之所以用100%和0%描述，就是让它看起来像一个概率分布。tf.nn.softmax_cross_entropy_with_logits()函数已经过时 (deprecated)，它在TensorFlow未来的版本中将被去除。取而代之的是tf.nn.softmax_cross_entropy_with_logits_v2()。参数labels,logits必须有相同的形状 [batch_size, num_classes] 和相同的类型(float16, float32, float64)中的一种，否则交叉熵无法计算。tf.nn.softmax_cross_entropy_with_logits 函数内部的 logits 不能进行缩放，因为在这个工作会在改函数内部进行(注意函数名称中的 softmax ，它负责完成原始数据的归一化)，如果 logits 进行了缩放，那么反而会影响计算正确性。** tf.greater(x, y)通过比较x、y两个值的大小来输出对错。当x=4，y=3时，输出结果为：True；当x=2，y=3时，输出结果为：False。 tf.where(condition, x=None, y=None, name=None)如果x、y均为空，那么返回condition中值为True的位置的Tensor：例如：x就是condition，y是返回值 condition:一个Tensor,数据类型为tf.bool类型]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec词向量]]></title>
    <url>%2F2018%2F11%2F11%2FNLP_word2vec%2F</url>
    <content type="text"><![CDATA[word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系 one-hot词向量使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。 优点：简单，缺点： 词汇表数量级大，每个词都用数量极大的向量来表示，内存损耗。 向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高。 无法表达不同词之间的相似度 Dristributed分布式通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。 King向量−Man向量+Woman向量=Queen向量 对比，word2vec有优点: 低维稠密，分布式词向量的维度设置成100-500就足够使用，而one-hot需要整个词汇表大小向量维度 蕴含语义信息余弦相似度：sim(w1,w2) = vec_w1 . vec_w2/(||vec_w1||.||vec_w2||) 从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word CBOW(连续词袋模型)训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。 ‘I like deep learning and NLP very match’ 上下文大小取值为3，特定词为learning,上下文对应的词有6个，前后各3个’I like deep’、’and NLP very’，这6个词是我们模型的输入。CBOW使用的是词袋模型，这6个词平等的，不考虑它们和关注的词之间的距离大小，在上下文即可 输入是6个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大）。对应的CBOW神经网络模型输入层有6个神经元，输出层有词汇表大小个神经元，隐藏层的神经元个数我们可以自己指定。 skip-gram(跳字模型)用一个词来预测它在文本序列周围的词。 Skip-GramSkip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为3， 特定的这个词”Learning”是我们的输入，而这6个上下文词是我们的输出。 输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。 word2vec基础之霍夫曼树word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。 霍夫曼树的建立其实并不难，过程如下： 输入：权值为(w1,w2,…wn)的n个节点 输出：对应的霍夫曼树 将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点。 在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。 重复步骤2）和3）直到森林里只有一棵树为止。 一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。 一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1。？ 在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。 模型优化欠采样 subsample‘是’、’的’这种词在任何场景中都可能出现，它们并不包含多少语义，而且出现的频率特别高，如果不加处理会影响词向量的效果。欠采样就是为了应对这种现象，它的主要思想是对每个词都计算一个采样概率，根据概率值来判断一个词是否应该保留。 概率：p(word) = (根号(f(word)/0.001 + 1)).(0.001/f(word)) 负采样 negative sample负采样是加快训练速度的一种方法，这里的负可以理解为负样本。挑选(建议)5~20个 p(word) = f(word)^(3/4)/Σ(i=1,10000)(f(word_i)^(3/4)) 层次softmax层次softmax的目的和负采样一样，也是为了加快训练速度，但它相对复杂，没有负采样这种来的简单粗暴.使用层次softmax时图4中的模型输出层不再是使用one-hot加softmax回归，而是使用Huffman树加softmax回归. 在模型训练的时候首先统计语料中词语的词频，然后根据词频来构建Huffman树. 为什么使用Huffman树可以加快训练速度:输出层不使用one-hot来表示，softmax回归就不需要对那么多0（也即负样本）进行拟合，仅仅只需要拟合输出值在Huffman树中的一条路径 由于Huffman树是二叉树，这意味着只需要判断向左还是向右就可以从根节点走到W2 ，判断向左还是向右其实就是进行二分类 softmax regression 做二分类的时候就退化为了logistic regression，因此虽然叫层次softmax但公式中其实用的是logistic function 代码使用1class gensim.models.word2vec.Word2Vec(sentences=None,size=100,alpha=0.025,window=5, min_count=5, max_vocab_size=None, sample=0.001,seed=1, workers=3,min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=&lt;built-in function hash&gt;,iter=5,null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000) 参数： sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或LineSentence构建。 sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。 size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。 window：表示当前词与预测词在一个句子中的最大距离是多少 alpha: 是学习速率 seed：用于随机数发生器。与初始化词向量有关。 min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5 max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。 sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5) workers参数控制训练的并行数。 hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用。 negative: 如果&gt;0,则会采用negativesamping，用于设置多少个noise words cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用。 hashfxn： hash函数来初始化权重。默认使用python的hash函数 iter： 迭代次数，默认为5 trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。 sorted_vocab： 如果为1（defaut），则在分配word index 的时候会先对单词基于频率降序排序。 batch_words：每一批的传递给线程的单词的数量，默认为10000]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP基础]]></title>
    <url>%2F2018%2F11%2F09%2FNLP_base%2F</url>
    <content type="text"><![CDATA[NLP基础文本的表示自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些文本符号数学化。词向量是用一个向量来表示某个词的方法。下面介绍几种常见的词向量。 SOW 词集模型忽略文本词序、语法和句法，仅仅记录某个词是否在文本中出现。具体地，根据语料库获得一个单词集合，集合中保存着语料库中出现过的所有单词，集合的长度为 |V|，作为词向量的长度。对于每一个词，将该词在字典中对应的序号所在的位置上的值置为1，其他位置上的值置为0。 例如，字典集合为(&quot;a&quot;, &quot;am&quot;, &quot;beautiful&quot;, &quot;i&quot;, &quot;is&quot;, &quot;not&quot;, &quot;she&quot;)，那么is对应的词向量为[0, 0, 0, 0, 1, 0, 0]，文本&quot;she is beautiful&quot;对应的向量为[0, 0, 1, 0, 1, 0, 1]。 BOW 词袋模型与SOW相比，BOW将文本中某个单词出现的次数也考虑到词向量中。 例如，&quot;she is not beautiful, is she?&quot;对应的向量为[0, 0, 1, 0, 2, 1, 2]。 nBOW 标准化的词袋模型与BOW相比，nBOW多了标准化的步骤。若 c_i代表向量在第 i个位置上对应的单词在文本中出现的次数，d_i为向量在第 i个位置上的值，则 : d_i=c_i/∑(j=1,|v|)c_j 例如：&quot;she is not beautiful, is she?&quot;对应的向量为[0, 0, 0.17, 0, 0.33, 0.17, 0.33]。 TF-IDF 词频-逆文档频率TF-IDF中，每一单词对应的值是词频和逆文本频率的乘积。TF基于单个文本计算，IDF则是基于整个语料库计算。 词频：是指某个词在文本中出现的次数 ci。由于不同的文本有不同的长度，为了方便文本之间的比较，可以将词频标准化。 方法一，TF=某个词在文本中出现的次数文本的总词数； 方法二，TF=某个词在文本中出现的次该文本中出现次数最多的单词的出现次数。 逆文档频率：如果一个单词在整个语料库中的很多文本里都出现，那么该单词的逆文本频率就越小。一种常见的计算方法是IDF=log(语料库中的总的文本数量/(包含该词的文本数量+1))。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。 TF-IDF与文本中的某个词的出现次数成正比，与整个语料库中存在该词的文本的总数量成反比。TF-IDF倾向于过滤掉常见的词语，保留重要的词语。一个文本中的某个单词的TF-IDF值越大，说明在给定语料库的前提下该单词在这个文本中越重要。 最后，它的缺点：一方面，单纯以“词频”衡量一个词的重要性不够全面，有时重要的单词可能出现次数并不多；另一方面，TF-IDF也忽略文本词序、语法和句法。 N-Gram 多元语言模型常见的N-Gram模型包括unigram一元语言模型、bigram二元语言模型和trigram三元语言模型。实践表明，四元及四元以上的语言模型对自然语言处理任务的帮助不大。前面所介绍的SOW、BOW、nBOW、TF-IDF都是基于unigram模型介绍的，它们假设文本中的单词都是相互独立的，忽略了单词出现顺序和文本结构的影响。 对于bigram和trigram模型，同样可以转换成以上的词向量表示方法。 对于bigram模型，&quot;she is not beautiful, is she?&quot;中包括[&quot;she_is&quot;, &quot;is_not&quot;, &quot;not_beautiful&quot;, &quot;beautiful_is&quot;, &quot;is_she&quot;]； 对于trigram模型，&quot;she is not beautiful, is she?&quot;中包括[&quot;she_is_not&quot;, &quot;is_not_beautiful&quot;, &quot;not_beautiful_is&quot;, &quot;beautiful_is_she&quot;]。 LSI/LDA向量空间模型VSM中词向量的长度都为 |V|，通常 |V|是一个很大的数，我们可以对其进行降维处理。经典的向量空间模型往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。VSM没有能力处理一词多义和一义多词问题。 例如同义词也分别被表示成独立的一维，计算向量的余弦相似度时会低估用户期望的相似度；而某个词项有多个词义时，始终对应同一维度，因此计算的结果会高估用户期望的相似度。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。主题模型是特征提取的一种方法，它是对文字隐含主题进行建模的方法，可以用来进行语义挖掘。 初始的词向量可以是SOW、BOW、nBOW、TF-IDF等，通常使用TF-IDF，得到隐含的低维语义空间，达到降维的目的。 潜在语义分析（LSA或LSI）：一种基于奇异值分解（SVD）的方法来得到文本的主题的简单实用的主题模型。LSA经过一次奇异值分解就可以得到主题模型，同时解决词义的问题。但其有很多不足，导致它在当前实际的主题模型中已基本不再使用。 首先，SVD的时间复杂度高，计算耗时【可以选择非负矩阵分解NMF方法来加快处理速度】。 其次，模型中的主题数目一般是凭经验选择的，这个问题存在于大部分的主题模型【可以使用交叉验证的方法寻找适合的主题数】。 还有就是，LSA得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释【可以用pLSA或LDA这类基于概率统计的主题模型来替代基于矩阵分解的主题模型】。 概率潜在语义分析（pLSA或pLSI）：pLSA是一个两层概率模型。在pLSA中，D代表文档，Z代表隐含类别或者主题，W为观察到的单词， P(d_i)表示文档 d_i出现的概率（顺从多项式分布） P(z_k|d_i)表示文档d_i中出现主题z_k下的单词的概率（顺从多项式分布） P(w_j|z_k)表示主题z_k下出现单词w_j的概率因此我们的训练目标是最大化:P(D,W)=∑(i=1,M)P(d_i) ∑(j=1,N_(d_i)) ∑(k=1,K) P(w_j|z_k) P(z_k|d_i)pLSA的参数个数是 M × K + K × |V|，所以参数个数随着文档数量的增加而增加，通过EM算法求解。同时，文档之间相互独立，文档内的单词之间也相互独立。但是很重要的是，pLSA只是对已有文档的建模，也就是说模型不适合于新文档，因此pLSA存在容易过拟合的缺点。【困难在于 P(zk|di)的构造】 隐含狄利克雷分布（LDA）：LDA引入了Dirichlet先验概率，pLSA是LDA的一种特殊情况，因此LDA是一个三层概率模型，相比pLSA更加复杂。【当训练样本量足够大，pLSA的效果等同于LDA】 LDA生成模型中， M篇文档会对应于 M个独立的 Dirichlet-Multinomial共轭结构； K个主题会对应于 K 个独立的 Dirichlet-Multinomial 共轭结构。 Word2vec模型对于BOW、TF-IDF等基于词袋模型来说，不同文本得到的词向量都近乎正交，而且无法计算独立单词组成的文本的距离。 word2vec只能得到词向量，比较词之间的相似度，通过简单的加权、tag加权、tf-idf加权等方式得到文档向量。 GloVe模型Doc2vec模型加权的方式丢失了最重要的句子结构信息（也可以说是词序信息），而doc2vec的方法则保存了这种信息。 距离度量欧氏距离dist(a,b)=||a−b|| 衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关。欧氏距离属于几何距离，几何距离还包括曼哈顿距离、切比雪夫距离。 余弦距离Cosine(a,b)=&lt;a,b&gt;/(|a|×|b|) 衡量的是空间向量的夹角，更体现方向上的差距，对绝对数值不敏感，修正了度量标准不统一的问题。由于余弦距离对绝对数值不敏感，所以可以通过所有的样本在维度上减去一个均值（即令特征的均值为0）来修正这种不合理。 杰卡徳系数Jaccard(a,b)=|a⋂b|/|a⋃b| 衡量的是两个集合中不同元素占所有元素的比例，不关注具体的值，而关注某个值是否存在。与其类似的还有Dice系数：2×|a⋂b| 除以 |a|+|b|。 汉明距离汉明距离的前提是待比较的两个序列是等长的，它表示两个序列对应位置的值不同的数量，相当于进行异或运算。 编辑距离编辑距离是指两个文本之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符／单词替换成另一个字符／单词，插入一个字符／单词，删除一个字符／单词【动态规划问题】。 相对熵（KL散度）D(p||q)=∑(i=1,|V|) p(x_i) log (p(x_i)/q(x_i)) 相对熵可以度量两个随机变量的距离。KL散度是两个概率分布P和Q差别的非对称性的度量。由于相对熵是非对称的，因此可以分别计算D(p||q)和D(q||p)然后取平均 Word Mover’s Distance 词移距离简单来说，对于一个文本中的所有词的词向量，分别找出另一个文本中距离和该词向量距离最近的词向量（即最小距离），最后将这些最小距离求和。不过实际中，在词移距离中的词向量与词向量之间并不是一对一的关系。一个文本中词向量可能对应着另一个文本中的多个词向量，只是权重不同。 min(T⩾0) ∑(i,j=1,n) T_ij ||x_i−x_j|| s.t.∑(j=1,n) T_ij = di, ∑(i=1,n)T_ij=dj]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习1]]></title>
    <url>%2F2018%2F11%2F06%2FML_TensorFlow_1%2F</url>
    <content type="text"><![CDATA[TensorFlow是指“张量的流动”。 TensorFlow的数据流图是由节点 (node)和边(edge)组成的有向无环图 (directed acycline graph，DAG)。TensorFlow由Tensor和Flow两部分组成，Tensor(张量)代表了数据流图中的边，而Flow(流动)这个动作就代表了数据流图中节点所做的操作。 背景以及优点，目前发展前景。 安装TensorFlow 依赖模块 numpy 存储和处理大型矩阵的科学计算包。conda install numpy matplotlib 绘图库 jupyter Ipython的升级版，在浏览器中创建和共享代码、方程、说明文档 scikit-image 对图像的预处理 librosa 音频特征提取。ps:conda好像没有找到还是不能安装？ nltk 语料库,方便地完成很多自然语言处理的任务 keras 第一个被添加到TensorFlow核心中的高级别框架,tensorflow默认API tflearn 另一个支持TensorFlow的第三方框架。pip install git+https://github.com/tflearn/tflearn.git 基础知识系统架构自下而上: 核心层： 设备层(CPU、GPU、FPGA)，网络层(gRPC、RDMA远程直接数据存取) 数据操作层 图计算层 API层(TensorFlow核心API,python,java,C++,Go) 应用层(训练相关类库，预测相关类库) 设计理念图的定义与图的运行完全分开，’符号主义’的库。 定义各种变量，建立数据流图(网络结构图)，在数据流图中规定各变量之间的计算关系。 1234import tensorflow as tf# 定义了一个操作，但实际上并没有运行t = tf.add(8, 9)print(t) # 输出 Tensor("Add_1:0", shape=(), dtype=int32) TensorFlow中涉及的运算都要放在图中，而图的运行只发生在会话 (session)中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后 12345678910import tensorflow as tf# 创建图a = tf.constant([1.0, 2.0])b = tf.constant([3.0, 4.0])c = a * b# 创建会话sess = tf.Session()# 计算cprint sess.run(c) # 进行矩阵乘法，输出[3., 8.]sess.close() 边边有两种连接关系：数据依赖和控制依赖。实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量 节点图中的节点 又称为算子 ，它代表一个操作 (operation，OP) 图把操作任务描述成有向无环图。那么，如何构建一个图呢？构建图的第一步是创建各个节点。 会话启动图的第一步是创建一个Session对象。会话(session)提供在图中执行操作的一些方法。 在调用Session对象的run()方法来执行图时，传入一些Tensor，这个过程叫填充 (feed)；返回的结果类型根据输入的类型而定，这个过程叫取回 (fetch) 会话主要有两个API接口：Extend和Run。Extend操作是在Graph中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出运算结果。 设备设备 (device)是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU。 变量变量 (variable)是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。使用tf.Variable()构造函数，这个构造函数需要一个初始值。123456789101112# 创建一个变量，初始化为标量0state = tf.Variable(0, name="counter")# 创建一个常量张量input1 = tf.constant(3.0)# TensorFlow 还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用结束后，填充数据就消失input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess:print sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)# 输出 [array([ 14.], dtype=float32)] 内核操作 (operation)是对抽象操作(如matmul或者add)的一个统称，而内核 (kernel)则是能够运行在特定设备(如CPU、GPU)上的一种对操作的实现。因此，同一个操作可能会对应多个内核。 变量作用域TensorFlow中有两个作用域 (scope)，一个是name_scope，另一个是variable_scope。 variable_scope: 12v = tf.get_variable(name, shape, dtype, initializer) # 通过所给的名字创建或是返回一个变量tf.variable_scope(&lt;scope_name&gt;) # 为变量指定命名空间 name_scope:用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级 批标准化批标准化 （batch normalization，BN）是为了克服神经网络层数加深导致难以训练而诞生的。深度神经网络随着网络深度加深，训练起来会越来越困难，收敛速度会很慢，常常会导致梯度弥散问题 方法：批标准化一般用在非线性映射（激活函数）之前。对x =Wu +b 做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练 优点：批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点： 加大探索的步长，加快收敛的速度； 更容易跳出局部最小值； 破坏原来的数据分布，一定程度上缓解过拟合 神经元函数与优化方法激活函数激活函数 （activation function）运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。12345678910tf.nn.relu()# 连续但不是处处可微的函数tf.nn.sigmoid()# 平滑非线性激活函数，tf.nn.tanh()# 平滑非线性激活函数tf.nn.elu()# 平滑非线性激活函数tf.nn.bias_add()tf.nn.crelu()# 连续但不是处处可微的函数tf.nn.relu6()# 连续但不是处处可微的函数tf.nn.softplus()# 平滑非线性激活函数tf.nn.softsign()# 平滑非线性激活函数tf.nn.dropout() # 防止过拟合，用来舍弃某些神经元 sigmoid:输出映射在(0,1)内，单调连续，非常适合用作输出层，并且求导比较容易。但是，它也有缺点，因为软饱和性 [15] ，一旦输入落入饱和区，f ‘ (x )就会变得接近于0，很容易产生梯度消失 tanh:tanh函数也具有软饱和性。因为它的输出以0为中心，收敛速度比sigmoid要快。但是仍无法解决梯度消失的问题 relu:relu在x &lt;0时硬饱和。由于x &gt;0时导数为1，所以，relu能够在x &gt;0时保持梯度不衰减，从而缓解梯度消失问题，还能够更快地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新，称为“神经元死亡” softplus可以看作是ReLU的平滑版本。relu定义为f(x)=max(x ,0)。softplus定义为f(x)=log(1+exp(x)) dropout：一个神经元将以概率keep_prob决定是否被抑制。 卷积函数 计算N维卷积的和:tf.nn.convolution(input, filter, padding, strides=None,dilation_rate=None, name=None, data_format=None) 对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果:tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None,data_format= None, name=None) 输入张量的数据维度是[batch, in_height, in_width, in_channels]，卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]，在通道in_channels上面的卷积深度是1，depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所以的结果进行汇总。最后输出通道的总数是in_channels * channel_multiplier:tf.nn.depthwise_conv2d (input, filter, strides, padding, rate=None, name=None,data_format=None) 利用几个分离的卷积核去做卷积:tf.nn.separable_conv2d (input, depthwise_filter, pointwise_filter, strides, padding,rate=None, name=None, data_format=None) Atrous卷积，又称孔卷积或者扩张卷积:tf.nn.atrous_conv2d(value, filters, rate, padding, name=None) 解卷积网络（deconvolutional network）中有时称为“反卷积”:tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=&#39;SAME&#39;,data_format=&#39;NHWC&#39;, name=None) 计算给定三维的输入和过滤器的情况下的一维卷积:tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None,data_format= None, name=None) 用来计算给定五维的输入和过滤器的情况下的三维卷积:tf.nn.conv3d(input, filter, strides, padding, name=None) 和二维反卷积类似:tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding=&#39;SAME&#39;, name=None) 池化函数池化函数一般跟在卷积函数的下一层：1234567891011tf.nn.avg_pool(value, ksize, strides, padding, data_format=&apos;NHWC&apos;, name=None)tf.nn.max_pool(value, ksize, strides, padding, data_format=&apos;NHWC&apos;, name=None)tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None)tf.nn.avg_pool3d(input, ksize, strides, padding, name=None)tf.nn.max_pool3d(input, ksize, strides, padding, name=None)tf.nn.fractional_avg_pool(value, pooling_ratio, pseudo_random=None, overlapping=None,deterministic=None, seed=None, seed2=None, name=None)tf.nn.fractional_max_pool(value, pooling_ratio, pseudo_random=None, overlapping=None,deterministic=None, seed=None, seed2=None, name=None)tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate=None, strides=None,name=None, data_format=None) 分类函数常见的分类函数主要有sigmoid_cross_entropy_with_logits、softmax、log_softmax、softmax_cross_entropy_with_logits等 优化方法12345678class tf.train.GradientDescentOptimizerclass tf.train.AdadeltaOptimizerclass tf.train.AdagradOptimizerclass tf.train.AdagradDAOptimizerclass tf.train.MomentumOptimizerclass tf.train.AdamOptimizerclass tf.train.FtrlOptimizerclass tf.train.RMSPropOptimizer BGD(批梯度下降):优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练数据，随着训练的进行，速度会越来越慢 SGD(随机梯度下降):优点是，SGD在训练数据集很大时，仍能以较快的速度收敛；缺点是，由于抽取不可避免地梯度会有误差，需要手动调整学习率 （learning rate），但是选择合适的学习率又比较困难；容易收敛到局部最优，并且在某些情况下可能被困在鞍点 Momentum(动量法):更新时在一定程度上保留之前的更新方向，利用当前的批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加. Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习；在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛 Adagrad法:自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改：如果本次更新时梯度大，学习率就衰减得快一些；如果这次更新时梯度小，学习率衰减得就慢一些。 Adadelta法：Adagrad法仍然存在一些问题：其学习率单调递减，在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率。Adadelta法用一阶的方法，近似模拟二阶牛顿法，解决了这些问题 RMSprop法：RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例 Adam法：自适应矩估计(adaptive moment estimation)。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率 模型的存储于加载 生成检查点文件 （checkpoint file），扩展名一般为.ckpt，通过在tf.train.Saver对象上调用Saver.save()生成。它包含权重和其他在程序中定义的变量，不包含图结构。如果需要在另一个程序中使用，需要重新创建图形结构，并告诉TensorFlow如何处理这些权重。 生成图协议文件（graph proto file），这是一个二进制文件，扩展名一般为.pb，用tf.train.write_graph()保存，只包含图形结构，不包含权重，然后使用tf.import_graph_def()来加载图形。 队列和线程1234567891011121314151617''' 创建一个先入先出队列,初始化队列插入0.1、0.2、0.3三个数字'''q = tf.FIFOQueue(3, "float")init = q.enqueue_many(([0.1, 0.2, 0.3],))# 定义出队、+1、入队操作x = q.dequeue()y = x + 1q_inc = q.enqueue([y])with tf.Session() as sess: sess.run(init) #sess.run(q_inc) quelen = sess.run(q.size()) for i in range(quelen-2): sess.run(q_inc) # 执行2次操作，队列中的值变为0.3,1.1,1.2 for i in range(quelen): print (sess.run(q.dequeue())) # 输出队列的值 123456'''创建一个随机队列，队列最大长度为10，出队后最小长度为2'''q = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2, dtypes="float")for i in range(0, 10): #10次入队 sess.run(q.enqueue(i))for i in range(0, 8): # 8次出队 print(sess.run(q.dequeue())) 加载数据预加载数据预加载数据缺点在于，将数据直接嵌在数据流图中，当训练数据较大时，很消耗内存123x1 = tf.constant([2, 3, 4])x2 = tf.constant([4, 0, 1])y = tf.add(x1, x2) 填充数据使用sess.run()中的feed_dict参数,将Python产生的数据填充给后端,填充的方式也有数据量大、消耗内存等缺点，并且数据类型转换等中间环节增加了不小开销。1234567891011import tensorflow as tf# 设计图a1 = tf.placeholder(tf.int16)a2 = tf.placeholder(tf.int16)b = tf.add(x1, x2)# 用Python产生数据li1 = [2, 3, 4]li2 = [4, 0, 1]# 打开一个会话，将数据填充给后端with tf.Session() as sess: print(sess.run(b, feed_dict=&#123;a1: li1, a2: li2&#125;)) 从文件读取数据 把样本数据写入TFRecords二进制文件 再从队列中读取]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy库学习]]></title>
    <url>%2F2018%2F09%2F20%2Fpython_numpy%2F</url>
    <content type="text"><![CDATA[NumPy是一个 Python 包。 它代表 “Numeric Python”。 它是一个由多维数组对象和用于处理数组的例程集合组成的库。 NumPy 通常与 SciPy（Scientific Python）和 Matplotlib（绘图库）一起使用。 这种组合广泛用于替代 MatLab，是一个流行的技术计算平台 ndarrayNumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。 存放同类型元素的多维数组 每个元素在内存中都有相同存储大小的区域 组成： 一个指向数据（内存或内存映射文件中的一块数据）的指针 数据类型或 dtype，描述在数组中的固定大小值的格子 一个表示数组形状（shape）的元组，表示各维度大小的元组 一个跨度元组（stride），其中的整数指的是为了前进到当前维度下一个元素需要”跨过”的字节数 跨度可以是负数，这样会使数组在内存中后向移动，切片中 obj[::-1] 或 obj[:,::-1] 就是如此 np.array(object, dtype=None, copy=True, order=&#39;K&#39;, subok=False, ndmin=0)|序号|参数|描述||- |- |- ||1. |object |任何暴露数组接口方法的对象都会返回一个数组或任何(嵌套)序列。||2. |dtype |数组的所需数据类型，可选。||3. |copy |可选，默认为true，对象是否被复制。||4. |order |C(按行)、F(按列)或A(任意，默认)。||5. |subok |默认情况下，返回的数组被强制为基类数组。 如果为true，则返回子类。||6. |ndimin |指定返回数组的最小维数。| ndarray 对象由计算机内存的连续一维部分组成，并结合索引模式，将每个元素映射到内存块中的一个位置。内存块以行顺序(C样式)或列顺序(FORTRAN或MatLab风格，即前述的F样式)来保存元素。 数据类型bool_、int_、intc、intp、int8(16\32\64)、unit8(16\32\64)、float_、float16(32\64)、complex_、complex64(128) numpy 的数值类型实际上是 dtype 对象的实例，并对应唯一的字符，包括 np.bool_，np.int32，np.float32，等等。 数据类型对象dtype数据类型对象是用来描述与数组对应的内存区域如何使用，这依赖如下几个方面： 数据的类型（整数，浮点数或者 Python 对象） 数据的大小（例如， 整数使用多少个字节存储） 数据的字节顺序（小端法&lt;或大端法&gt;） 在结构化类型的情况下，字段的名称、每个字段的数据类型和每个字段所取的内存块的部分 如果数据类型是子数组，它的形状和数据类型 numpy.dtype(obj, align=False, copy=False) object - 要转换为的数据类型对象 align - 如果为 true，填充字段使其类似 C 的结构体。 copy - 复制 dtype 对象 ，如果为 false，则是对内置数据类型对象的引用 numpy常用矩阵运算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# 矩阵的截取a = np.array([[1,2,3,4,5],[6,7,8,9,10]])print(a[0:1]) #=a[0]截取第一行,返回 [[1 2 3 4 5]]print(a[1,2:5]) #截取第二行，第三、四列，返回 [8 9]print(a[1,:]) #截取第二行,返回 [ 6 7 8 9 10]# 按条件截取b = a[a&gt;6] # 截取矩阵a中大于6的元素，范围的是一维数组print(b) # 返回 [ 7 8 9 10]# 布尔矩阵print(a&gt;6) # [[False False False False False] [False True True True True]]# 清零矩阵a[a&gt;6] = 0print(a) # [[1 2 3 4 5][6 0 0 0 0]]# 矩阵的合并可以通过numpy中的hstack方法和vstack方法实现a1 = np.array([[1,2],[3,4]])a2 = np.array([[5,6],[7,8]])# !注意 参数传入时要以列表list或元组tuple的形式传入# 横向合并，返回结果如下print(np.hstack([a1,a2])) # [[1 2 5 6][3 4 7 8]]# 纵向合并，返回结果如下print(np.vstack((a1,a2))) # [[1 2][3 4][5 6][7 8]]# 矩阵的合并也可以通过concatenatef方法。np.concatenate((a1,a2), axis=0) # 等价于 np.vstack((a1,a2))np.concatenate((a1,a2), axis=1) # 等价于 np.hstack((a1,a2))# 函数创建矩阵a2 = np.arange(5,20,2) # 从5开始到20（不包括20），步长为2print(a2) # 返回 [ 5 7 9 11 13 15 17 19]# linspace创建指定数量等间隔的序列，生成等差数列a = np.linspace(0,10,7) # 生成首位是0，末位是10，含7个数的等差数列print(a) # 结果 [0. 1.66666667 3.33333333 5. 6.66666667 8.33333333 10.]# logspace用于生成等比数列a = np.logspace(0,2,5) # 生成首位是100，末位是102，含5个数的等比数列print(a) # 结果[1. 3.16227766 10. 31.6227766 100.]# ones、zeros、eye、empty#ones创建全1矩阵a_ones = np.ones((3,4)) # 创建3*4的全1矩阵 #[[ 1. 1. 1. 1.][ 1. 1. 1. 1.][ 1. 1. 1. 1.]]#zeros创建全0矩阵a_zeros = np.zeros((3,4)) # 创建3*4的全0矩阵#[[ 0. 0. 0. 0.][ 0. 0. 0. 0.][ 0. 0. 0. 0.]]#eye创建单位矩阵a_eye = np.eye(3) # 创建3阶单位矩阵#[ 1. 0. 0.][ 0. 1. 0.][ 0. 0. 1.]]#empty创建空矩阵（实际有值）a_empty = np.empty((3,4)) # 创建3*4的空矩阵 #[[ 1.78006111e-306 -3.13259416e-294 4.71524461e-309 1.94927842e+289][ 2.10230387e-309 5.42870216e+294 6.73606381e-310 3.82265219e-297][ 6.24242356e-309 1.07034394e-296 2.12687797e+183 6.88703165e-315]]# 矩阵乘法（点乘）a1 = np.array([[1,2,3],[4,5,6]]) # a1为2*3矩阵a2 = np.array([[1,2],[3,4],[5,6]]) # a2为3*2矩阵print(a1.shape[1]==a2.shape[0]) # True, 满足矩阵乘法条件print(a1.dot(a2)) # # 结果[[22 28][49 64]]# a1.dot(a2)相当于matlab中的a1*a2# 而python中的a1*a2相当于matlab中的a1.*a2# 矩阵的转置 a.T = a.transpose()# 矩阵的逆 a−1,用linalg的inv函数来求逆**求逆的条件是矩阵的行数和列数相同**print(np.linalg.inv(np.mat(np.arange(1,10).reshape(3,3))))# 获得矩阵中元素最大最小值的函数分别是max和min，可以获得整个矩阵、行或列的最大最小值。a = np.array([[1,2,3],[4,5,6]])print(a.max()) #获取整个矩阵的最大值 结果： 6print(a.min()) #结果：1# 可以指定关键字参数axis来获得行最大（小）值或列最大（小）值# axis=0 行方向最大（小）值，即获得每列的最大（小）值# axis=1 列方向最大（小）值，即获得每行的最大（小）值# 例如print(a.max(axis=0)) # 结果为 [4 5 6]print(a.max(axis=1)) # 结果为 [3 6]# 要想获得最大最小值元素所在的位置，可以通过argmax函数来获得print(a.argmax(axis=1)) # 结果为 [2 2]# 平均值meana = np.array([[1,2,3],[4,5,6]])print(a.mean()) #结果为： 3.5# 同样地，可以通过关键字axis参数指定沿哪个方向获取平均值print(a.mean(axis=0)) # 结果 [ 2.5 3.5 4.5]print(a.mean(axis=1)) # 结果 [ 2. 5.]# 方差 var()相当于函数mean(abs(x - x.mean())**2),其中x为矩阵a = np.array([[1,2,3],[4,5,6]])print(a.var()) # 结果 2.91666666667print(a.var(axis=0)) # 结果 [ 2.25 2.25 2.25]print(a.var(axis=1)) # 结果 [ 0.66666667 0.66666667]# 标准差 std()相当于sqrt(mean(abs(x - x.mean())**2))，或相当于sqrt(x.var())a = np.array([[1,2,3],[4,5,6]])print(a.std()) # 结果 1.70782512766print(a.std(axis=0)) # 结果 [ 1.5 1.5 1.5]print(a.std(axis=1)) # 结果 [ 0.81649658 0.81649658]# 中值 中值指的是将序列按大小顺序排列后，排在中间的那个值，如果有偶数个数，则是排在中间两个数的平均值。# 例如序列[5,2,6,4,2]，按大小顺序排成 [2,2,4,5,6]，排在中间的数是4，所以这个序列的中值是4。# 又如序列[5,2,6,4,3,2]，按大小顺序排成 [2,2,3,4,5,6]，因为有偶数个数，排在中间两个数是3、4，所以这个序列中值是3.5。# 中值的函数是median()，调用方法为numpy.median(x,[axis])，axis可指定轴方向，默认axis=None，对所有数去中值。x = np.array([[1,2,3],[4,5,6]])print(np.median(x)) # 对所有数取中值 # 结果 3.5print(np.median(x,axis=0)) # 沿第一维方向取中值 # 结果[ 2.5 3.5 4.5]print(np.median(x,axis=1)) # 沿第二维方向取中值# 结果[ 2. 5.]# 求和 矩阵求和的函数是sum()，可以对行，列，或整个矩阵求和a = np.array([[1,2,3],[4,5,6]])print(a.sum()) # 对整个矩阵求和 # 结果 21print(a.sum(axis=0)) # 对行方向求和 # 结果 [5 7 9]print(a.sum(axis=1)) # 对列方向求和 # 结果 [ 6 15]# 累积和 ：某位置累积和指的是该位置之前(包括该位置)所有元素的和。# 例如序列[1,2,3,4,5]，其累计和为[1,3,6,10,15]，即第一个元素为1，第二个元素为1+2=3，……，第五个元素为1+2+3+4+5=15。# 矩阵求累积和的函数是cumsum()，可以对行，列，或整个矩阵求累积和。a = np.array([[1,2,3],[4,5,6]])print(a.cumsum()) # 对整个矩阵求累积和# 结果 [1 3 6 10 15 21]print(a.cumsum(axis=0)) # 对行方向求累积和# 结果[[1 2 3][5 7 9]]print(a.cumsum(axis=1)) # 对列方向求累积和# 结果[[1 3 6][4 9 15]]#numpy中有一些常用的用来产生随机数的函数，randn()和rand()就属于这其中。#numpy.random.randn(d0, d1, …, dn)是从标准正态分布中返回一个或多个样本值。#umpy.random.rand(d0, d1, …, dn)的随机样本位于[0, 1)中。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistics(sigmoid)]]></title>
    <url>%2F2018%2F09%2F19%2FML_sigmoid%2F</url>
    <content type="text"><![CDATA[测试数学公式: $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ sigmoid函数有$z=wx+b,\sigma(z)={1 \over 1+e^{-z}}$ $$g(x) = 1/(1+e^{-z})$$ 有z=wx+b,α(z)=1/(1+e^-z) 倒数法则：g(x) = 1/f(x)g&#39;(x) = -(f&#39;(x)/f(x)^2) 冥指数求导法则： f(x) = e^z = z * e^z 故α&#39;(z) = α(z)(1-α(z)) 损失函数损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好 logistic函数可以定义损失函数为y^和y的差，或者他们差的平方的一半，结果表明你可能这样做，但是实际当中，大家都不会这么做，因为当你学习这些参数的时候，你会发现之后讨论的优化问题，会变成非凸的，最后会得到很多的局部最优解，梯度下降算法可能找不到最优的全局最优值 平方损失函数假设为高斯分布逻辑回归为伯努利分布softmax为多项式分布]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda使用]]></title>
    <url>%2F2018%2F09%2F15%2Ftool_anaconda%2F</url>
    <content type="text"><![CDATA[Anaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。可以理解为一个python环境平台，其中conda是一个开源的包、环境管理器，可以用于在同一个机器上安装不同版本的软件包及其依赖，并能够在不同的环境之间切换。 配置更改源国内网络资源问题：123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 命令 查看anaconda版本信息：conda --version 查看anaconda环境信息：conda info --envs == conda env list 查看目前有哪些版本的python可以安装：conda search --full-name python 创建一个名为py36的环境：conda create --name py36 python=x.x 激活或进入环境：Linux，OS X: source activate py36,Windows：activate py36 退出一个环境：Linux，OS X: source source deactivate,Windows：activate deactivate 删除一个环境：conda remove -n py36 --all 查看当前环境安装的包：conda list 查找一个包是否可以被安装：conda search pkgName 安装一个新包： 1234conda install --name localPkgName pkgNameconda install --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/win-64/ tensorflow=1.6.0conda install -c https://github.com/tflearn/tflearn.git opencv3=3.1.0不能安装时请使用`pip install pkgName` 删除一个包：conda remove -n localPkgName 参考Anaconda完全入门指南]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用包和函数]]></title>
    <url>%2F2018%2F09%2F04%2Fpython_fun%2F</url>
    <content type="text"><![CDATA[python常用包和函数排序函数 sort() 内置 sorted() 内置 argsort() numpy包sort()sort() 函数用于对原列表进行排序，如果指定参数，则使用比较函数指定的比较函数。 list.sort(cmp=None, key=None, reverse=False) cmp – 好像py3.5没有可选参数, 如果指定了该参数会使用该参数的方法进行排序。 key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。 reverse – 排序规则，reverse = True 降序， reverse = False 升序（默认）。 没有返回值 1234567891011121314151617181920212223ls = list([5, 2, 3, 1, 4])ls.sort()print(ls)'''[1, 2, 3, 4, 5]'''ls = list([5, 2, 3, 1, 4])ls.sort(reverse=True)print(ls)'''[5, 4, 3, 2, 1]'''''''key示例'''random = [('你', 2), ('A', 4), (4, 1), (list(), 3)]def takeSecond(elem): return elem[1]# 即该对象的第1(0开始)个属性random.sort(key=takeSecond)print(random)'''[(4, 1), ('你', 2), ([], 3), ('A', 4)]''' sorted()sorted() 函数对所有可迭代的对象进行排序操作。 sorted(iterable[, cmp[, key[, reverse]]]) iterable – 可迭代对象。 cmp – 好像py3.5没有比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。 key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。 reverse – 排序规则，reverse = True 降序 ， reverse = False 升序（默认）。 返回重新排序的列表。 sort 与 sorted 区别： sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。 list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。 1234567891011121314ls = [5, 2, 3, 1, 4]print(sorted(ls))'''[1, 2, 3, 4, 5]''''''cmp'''ls = [('b',2),('a',1),('c',3),('d',4)]print(sorted(ls, key=lambda x:x[1], reverse=True))'''[('a', 1), ('b', 2), ('c', 3), ('d', 4)]'''# reverse=True同上list.sort()字面意思 zip()zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。 可以使用 list()转换来输出列表。 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。 zip 方法在 Python 2 和 Python 3 中的不同：在 Python 2.x zip() 返回的是一个列表。12345678910111213141516171819202122232425262728293031323334353637383940414243444546x = [1, 2, 3]x = zip(x)print(list(x))'''[(1,), (2,), (3,)]'''x = [1, 2, 3]y = [4, 5, 6]z = [7, 8, 9]xyz = zip(x, y, z)print(list(xyz))'''[(1, 4, 7), (2, 5, 8), (3, 6, 9)]'''x = [1, 2, 3]y = [4, 5, 6, 10]z = [7, 8, 9, 11, 12]xyz = zip(x, y, z)print(list(xyz))'''[(1, 4, 7), (2, 5, 8), (3, 6, 9)]'''x = [1, 2, 3]y = [4, 5, 6]z = [7, 8, 9]xyz = zip(x, y, z)u = zip(*xyz)#看作unzip?print(list(u))'''[(1, 2, 3), (4, 5, 6), (7, 8, 9)]'''ls = [('A',1),('B', 3),('C', 2),('d', 5),('e', 4)]tx = zip(*list(filter(lambda wc: wc[1] &gt;= 3, ls)))print(list(tx))'''[('B', 'd', 'e'), (3, 5, 4)]'''tx, _= zip(*list(filter(lambda wc: wc[1] &gt;= 3, ls)))print(tx)'''('B', 'd', 'e')''' filter()filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换 filter(function, iterable) function – 判断函数。 iterable – 可迭代对象。 12345678910def is_odd(n): '''过滤出所有奇数''' return n % 2 == 1tmplist = filter(is_odd, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])newlist = list(tmplist)print(newlist)'''[1, 3, 5, 7, 9]''' python同时遍历数组的索引和值12345678my_list = ['a', 'b', 'c']for idx, val in enumerate(my_list): print(idx, val)'''0 a1 b2 c''' 参考：https://blog.csdn.net/lilongsy/article/details/80242536]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[list列表]]></title>
    <url>%2F2018%2F09%2F01%2Fpython_list%2F</url>
    <content type="text"><![CDATA[收集记录一些容易遗忘混淆的概念。123456789101112131415161718192021222324252627'''三种遍历获取索引方法'''list3 = [1,3,2,3,1,1]# 第一种遍历：list.index(value)for v in list3: print('idx:%d, value:%d' % (list3.index(v), v))# 第二种遍历：range(len(list))for idx in range(len(list3)): print('idx:%d, value:%d' % (idx, list3[idx]))# 第三种遍历：enumerate(list)for idx,v in enumerate(list3): print('idx:%d, value:%d' % (idx, v))'''两种去重方式'''# 第一种去重：新建newList,遍历原list,not in newList# 保证了列表的顺序性，但过程不够简单new_list = []for v in list3: if v not in new_list: new_list.append(v)print(new_list)#[1, 3, 2]# 第二种去重：利用set()# 简单快速,无法保证去重后的顺序！-&gt;需通过原list索引排序保证顺序不变new_list = list(set(list3))print(new_list)#[1, 2, 3]new_list.sort(key=list3.index)print(new_list)#[1, 3, 2]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_1_start%2F</url>
    <content type="text"><![CDATA[python学习计算机能且只能做两件事情：执行计算与保存计算结果。 计算思维计算思维到底指什么？ 所有知识都可以归结为两类：陈述性知识和程序性知识。 陈述性知识由对事实的描述组成。 程序性知识说明“如何做”，描述的是信息演绎的过程。123456789101112## 如何计算一个数的平方根# (1) 随机选择一个数g；# (2) 如果g × g足够接近x，那么停止计算，将g作为答案；# (3) 否则，将g和x/g的平均数作为新数，也就是(g + x/g)/2；# (4) 使用新选择的数——还是称其为g——重复这个过程，直到g × g足够接近x。## 求出25的平方根。# (1) 为g设置一个任意值，例如3；# (2) 我们确定3 × 3 = 9，没有足够接近25；# (3) 设置g为(3 + 25/3)/2 = 5.67；# (4) 我们确定5.67 × 5.67 = 32.15还是不够接近25；# (5) 设置g为(5.67 + 25/5.67)/2 = 5.04；# (6) 我们确定5.04 × 5.04 = 25.4已经足够接近25了，所以停止计算，宣布5.04就是25的平方根 的一个合适的近似值 方法描述的是一系列简单的步骤，以及一个控制流，用来确定某个步骤在什么 情况下得以执行。这种描述称为算法。 算法正式的定义：算法是一个有穷指令序列，描述了这样一种计算过程，即在给定的输入 集合中执行时，会按照一系列定义明确的状态进行，最终产生一个输出结果。 python简介Python是一门通用性编程语言，几乎可以快速创建任何类型的程序，而不需要直接访问计 算机硬件。 Python 是解释型语言。 Python基本元素Python程序有时称为脚本，是一系列定义和命令。Python解释器，有时称为shell，用来求值这些定义并执行命令。 对象、表达式和数值类型对象是Python程序处理的核心元素。每个对象都有类型。类型分为标量和非标量。 标量对象是不可分的，可以把它们视为语言中的原子。 非标量对 象，比如字符串，具有内部结构。 4类标量对象： int：表示整数。int类型的字面量在形式上与通常的整数一样（如-3、5或10 002)。 float：表示实数。float类型的字面量总是包括一个小数点（如3.0、3.17或-28.72）。 bool：表示布尔值True和False。 None：这个类型只有一个值。 对象和操作符可以组成表达式。 用Python内置函数type给出对象类型：123456789101112131415161718print(type(3)) # &lt;class 'int'&gt;print(type(3.0)) # &lt;class 'float'&gt;``` +-*/ 只要有一个float返回float(**/操作符执行的是浮点数除法**)__//(整除)整数除法只返回商，不返回余数。__%(取余)表示int i除以int j的余数。_通常读作i mod j_i ** j：表示i的j次方。bool类型上的基本操作符为and、or和not### 变量与赋值变量将名称与对象关联起来。```pythoni=3str='hello,python' 变量仅是名称，没有其他意义，但程序中应取大家熟知的名称，因为程序代码不止计算机读取~ 变量名可以包含大写字母、小写字母、数字（但不能以数字开头）和特殊字符_。Python变量名是大小写敏感的。 保留字（py中亦称关键字）and、as、assert、break、class、 continue、def、del、elif、else、except、False、finally、for、from、global、if、 import、in、is、lambda、nonlocal、None、not、or、pass、raise、return、True、try、 while、with和 yield 注释与多重赋值#符号单行注释，不会被python解释。 多重赋值：x, y = 2, 3;将2赋值给x,3赋值给y。使用多重赋值交换对两个变量的绑定1234x, y = 2, 3x, y = y, xprint('x =', x) #x = 3print('y =', y) #y = 2 程序分支简单的分支语句是条件语句。1234567x,y,z=3,1,2if x &lt; y and x &lt; z: print('x is least')elif y &lt; z: print('y is least')else: print('z is least') 字符串和输入字符串str类型的对象用来表示由字符组成的字符串。 str类型的字面量可以用单引号或双引号表示，如’abc’或”abc”。字面量’123’表示有3个字符的字符串，而不是数值123。 操作符+被称为重载，根据应用其上的对象类型的不同，它的意义也不同: 两个数值对象时，它表示相加 两个字符串时，它表示连接 操作符*也是重载 两个数值对象时，它表示相乘 一个int对象，一个str对象，表示重复123print(3 * 'hello') #'hellohellohello'print('hello' * 3) #'hellohellohello'#print('hello' * 'deng')#错误消息是TypeError: can't multiply sequence by non-int of type 'str' _Python中的类型检查不如某 些其他语言严格（如Java），但Python 3中的检查要强于Python 2。python 3明确规定了&lt;在比较两个字符串或两个数值时的意义，&#39;4&#39; &lt; 3在Python 2是False，但Python 3认为该表达式没有明显的意义，会产生错误消息。 _ 字符串是Python中的序列类型之一。所有序列类型都可以执行以下操作： 使用len函数求出字符串的长度 len(&#39;abc&#39;)的值为3 使用索引从字符串提取单个字符(索引从0开始) &#39;abc&#39;[0]的值为’a’ &#39;abc&#39;[-1]的值为’c’(使用负数表示字符串从末尾开始的索引) &#39;abc&#39;[3]会报错：IndexError: string index out of range 使用分片操作从字符串提取任意长度的子串 s为字符串,s[start:end]就表示s中从索引start开始至索引end-1结束的子句：&#39;abc&#39;[1:3] = &#39;bc&#39;。前start省略，默认为0，后end省略，默认为字符串长度len(s)。 输入Python 3中有一个input函数，可以直接接受用户输入(字符串，int需类型转换)。_ Python 2中有两个函数可以接受用户输入，input和raw_input。，Python 2中的raw_input 在语义上和Python 3中的input一样。Python 2中的input函数将输入行当作Python表达式，然后推断具体类型_12345name = input('Enter your name: ') # 例如输入dengprint('Are you really', name, '?') #Are you really deng ?输出结果的?前面有一个空格.当print语句有多个参数时，会在每个参数对应的值之间加上一个空格print('Are you really ' + name + '?') #Are you really deng?使用连接生成一个没有多 余空格的字符串，然后作为唯一参数传递给了printnum = input('Enter number:') #例如输入2print(num * 3) #222 ，input输入的是字符串'2',重复3次 类型转换：int(&#39;2&#39;) * 3的值是6。当一个float值被转换成int值时，数值是被截断的（不是四舍五入）。 杂谈字符编码多数Python实现会默认使用UTF-8。指定python文件使用格式（在程序的第一行或第二行插入一条注释） 格式：1# -*- coding: encoding name -*- 例如：1# -*- coding: utf-8 -*- 迭代迭代是从一个测试条件开始。如果测试条件取值为True，程序就执行一次循环体，然后重新检查测试条件。一直重复这个过程，直到测试条件为False，此后程序控制权就传递给迭代语句后面的代码。123456789# 整数平方num = input('Enter number:') #例如输入5x = int(num)ans = 0itersLeft = xwhile (itersLeft != 0): ans = ans + x itersLeft = itersLeft - 1print(str(x) + '*' + str(x) + ' = ' + str(ans)) #5*5=25 使用break语句结束它所在的循环，嵌套的循环语句中break只会结束break所在内层循环。 练习编写一个程序，要求用户输入10个整数，然后输出其中大的奇数。如果用户没 有输入奇数，则输出一个消息进行说明。12345678910111213141516171819202122232425# -*- coding: utf-8 -*-# 编写一个程序，要求用户输入10个整数，然后输出其中大的奇数。如果用户没 有输入奇数，则输出一个消息进行说明。print('请输入10个整数(用enter健换行),用于判断最大的奇数')numList=[];i=0while i&lt;10: print('请输入第%d个数:'%(i+1)) numStr = input('') if numStr.isdigit(): # 字符串对象的isdigit():检测字符串是否只由数字组成。 ps:isnumeric()也是检测字符串是否只由数字组成但只针对unicode对象 i = i+1 numList.append(int(numStr)) else: print('请重试！请输入数字！')print('您输入了以下数字:', numList)numList.sort(reverse=True)# 使用列表排序方法，参数设置逆序print('从大到小排序为:', numList)for n in numList: if n%2 != 0: print('最大的奇数为:', n) breakelse: print('您输入数字的没有奇数')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python结构化类型、可变性与高阶函数]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_4_structurization%2F</url>
    <content type="text"><![CDATA[之前使用了3种类型：int、float和str，其中int与float是标量类型（这种类型的对象没有可以访问的内部结构）。str是一种结构化的、非标量的类型。 结构化类型元组tuple与字符串一样，元组是一些元素的不可变有序序列。与字符串的区别是，元组中的元素不一 定是字符，其中的单个元素可以是任意类型，且它们彼此之间的类型也可以不同。字符串可以看做一种特殊的元组。 字面量形式是位于小括号之中的由逗号隔开的一组元素：tup = (1,&#39;A&#39;,0.2)123456789tup = (1,'hello',0.1)print(tup)#(1, 'hello', 0.1)# 只包含一个元素1的元组应该写成（小括号是用来分组表达式）tup = (1,) #(1,) tup(1)打印为整数1（(1)只不过是整数1的一种更加冗长的写法）print(tup)tup=(3, 'a')print(2 * tup)#(3, 'a', 3, 'a') 重复 与字符串一样，元组可以进行连接、索引和切片等操作12345tup=(3, 'a', (1,'O')) #元组可以包含元组tup = tup + (1,) #链接print(tup)#(3, 'a', (1, 'O'), 1)print(tup[1])#a 索引print(tup[1:3])#('a', (1, 'O')) 切片 序列与多重赋值a, b, c = &#39;xyz&#39;会将a绑定到x、b绑定到y、c绑定到z123456789101112131415def findExtremeDivisors(n1, n2): """假设n1和n2是正整数 返回一个元组，包含n1和n2的最小公约数和最大公约数，最小公约数大于1， 如果没有公约数，则返回(None, None)。""" minVal, maxVal = None, None for i in range(2, min(n1, n2) + 1): if n1%i == 0 and n2%i == 0: if minVal == None: minVal = i maxVal = i return (minVal, maxVal)# 多重赋值语句minDivisor, maxDivisor = findExtremeDivisors(100, 200)print(minDivisor)#2print(maxDivisor)#100 范围range?range函数会返回一个range类型的对象。？range函数接受3个整数参数：start、stop和step，并返回整数数列start、start + step、start + 2 * step、等等。 如果step是个正数，那么后一个元素就是小于stop的大 整数start + i * step。 如果step是个负数，那么后一个元素就是大于stop的小整数start + i * step。 如果只有2个实参，那么步长就为1。 如果只有1个实参，那么这个参数就是结束值， 起始值默认为0，步长默认为1 除了连接操作和重复操作，其他所有能够在元组上进行的操作同样适用于范围。与元组类型的对象不同，range类型的对象占用的空间与其长度不成正比。因为范围是由起 始值、结束值和步长定义的，它的存储仅占用很小的一部分空间 列表与可变性与元组类似，列表也是值的有序序列，每个值都可以由索引进行标识。list=[1,&#39;A kit&#39;,0]中括号还可以用于表示list类型字面量、列表索引和列表切片. 列表与元组相比有一个特别重要的区别：列表是可变的，而元组和字符串是不可变的. “使对象发生变化”与“将对象赋给变量”这二者之间的区别乍看上去不太明显，但如果你 不断重复“在Python中，变量仅是个名称，就是贴在对象上的标签”这句话，没准儿就顿悟了. 可以使用Python内置函数id验证这一点，id会返回一 个对象的唯一整数标识符。可以用这个函数检测对象是否相等12345678un1 = ['hello',1,2]un2 = ['hello',1,2]print(un1 == un2)#True# id函数会返回一 个对象的唯一整数标识符。可以用这个函数检测对象是否相等(，Python仅要求任意两个对象具有不同标识符)print(id(un1))# 1654612872072print(id(un2))# 1654612872072print(id(un1) == id(un2))#False append方法具有副作用。它不创建一个新列表，而是通过向列表的末尾添加一个新元素，改变这个已有的列表。12un1.append('python')print(un1)# ['hello', 1, 2, 'python'] 操作符+确实没有副作用，它会创建并返回一个新的列表。相反，extend和append都会改变原列表 除了count和index外，其他方法都会改变列表： L.append(e)：将对象e追加到L的末尾。 L.count(e)：返回e在L中出现的次数。 L.insert(i, e)：将对象e插入L中索引值为i的位置。 L.extend(L1)：将L1中的项目追加到L末尾。 L.remove(e)：从L中删除第一个出现的e。 L.index(e)：返回e第一次出现在L中时的索引值。如果e不在L中，则抛出一 个异常（参见第7章）。 L.pop(i)：删除并返回L中索引值为i的项目。如果L为空，则抛出一个异常。 如果i被省略，则i的默认值为-1，删除并返回L中的后一个元素。 L.sort()：升序排列L中的元素。 L.reverse()：翻转L中的元素顺序。 克隆我们通常应该尽量避免修改一个正在进行遍历的列表。避免这种问题的方法是使用切片操作克隆（即复制）这个列表，并使用for e1 in L1[:]这种写法。 表达式list(L)会返回列表L的一份副本。如果待复制的列表包含可变对象，而且你也想复制这些可变对象，那么可以导入标准库模块copy，然 后使用函数copy.deepcopy 列表推导列表推导式提供了一种简洁的方式，将某种操作应用到序列中的一个值上。它会创建一个新 列表，其中的每个元素都是一个序列中的值（如另一个列表中的元素）应用给定操作后的结果。12list1 = [x**2 for x in range(1,7)]print(list1)#[1, 4, 9, 16, 25, 36] 列表推导中的for从句后面可以有一个或多个if语句和for语句，它们可以应用到for子句产 生的值。这些附加从句可以修改第一个for从句生成的序列中的值，并产生一个新的序列：123mixed = [1, 2, 'a', 3, 4.0]list2 = [x**2 for x in mixed if type(x) == int]print(list2)#[1, 4, 9] 函数对象在Python中，函数是一等对象，可以像对待其他类型的对象（如int或list） 一样对待函数12345def hello(): print('hello')print(type(hello))# &lt;class 'function'&gt;print(type(abs))# &lt;class 'builtin_function_or_method'&gt; 使用函数作为实参可以实现一种名为高阶编程的编码方式，这种方式与列表结合使用非常方便Python中有一个内置的高阶函数map。 Python还支持创建匿名函数（即没有绑定名称的函数），这时要使用保留字lambda。 Lambda 表达式的一般形式为：lambda &lt;sequence of variable names&gt;: &lt;expression&gt;Lambda表达式lambda x, y: x*y会返回一个函数，这个函数的返回值为两个参 数的乘积。 Lambda表达式经常用作高阶函数的实参：1234L = []for i in map(lambda x, y: x**y, [1 ,2 ,3, 4], [3, 2, 1, 0]): L.append(i)print(L) #[1, 4, 3, 1] -&gt;1**3, 2**2,3**1,4**0 字符串、元组、范围与列表str、tuple、range和list。 共同之处： seq[i]：返回序列中的第i个元素。 len(sep)：返回序列长度。 seq1 + seq2：返回两个序列的连接（不适用于range）。 n*seq：返回一个重复了n次seq的序列。 seq[start:end]：返回序列的一个切片。 e in seq：如果序列包含e，则返回True，否则返回False。 e not in seq：如果序列不包含e，则返回True，否则返回False。 for e in seq：遍历序列中的元素。 类型 元素类型 字面量示例 是否可变 str 字符型 ‘’、’a’、’abc’ 否 tuple 任意类型 ()、(3,)、(‘abc’, 4) 否 range 整型 range(10)、range(1, 10, 2) 否 list 任意类型 []、[3]、[‘abc’, 4] 是 字符串是不可变的，所以这些方法都返回一个值，而不会对原字符串产生副作用 字典字典（dict，dictionary的缩写）类型的对象与列表很相似，区别在于字典使用键对其中的值 进行引用，可以将字典看作一个键/值对的集合。字典类型的字面量用大括号表示，其中的元素 写法是键加冒号再加上值。dic={&#39;key&#39;:value} dict中的项目是无序的，不能通过索引引用。通过dic[key]来获取健为key的值。 和列表一样，字典是可变的。我们可以使用以下代码增加或改变一个项目：dic[&#39;newKey&#39;] = newValue 123456789101112monthNumbers = &#123;'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':6, 1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May'&#125;monthNumbers['jun']=6#没有就新增monthNumbers['May']=5#有就更改keys = []for e in monthNumbers:# 但分配给迭代变量的值是字典键，不是键/值对，且无序的 keys.append(str(e))print(keys)keys.sort()print(keys)print(monthNumbers.keys())#keys方法返回一个dict_keys类型的对象：dict_keys(['Jan', 'Feb', 'Mar', 'Apr', 'May', 1, 2, 3, 4, 5, 'jun']) 可以使用for语句遍历dict_type类型的对象，也可以使用in检测其中的成员。dict_type类型的对象可以很容易地转换为列表。 并非所有对象都可以用作字典键：键必须是一个可散列类型的对象。可散列的(所有Python内置的不可变类型都是可散列的,所有Python内置的可变类型都是不可散列 的)： 具有hash方法，可以将一个这种类型的对象映射为一个int值，而且对于每一个对象， 由hash返回的值在这个对象的生命周期中是不变的。 具有eq方法，可以比较两个对象是否相等。 1234str1 = 'hello'print(id(str1))#1521781279184print(str1.__hash__)#&lt;method-wrapper '__hash__' of str object at 0x00000162513B89D0&gt;print(str1.__eq__)#&lt;method-wrapper '__eq__' of str object at 0x00000162513B89D0&gt; 常用方法操作： len(d)：返回d中项目的数量。 d.keys()：返回d中所有键的视图。 d.values()：返回d中所有值的视图。 k in d：如果k在d中，则返回True。 d[k]：返回d中键为k的项目。 d.get(k, v)：如果k在d中，则返回d[k]，否则返回v。 d[k] = v：在d中将值v与键k关联。如果已经有一个与k关联的值，则替换。 del d[k]：从d中删除键k。 for k in d：遍历d中的键。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python测试与调试]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_5_test%2F</url>
    <content type="text"><![CDATA[测试和调试不是你编写完程序之后才开始考虑的问题，优秀的程序员在设计程序时，就已经开始考虑如何使程序易于测试和调试了。关键就是将程序分解成独立的部件，可以在不受其他部 件影响的情况下实现、测试和调试。 测试测试的目的是证明错误的存在，而不是证明程序没有错误。 基于代码探索路径的启发式方法称为白盒测试 基于规范探索路径的启发式方法称为黑盒测试 测试一般分为两个阶段： 第一个阶段称为单元测试。在这个阶段中，测试者构建并执行测试， 用来确定代码的每个独立单元（例如：函数）是否正常工作 第二个阶段称为集成测试，用来确 定整个程序能否按预期运行 测试过程自动化的一个显著优点是更易于进行回归测试。 调试修复软件缺陷的过程被称为调试。找不到问题时采用二分查找。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python类与面向对象编程]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_8_classObject%2F</url>
    <content type="text"><![CDATA[使用类围绕模块和数据抽象来组织程序。 对象是Python程序处理的核心元素。每个对象都有类型，定义了程序能够在这个对象上执行的操作。 抽象数据类型与类抽象数据类型是一个由对象以及对象上的操作组成的集合，对象和操作被捆绑为一个整体，可以从程序的一个部分传递到另一个部分。在这个过程中，不但可以使用对象的数据属性，还可以使用对象上的操作，这使得数据处理更加容易。 分解和抽象。分解使程序具有结构，抽象则隐藏细节。抽象的关键是隐藏合适的细节，这就是数据抽象的根本目标。 类定义中存在一个函数定义时，被定义的函数称为方法，并与这个类相关联。这些方法有时称为类的方法属性。 类支持两种操作： 实例化：创建类的实例。例如，语句 s = IntSet() 会创建一个新的 IntSet 类型的对象，这个对象就称为 IntSet 类的一个实例。 属性引用：通过点标记法访问与类关联的属性。例如， s.member 表示与 IntSet 类型的实例 s 关联的 member 方法 Python中有一些特殊的方法名，这些名称的开头和结尾都是两个下划线。 __init__ ，只要一个类被实例化，就会调用该类中定义的 __init__方法。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python异常与断言]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_7_exceptionAssert%2F</url>
    <content type="text"><![CDATA[异常最常见的异常类型是 TypeError 、 IndexError 、 NameError 和 ValueError。 处理异常1234try: ...except xxError: ... 将异常用作控制流Python语言中的 raise 语句可以强制引发一个特定的异常。 raise 语句的形式如下：raise exceptionName(arguments) 其中 exceptionName 通常是一种内置的异常，如 ValueError。可以通过为内置的Exception 类创建一个子类，来定义一个新的异常。 以下代码演示了程序使用 getRatios 函数的方法。 except ValueError as msg: 这行代码中的名称 msg 绑定了抛出 ValueError 时使用的参数（一个字符串）123456789def getRatios(list1,list2): return list1 + list2try: print(getRatios([1.0,2.0,7.0,6.0], [1.0,2.0,0.0,3.0])) print(getRatios([], [])) print(getRatios([1.0, 2.0], [3.0]))except ValueError as msg: print(msg) 断言Python语言中的 assert 语句为程序员提供了一种确保程序状态符合预期的简单方法。语句可以有以下两种形式：assert Boolean expression或者：assert Boolean expression, argument 执行 assert 语句时，先对布尔表达式求值。如果值为 True ，程序就愉快地继续向下执行；如果值为 False ，就抛出一个 AssersionError 异常。断言是一种非常有用的防御性编程工具，可以用来确保函数参数具有恰当的类型。它同时也是一种非常有用的调试工具，可以确保中间值符合预期，或者确保函数返回一个可接受的值。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python函数、作用域与抽象]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_3_fun%20%2F</url>
    <content type="text"><![CDATA[函数与作用域函数定义在Python中，按如下形式进行函数定义：12def functionName(list of formal parameters): body of function def是个保留字。 函数名(functionName)只是个名称，用来引用函数 函数名后面括号中的一系列名称是函数的形式参数(参数有一个特性，称为Lambda抽象:允许程序员编写的代码所处理的不是具体对象，而是函数调用者选定用作实参的任何对象) 使用函数时，形式参数在函数调用时被绑定（和赋值语句一样）到实际参数（通常指代函数调用时的参数） 执行return语句会结束对函数的调用(或者没有语句可以继续执行，这时函数返回的值为None) 练习编写一个函数isIn，接受两个字符串作为参数，如果一个字符串是另一个字符串的一部分，返回True，否则返回False。提示：你可以使用内置的str类型的操作符in:12345678910# 编写一个函数isIn，接受两个字符串作为参数，如果一个字符串是另一个字符串的一部分，返回True，否则返回False。提示：你可以使用内置的str类型的操作符indef isIn(str1,str2): if str1 in str2 or str2 in str1: return True else: return Falseprint(isIn('h','hello')) #Trueprint(isIn('H','hello')) #Falseprint(isIn('hello','hello')) #True 关键字参数和默认值python中有两种方式将形参绑定到实参： 位置参数：第一个形参绑定到第一个实参，第二个形参绑定到第二个实参 持关键字参数：：形参根据名称绑定到实参1isIn(str2='h',str1='hello') # 等同于isIn('hello','h') 尽管关键字参数可以在实参列表中以任意顺序出现，但将关键字参数放在非关键字参数后面是不合法的 作用域每个函数都定义了一个命名空间，也称为作用域 。 在最顶层，比如shell层，有一个符号表会跟踪记录这一层所有的名称定义和它们当前的绑定 调用函数时，会建立一个新的符号表（常称为栈帧）。这个表跟踪记录函数中所有的名称定义（包括形参）和它们当前的绑定。如果函数体内又调用了一个函数，就再建立一个栈帧。 函数结束时，它的栈帧也随之消失。 12345678910111213141516171819def f(x): # 函数定义 def g(): # 嵌套函数定义 x = 'abc' print('g() x =', x) def h(): # 嵌套函数定义 z = x print('h() z =', z) x = x + 1 #函数f()执行的第一步 print('f() x =', x) h() g() print('f()+ x =', x) return gx = 3z = f(x)print('x =', x)print('z =', z) # z是函数f()的引用参数z() # f()返回的是函数g,故执行g() 输出结果：1234567f() x = 4h() z = 4g() x = abcf()+ x = 4x = 3z = &lt;function f.&lt;locals&gt;.g at 0x00000173A06420D0&gt;g() x = abc 引用名称时，顺序并不重要。只要在函数体内任何地方有对象与名称进行绑定（即使在名称作为赋值语句左侧项之前，就已经出现在某个表达式中），就认为这个名称是函数的局部变量。 规范编写测试代码经常是“一本万利”的事情。 三引号之间的文本在Python中称为文档字符串。按照惯例，Python程序员使用文档字符串提供函数的规范。可以使用内置函数help访问这些字符串12345678910111213141516171819202122232425262728293031def findRoot(x, power, epsilon): """ x和epsilon是整数或者浮点数，power是整数 epsilon&gt;0 且power＞＝1 如果y**power和x的差小于epsilon，就返回浮点数y， 否则返回None""" if x &lt; 0 and power%2 == 0: #Negative number has no even-powered #roots return None low = min(-1.0, x) high = max(1.0, x) ans = (high + low)/2.0 while abs(ans**power - x) &gt;= epsilon: if ans**power &lt; x: low = ans else: high = ans ans = (high + low)/2.0 return ans# 测试函数def testFindRoot(): epsilon = 0.0001 for x in [0.25, -0.25, 2, -2, 8, -8]: for power in range(1, 4): print('Testing x =', str(x), 'and power = ', power) result = findRoot(x, power, epsilon) if result == None: print(' No root') else: print(' ', result**power, '~=', x) help(findRoot) 返回结果：12345findRoot(x, power, epsilon) x和epsilon是整数或者浮点数，power是整数 epsilon&gt;0 且power＞＝1 如果y**power和x的差小于epsilon，就返回浮点数y 否则返回None 抽象隐藏了细节。抽象归根结底就是忽略它允许我们将一段代码当作黑箱使用。所谓黑箱，是指那些我们不能看见、不需看见甚至根本不想看见内部细节的东西。抽象的精髓在于，在具体背景之下，保留那些该保留的，忽略那些该忽略的。 在编程中有效使用抽象的关键在于，找到一个对于抽象创建者和抽象潜在使用者都很合适的相关性表示。这才是真正的程序设计艺术。 递归抽象的概念(头疼):一般情况下，递归定义包括两部分。其中至少有一种基本情形可以直接得出某种特定情形的结果，还至少有一种递归情形（或称归纳情形）定义了该问题在其他情形下的结果，其他情形通常是同样问题的简化版本。 1234567def factR(n): if n==1: return n else: return n*factR(n-1)print(factR(5)) 斐波那契数列斐波那契数列是另一个经常使用递归方式定义的常用数学函数。 回文1234567891011121314151617181920def isPalindrome(s): """假设s是字符串 如果s是回文字符串则返回True，否则返回False。 忽略标点符号、空格和大小写。""" def toChars(s): s = s.lower() letters = '' for c in s: if c in 'abcdefghijklmnopqrstuvwxyz': letters = letters + c return letters def isPal(s): if len(s) &lt;= 1: return True else: return s[0] == s[-1] and isPal(s[1:-1]) return isPal(toChars(s)) 这种对isPalindrome的实现是分治策略(与分治算法密切相关，但又有点不一样)的典型例子。这种解决问题的原则就是，将一个困难问题分解成一 组子问题逐个解决。分解出来的子问题具有以下特性： 子问题比初始问题更容易解决 子问题的解决方案可以组合起来解决初始问题 全局变量123456num = 0def fun1(n): for i in range(n): global num # global关键字 num += 1 print(num) 模块模块就是一个包含Python定义和语句的.py文件,程序可以通过import语句访问一个模块。 还有一种import语句的变种，允许导入程序不需使用模块名称即可访问定义在被导入模块中的名称。执行语句from M import *会将M中定义的所有对象绑定到当前作用域，而不是M本身。 文件Python为创建和使用文件提供了非常多的功能。Python通 过文件句柄处理文件，实现了操作系统的独立性 12345fh = open('pyopenTest','w')# 操作系统创建名为'pyopenTest','w'可写('r'可读,'a'用追加（不使用可写方式）方式打开文件)for i in range(2): str = input('请输入：') nameHandle.write(str + '\n');fh.close() # 一定记得关闭文件，否则写入的内容可能部分或全部丢失 ‘w’: 创建一个文件用来写入数据 ‘r’: 打开一个已有文件读取数据 ‘a’: 打开一个已有文件用来追加数据 Python将文件看 成是行的序列，所以可以使用for语句遍历文件内容：12345fh = open('pyopenTest', 'r')for line in nameHandle: #print('打印：' + line) print(line[:-1])#避免输出空行fh.close() 还有以下方法： fh.read()：返回一个字符串，其中包含与文件句柄fh相关的文件中的内容。 fh.readline()：返回与文件句柄fh相关的文件中的下一行。 fh.readlines()：返回一个列表，列表中的每个元素都是与文件句柄fh相关的文件中的一行。 fh.write(s)：将字符串s写入与文件句柄fh相关的文件末尾。 fh.writeLines(S)：S是个字符串序列。将S中的每个元素作为一个单独的行写入与文件句柄fh相关的文件。 fh.close()：关闭与文件句柄fh相关的文件]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python整理学习]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_0%2F</url>
    <content type="text"><![CDATA[Python是一种高层次，解释，互动性和面向对象的脚本语言。 python3.x变化 print函数，python3.x中print函数强制使用括号(),在 python2.x中是可选的。还有换行规则。 键盘读取输入，python2.x中有输入函数两个版本。 input()和 raw_input()。如果被包含在引号 ‘’ 或 “”，input()对待接收到的数据作为字符串，否则数据将被视为数字类型。而python3.x中 raw_input()函数已被弃用。此外，接收到的输入数据总是作为字符串处理。 整数除法，在python2.x中两个整数的除法的结果会四舍五入到最接近的整数：3/2=1(除非分子或分母必须为浮点数，可以理解为3.x的//)，python3.x中更直观：3/2=1.5 Unicode表示，python2.x里如果你想将它保存为 Unicode，需要标记为 U的字符串。python3.x中的字符串默认存储为 Unicode。在python3.x，有Unicode(UTF-8)字符串和字节类：字节和字节数组。 xrange()函数，python2.x中range()返回列表，xrange()返回一个对象以节省内存。python3.x中取消range(),并将xrange()更名为range()，3.2x以上支持切片。 引发异常，python3.x更严格，必须使用() 异常的参数，python3.x异常参数应以 ‘as’关键字来声明 next()函数和.next()方法，？ __future__ 模块，python3.x引入一些python2.x不兼容的关键字和函数，可以通过在 python2.x 内置的模块导入。eg:python2.x中使用python3.x除法：from __future__ import division 2to3实用工具：2t03.py 脚本将被通常安装在 tools/scripts 文件夹。 它读取 python2.x 源代码，并应用了一系列的修复将它转变成有效的 python3.x 代码 语法 变量名可以包含大写字母、小写字母、数字（但不能以数字开头）和特殊字符_。python变量名是大小写敏感的。 使用行和缩进 引号&#39;&#39;,&quot;&quot;,&quot;&quot;&quot;表示字符串，#表示注释 多重赋值：一个值分配给多个变量a = b = c = 1，多个值分配给多个对象a, b, c = 1, 2, &quot;john&quot; Python有五个标准数据类型:数字，字符串，列表，元组，字典 数字：数值类型：int(有符号整数),float(浮点实数值),complex(复数) 字符串：用单引号或双引号包裹，可以使用切片操作[],:，+为连接，*为重复 列表：list=[1,2,&#39;ABC&#39;]，可以不同数据类型，可以使用切片操作，(从0开始，-1结束)，+为连接，*为重复 元组：tuple=(1,2,&#39;ABC&#39;)，可以不同数据类型，不可更改，只读。字符串可以看做特定的元组。 字典：tinydict = {&#39;name&#39;: &#39;john&#39;,&#39;code&#39;:6734, &#39;dept&#39;: &#39;sales&#39;}，无序的，是一种哈希表类型。字典的键(不可重复)可以是几乎任何Python类型，但通常是数字或字符串。另一方面，它的值可以是任意Python对象，]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python简单的数值程序]]></title>
    <url>%2F2018%2F08%2F28%2Fpython_2_value%2F</url>
    <content type="text"><![CDATA[穷举法下面这个程序使用的算法技术称为穷举法，是猜测与检验算法的一个变种。我们枚举所有可能 性，直至得到正确答案或者尝试完所有值。12345678910111213141516171819202122# -*- coding: utf-8 -*-# 寻找完全立方数的立方根 目前只是正整数num = ans = 0while True: num = input('请输入整数：') if num.isdigit(): num = int(num) break else: print('您输入的不是整数，请重试!')while ans ** 3 &lt; abs(num): ans = ans + 1if ans ** 3 != abs(num): print('%d 没有立方根。' % (num))else: if num &lt; 0: ans = -ans print('%d 最小立方根为 %d' % (num, ans)) 练习编写一个程序，要求用户输入一个整数，然后输出两个整数root和pwr，满足0 &lt; pwr &lt; 6，并且root**pwr等于用户输入的整数。如果不存在这样一对整数，则输出一条消息进 行说明。12345678910111213141516171819202122232425# -*- coding: utf-8 -*-# 编写一个程序，要求用户输入一个整数，然后输出两个整数root和pwr，满足0 &lt; pwr &lt; 6，并且root**pwr等于用户输入的整数。如果不存在这样一对整数，则输出一条消息进行说明。num = 0root = 0pwr = 5while True: num = input('请输入整数：') if num.isdigit(): num = int(num) break else: print('您输入的不是整数，请重试!')while pwr &gt; 0: while root * pwr &lt; num: #print('pwr:%d, root: %d 不符合条件！' % (pwr, root)) root = root + 1 if root * pwr == num: print('pwr:%d, root: %d = %d ' % (pwr, root, num)) pwr = pwr - 1 root = 0print('执行结束') for循环for后面的变量被绑定到序列中的第一个值，并执行下面的代码块。然后变量被赋给序列中的第 二个值，再次执行代码块。该过程一直继续，直到穷尽这个序列或者执行到代码块中的break语句。12for variable in sequence: code block 绑定到变量的序列值通常使用内置函数range生成，它会返回一系列整数。range(start, stop[, step]) start: 计数从 start 开始。默认是从 0 开始。例如range（5）等价于range（0， 5）; stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5 step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1) 如果在循环中改变x的值，能否影响迭代次数？答案是“不能”。在for循环那行代码中，range函数的参数在循环的第一次迭代之前就已经被解释器求值，随后的迭代中不会再次求值 下面这个程序：为外层循环中的range函数只被求值一次，但内层循环中的range函数则在每次执行内层 for语句时都被求值12345x = 4for j in range(x): for i in range(x): print(i) x = 2 字符串：1234total = 0for c in '12345678': total = total + int(c) print(total) 近似解和二分查找我们先认为“足够接近”的意 思就是，近似解位于实际解附近的一个常数范围内，这个常数我们称为ε(E:epsilon)。 每经过一次循环迭代，待查找空间都缩小了一半。因为这种算法每一 步都将查找空间分为两部分，所以称为二分查找。 操作符+=、*=、-= 关于浮点数很多时候，float类型的数值是实数的一个非常好的近似。但“很多时候”并不代表所有情 况，这个功能失效时会引起不可思议的后果。1234567x = 0.0for i in range(10): x = x + 0.1 if x == 1.0: print(x, '= 1.0') else: print(x, 'is not 1.0') 1输出结果：0.9999999999999999 is not 1.0 在Python 2中，另一种奇怪的事情发生了。因为输出语句会自动进行某种舍入，所以else从句会输出1.0 is not 1.0。 浮点数在计算机中是如何表示？了解二进制！十进制情况将一个数表示成一个整数对：有效数字和指数。例如，1.949可以表 示为数对(1949,-3)，它代表1949×10的负3次方的积 有效数字的数量决定了数值能被表示的精度。 现代计算机使用二进制表示法，而不是十进制表示法。我们使用二进制表示有效数字和指数：0.625（5/8）会表示成数对(101,-11)， 因为5/8是二进制的0.101，-11是-3的二进制表示，所以数对(101,-11)代表5 × 2的负3次方 = 5/8 = 0.625。 那Python中写作0.1的十进制分数1/10呢？若使用4位有效数字，好的表示方式是(0011, -101)，等于3/32，也就是0.09375。如果有5位有效的二进制数字，可以将0.1表示成(11001,-1000)， 等于25/256，也就是0.09765625。那么，需要多少位有效数字才能使用浮点数准确表示0.1呢？需 要无穷位！不存在两个整数sig和exp，使sig × 2的负exp次方 =0.1。 _在多数Python版本中，使用53位精度表示浮点数，所以为保存十进制0.1而使用的有效数字为：11001100110011001100110011001100110011001100110011001它等于十进制中的：0.1000000000000000055511151231257827021181583404541015625非常接近1/10，但并不是1/10。 顺便说一下，如果对浮点数进行舍入操作，可以使用round函数。表达式round(x, numDigits) 编写代码时，abs(x - y) &lt; 0.0001就比x == y 更好 牛顿 －拉弗森法？？？]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slf4j日志框架学习]]></title>
    <url>%2F2018%2F05%2F29%2Flog_slf4j%2F</url>
    <content type="text"><![CDATA[slf4j(Simple Logging Facade for Java) 即简单日志门面，不是具体的日志解决方案，它只服务于各种各样的日志系统，如java.util.logging, logback和log4j。 概念SLF4J 提供了统一的记录日志的接口，对不同日志系统的具体实现进行了抽象化，只要按照其提供的方法记录即可，最终日志的格式、记录级别、输出方式等通过绑定具体的日志系统来实现。 使用SLF4J的好处在于，你只需要按统一的方式写记录日志的代码，而无需关心日志是通过哪个日志系统，以什么风格输出的。因为它们取决于部署项目时绑定的日志系统。 slf4j 重要的接口与类 Logger：接口，用来记录日志，提供了打印各种级别日志的功能； ILoggerFactory：接口，获取 Logger； LoggerFactory：类，获取 Logger 的门面，他的内部是通过ILoggerFactory 来获取 Logger 的； 注意建议使用 slf4j 而不是 log4j。参考http://www.importnew.com/7450.html slf4j-api 包是日志的接口，log4j, logback 等等才是日志的真正实现。slf4j-simple 包是 slf4j 提供的一个简单实现 。 日志规范在《阿里巴巴Java开发手册(正式版)》中，日志规约一项第一条就强制要求使用slf4j： 【强制】应用中不可直接使用日志系统（Log4j、Logback）中的API，而应依赖使用日志框架SLF4J中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。 参考slf4j 官网 SLF4J简介与使用(整合log4j) Slf4j分析]]></content>
      <categories>
        <category>日志框架</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>slf4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Quartz 组件学习]]></title>
    <url>%2F2018%2F05%2F29%2Fquartz%2F</url>
    <content type="text"><![CDATA[Quartz 是 OpenSymphony 开源组织在 任务调度 领域的一个开源项目，完全基于 Java 实现。 获取资源可以在官网 http://www.quartz-scheduler.org/ 学习或下载 Quartz 发布版本及其源代码。 Maven 管理的项目可以在 pom.xml 中引入依赖( version 请参照官网与实际选择)：12345678910&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135/** * 自定义的任务,实现Job接口 * @author deng * @date 2018年5月29日 */public class MyJob implements Job&#123; private static Logger logger = LoggerFactory.getLogger(MyJob.class); @Override public void execute(JobExecutionContext context) throws JobExecutionException &#123; JobDetail jobDetail = context.getJobDetail(); //String description = jobDetail.getDescription(); logger.info("这是quartz的job实现:\n" + jobDetail.toString()); &#125;&#125;/** * 任务监听，实现JobListener接口 * * @author deng * @date 2018年5月29日 */public class MyJobListener implements JobListener &#123; public static final String LISTENER_NAME = "myJobListenerName"; @Override public String getName() &#123; return LISTENER_NAME; &#125; /** * Scheduler 在 JobDetail 将要被执行时调用这个方法 */ @Override public void jobToBeExecuted(JobExecutionContext context) &#123; String jobName = context.getJobDetail().getKey().toString(); System.out.println("Job : " + jobName + " 将被执行..."); &#125; /** * Scheduler 在 JobDetail 即将被执行，但又被 TriggerListener 否决了时调用这个方法 */ @Override public void jobExecutionVetoed(JobExecutionContext context) &#123; String jobName = context.getJobDetail().getKey().toString(); System.out.println("Job : " + jobName + " 被否决。 "); &#125; /** * Scheduler 在 JobDetail 被执行之后调用这个方法 */ @Override public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException) &#123; String jobName = context.getJobDetail().getKey().toString(); System.out.println("Job : " + jobName + " 执行完毕..."); /*if (!jobException.getMessage().equals("")) &#123;// 版本问题出现异常 System.out.println("Job: " + jobName + " 发生异常: " + jobException.getMessage()); &#125;*/ &#125;&#125;/** * quartz 学习 * * @author deng * @date 2018年5月29日 */public class QuartzStu &#123; private static Logger logger = LoggerFactory.getLogger(QuartzStu.class); public static void main(String[] args) &#123; myJobStu(); &#125; public static void myJobStu() &#123; try &#123; // 创建scheduler Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler(); // 创建Scheduler的工厂 //SchedulerFactory sf = new StdSchedulerFactory(); // 从工厂中获取调度器实例 //Scheduler scheduler = sf.getScheduler(); // 创建Trigger ,使用SimpleScheduleBuilder或者CronScheduleBuilder Trigger trigger = TriggerBuilder.newTrigger() .withDescription("个人任务的触发器") .withIdentity("myTrigger", "myTriggerGroup") .startAt(new Date()) // 默认当前时间启动 // .withSchedule(SimpleScheduleBuilder.simpleSchedule()) .withSchedule(CronScheduleBuilder.cronSchedule("0/10 * * * * ?")) // 10秒执行一次 .build(); // 定义一个JobDetail JobDetail jobDetail = JobBuilder.newJob(MyJob.class) .withDescription("个人任务") .withIdentity("myJob", "myJobGroup") .build(); // 新建监听器 MyJobListener listener = new MyJobListener(); // 构造匹配pickNewsJob中的JobKey的keyMatcher（单个任务的监听器） Matcher&lt;JobKey&gt; matcher = KeyMatcher.keyEquals(jobDetail.getKey()); // 任务组的监听器 // Matcher&lt;JobKey&gt; groupMatcher = GroupMatcher.jobGroupContains("myTriggerGroup"); // 为job添加全局监听器 scheduler.getListenerManager().addJobListener(listener, matcher); // 加入这个调度 scheduler.scheduleJob(jobDetail, trigger); // 启动 scheduler.start(); logger.info("任务调度启动"); // 运行一段时间后关闭 Thread.sleep(10000); // 关闭 scheduler.shutdown(true); logger.info("任务调度停止"); &#125; catch (SchedulerException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考Quartz 教程 Quartz 学习 JobListener 分版本超详细解析]]></content>
      <categories>
        <category>调度框架</category>
      </categories>
      <tags>
        <tag>Quartz</tag>
        <tag>调度框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring Ioc]]></title>
    <url>%2F2018%2F05%2F24%2Fspring_ioc%2F</url>
    <content type="text"><![CDATA[概念控制反转是一个对象如何获取它所依赖的对象的引用； 控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现。那么必然的我们需要创建一个容器，同时需要一种描述来让容器知道需要创建的对象与对象的关系。这个描述最具体表现就是我们可配置的文件。 参考Spring：源码解读Spring IOC原理]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 系统基础]]></title>
    <url>%2F2018%2F05%2F21%2Flinux_base%2F</url>
    <content type="text"><![CDATA[在写这篇文章前，我在思考，为什么要学linux。相对于windows的简单粗暴无脑的操作系统，linux系统确实需要更多的学习才能驾驭它。 为什么学linux，无非开源的（社区发达，它的任何问题都能找到解决方案），免费的（嗯。你不需要支付版权等费用，部分基于linux系统的可能要），高可用的（良好的运行状态，基本不会出现卡顿问题）…等。知乎上有一篇关于《为什么学习linux的文章》的讨论，ZHANG Zaikun 的回答可能会让你我满意。 简介Linux 内核最初只是由芬兰人李纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。Linux 是一套免费使用和自由传播的类Unix操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。 Linux 的发行版Linux的发行版说简单点就是将Linux内核与应用软件做一个打包。目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。 Linux 应用领域各种场合都有使用各种 Linux 发行版，从嵌入式设备到超级计算机，并且在服务器领域确定了地位，通常服务器使用 LAMP（Linux + Apache + MySQL + PHP）或LNMP（Linux + Nginx+ MySQL + PHP）组合。 Linux vs Windows目前国内Linux更多的是应用于服务器上，而桌面操作系统更多使用的是 Windows。请参考（看看就行）：windows和linux到底有哪些区别？ 系统安装各发行版本安装自己 baidu 或 google。脚本安装还不会，暂且搁置。 系统启动过程Linux系统的启动过程可以分为5个阶段： 内核的引导； 运行 init； 系统初始化； 建立终端； 用户登录系统； 可以参考 Linux 的启动流程 内核引导当计算机打开电源后，首先是 BIOS 开机自检，按照 BIOS 中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行initinit 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。init 程序首先是需要读取配置文件 /etc/inittab。 init程序的类型： SysV: init, CentOS 5之前, 配置文件： /etc/inittab; Upstart: init, CentOS 6, 配置文件： /etc/inittab, /etc/init/*.conf； Systemd： systemd, CentOS 7,配置文件: /usr/lib/systemd/system、 /etc/systemd/system; 运行级别许多程序需要开机启动。它们在 Windows 叫做”服务”（service），在 Linux 就叫做”守护进程”（daemon）。init 进程的一大任务，就是去运行这些开机启动的程序。但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动 Apache，用作桌面就不需要。Linux 允许为不同的场合，分配不同的开机启动程序，这就叫做”运行级别”（runlevel）。也就是说，启动时根据”运行级别”，确定要运行哪些程序。 Linux系统有7个运行级别(runlevel)： 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root 权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有 NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11 控制台，登陆后进入图形 GUI 模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 系统初始化在 init 的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而 rc.sysinit 是一个 bash shel l的脚本，它主要是完成一些系统初始化的工作，rc.sysinit 是每一个运行级别都要首先运行的重要脚本。它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 建立终端rc(系统初始化)执行完毕后，返回 init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。init 接下来会打开6个终端，以便用户登录系统。在inittab中的以下6行就是定义了6个终端： 1:2345:respawn:/sbin/mingetty tty1 2:2345:respawn:/sbin/mingetty tty2 3:2345:respawn:/sbin/mingetty tty3 4:2345:respawn:/sbin/mingetty tty4 5:2345:respawn:/sbin/mingetty tty5 6:2345:respawn:/sbin/mingetty tty6 用户登录系统一般来说，用户的登录方式有三种: 命令行登录; SSH 登录; 图形界面登录; 图形模式与文字模式的切换方式Linux 预设提供了六个命令窗口终端机让我们来登录。默认我们登录的就是第一个窗口，也就是 tty1，你可以按下 Ctrl + Alt + F1 ~ F6 来切换它们。如果你安装了图形界面，默认情况下是进入图形界面的，此时你就可以按 Ctrl + Alt + F1 ~ F6 来进入其中一个命令窗口界面。当你进入命令窗口界面后再返回图形界面只要按下 Ctrl + Alt + F7 就回来了。 Linux 关机正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt关机指令为：shutdown ，你可以 man shutdown 来看一下帮助文档。12345678910111213141516171819sync 将数据由内存同步到硬盘中。shutdown 关机指令，你可以man shutdown 来看一下帮助文档。例如你可以运行如下命令关机：shutdown –h 10 ‘This server will shutdown after 10 mins’ 这个命令告诉大家，计算机将在10分钟后关机，并且会显示在登陆用户的当前屏幕中。Shutdown –h now 立马关机Shutdown –h 20:25 系统会在今天20:25关机Shutdown –h +10 十分钟后关机Shutdown –r now 系统立马重启Shutdown –r +10 系统十分钟后重启reboot 就是重启，等同于 shutdown –r nowhalt 关闭系统，等同于shutdown –h now 和 poweroff 最后总结一下，不管是重启系统还是关闭系统，首先要运行 sync 命令，把内存中的数据写到磁盘中。关机的命令有 shutdown –h now halt poweroff 和 init 0 , 重启系统的命令有 shutdown –r now reboot init 6 Linux 系统目录结构以下是对这些目录的解释： /bin：bin 是 Binary 的缩写, 这个目录存放着最经常使用的命令； /boot：这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件； /dev ：dev 是 Device (设备)的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux中访问设备的方式和访问文件的方式是相同的； /etc：这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media：linux 系统会自动识别一些设备，例如 U 盘、光驱等等，当识别后，linux 会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt：这是给主机额外安装软件所摆放的目录。比如你安装一个 ORACLE 数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s 就是 Super User 的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux：这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放 selinux 相关的文件的。 /srv：该目录存放一些服务启动之后需要提取的数据。 /sys：这是 linux2.6 内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。sysfs文件系统集成了下面3种文件系统的信息： 针对进程信息的 proc 文件系统; 针对设备的 devfs 文件系统 针对伪终端的 devpts 文件系统。 /tmp：这个目录是用来存放一些临时文件的。 /usr：这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的program files 目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 在linux系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 Linux 忘记密码解决方法参考：Linux 忘记密码解决方法 参考Linux 教程 -runoob]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux收集学习]]></title>
    <url>%2F2018%2F05%2F21%2Flinux_collect%2F</url>
    <content type="text"><![CDATA[设置全局目录变量进入 shell 中，每次输入 cd /usr/java/jdk1.6.0_43 或更多级目录让人头疼，可以设置全局变量，每次进入可以如下使用：12345[jboss6@receipt ~]$[jboss6@receipt ~]$ cd $JAVA_HOME[jboss6@receipt jdk1.6.0_43]$ pwd/usr/java/jdk1.6.0_43[jboss6@receipt jdk1.6.0_43]$ 编辑 profile 文件 用 vi 或 vim 编辑 /etc/profile 文件（root 用户才有可写权限, 也可以 chmod u+w /etc/profile 给用户添加可写权限）: vim /etc/profile ； 按键盘 i 进入编辑模式，在文件末尾 unset i 前添加 export JAVA_HOME=/usr/java/jdk1.6.0_43，按键盘 Esc，输入 :wq! 保存文件； Source 让设置立即生效 想让配置立刻生效，而不用重新登录用 source 命令：source /etc/profile 你可以使用 cd $JAVA_HOME 快速进入 jdk 目录了，其他脚本中也可以使用该变量； sudo sudo -l 查看sudo权限 sudo -u username /bin/mkdir logs 新建 logs 文件夹]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srpingBoot 学习]]></title>
    <url>%2F2018%2F05%2F15%2Fspring_boot%2F</url>
    <content type="text"><![CDATA[Spring Boot 是由Pivotal团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。其约定优于配置的惯例，让开发者写更少的配置，程序能够更快的运行和启动。 概念Spring 框架就像一个家族，有众多衍生产品例如 boot、security、jpa 等等。但他们的基础都是 Spring 的 ioc和 aop ioc 提供了依赖注入的容器 aop ，解决了面向横切面的编程，然后在此两者的基础上实现了其他延伸产品的高级功能。Spring MVC 是基于 Servlet 的一个 MVC 框架 主要解决 WEB 开发的问题，因为 Spring 的配置非常复杂，各种 XML、 JavaConfig、hin 处理起来比较繁琐。于是为了简化开发者的使用，从而创造性地推出了Spring boot，约定优于配置，简化了 spring 的配置流程。 Spring 是一个”引擎”、”核心”； Spring MVC 是基于 Spring 的一个 MVC 框架； Spring Boot 是基于 Spring4 的一套快速开发整合包； 快速构建 新建maven项目，配置pom.xml: 1234567891011121314151617181920212223242526&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.12.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;finalName&gt;spring-boot-stu&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt; &lt;!--fork : 如果没有该项配置，devtools不会起作用，即应用不会restart --&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 新建配置java类： 12345678910111213141516/** * 新建类（类名不重要），配置@SpringBootApplication注解 * @author deng * @date 2018年5月15日 */@SpringBootApplicationpublic class Application &#123; /** * 应用启动入口 * @param args */ public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 运行上述java类的main方法，即默认(约定)配置启动，浏览器访问：http://localhost:8080/项目名 即可（因为没有编写接口，默认页面是这样-丑）。 在 resources 目录配置配置文件，新建 application.yml（或 application.properties），默认为空。可以写点配置 ,程序的端口为8180，context-path为 /boot-demo 123server: port: 8180 context-path: /boot-demo 新建demoController： 1234567891011121314/** * demo 'hello' * @RestController 将低版本spring的@controller与@ResponseBody 合并 * @author deng * @date 2018年5月15日 */@RestControllerpublic class DemoController &#123; @RequestMapping("/hello") public String hello()&#123; return "hello,springBoot"; &#125;&#125; 再次运行上述配置java类的main方法，浏览器访问：http://localhost:8180/boot-demo/hello 即可。你可以看到页面显示：hello,springBoot。 属性配置可以用配置文件新增属性配置内容：1234567server: port: 8180 context-path: /boot-demopeople: name: nocoder work: IT 可以用以下两种读取属性配置文件： 使用 @Value 读取单个属性 1234567891011121314@Componentpublic class PropertiesStu &#123; @Value("$&#123;people.name&#125;") private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 使用 @ConfigurationProperties(prefix = &quot;people&quot;) 读取某一节点所有属性： 123456789101112131415161718192021222324@ConfigurationProperties(prefix = "people")@Componentpublic class PropertiesStu &#123; private String name; private String work; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getWork() &#123; return work; &#125; public void setWork(String work) &#123; this.work = work; &#125;&#125; 其他类调用： 12345678910// @RequestMapping("/people/&#123;id&#125;")@RequestMapping(value = "/people/&#123;id&#125;", method = &#123; RequestMethod.POST, RequestMethod.GET &#125;)public String people(@PathVariable("id") String id) &#123; StringBuffer sb = new StringBuffer(); sb.append("id=" + id + "\t"); sb.append("name=" + propertiesStu.getName() + "\t"); //sb.append("work=" + propertiesStu.getWork() + "\t"); return sb.toString();&#125; 参考SpringBoot非官方教程 CSDN]]></content>
      <categories>
        <category>springBoot</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RSA 加密算法]]></title>
    <url>%2F2018%2F05%2F15%2Fsecurity_RSA%2F</url>
    <content type="text"><![CDATA[RSA 加密算法是一种非对称加密算法。在公开密钥加密和电子商业中 RSA 被广泛使用。 对极大整数做因数分解的难度决定了 RSA 算法的可靠性。换言之，对一极大整数做因数分解愈困难，RSA 算法愈可靠。假如有人找到一种快速因数分解的算法的话，那么用 RSA 加密的信息的可靠性就肯定会极度下降。但找到这样的算法的可能性是非常小的。今天只有短的 RSA 钥匙才可能被强力方式解破。 背景与原理RSA 公开密钥密码体制。所谓的公开密钥密码体制就是使用不同的加密密钥与解密密钥，是一种“由已知加密密钥推导出解密密钥在计算上是不可行的”密码体制。 Java 应用RSA 公钥密钥的创建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class RsaStu&#123; /** * 生成RSA公钥私钥Demo */ public static void rsaStu()&#123; /** RSA算法要求有一个可信任的随机数源 */ SecureRandom random = new SecureRandom(); KeyPairGenerator keyPairGenerator = null; try &#123; keyPairGenerator = KeyPairGenerator.getInstance("RSA");//RSA/ECB/PKCS1PADDING &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); return;//或者 throw new Exception(); &#125; keyPairGenerator.initialize(512, random);//秘钥长度 ,最短512 KeyPair keyPair = keyPairGenerator.generateKeyPair(); /* 生成公钥 */ RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic(); /* 生成私钥 */ RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate(); System.out.println("RSA publicKey:" + parseByte2HexStr(publicKey.getEncoded())); System.out.println("RSA privateKey:" + parseByte2HexStr(privateKey.getEncoded())); &#125; /** * * 功能描述: 将二进制转换成16进制字符串 * @param buf 二进制组 * @return 十六进制字符串 * @date: 2018年3月8日 */ private static String parseByte2HexStr(byte buf [])&#123; StringBuffer sb = new StringBuffer(); for(int i = 0; i &lt; buf.length; i++)&#123; String hex = Integer.toHexString(buf[i] &amp; 0xFF); if(hex.length() == 1)&#123; hex = '0' + hex; &#125; sb.append(hex.toUpperCase()); &#125; return sb.toString(); &#125;&#125;]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Security</tag>
        <tag>RSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日常电脑技巧收集]]></title>
    <url>%2F2018%2F05%2F15%2Fcollected_sum%2F</url>
    <content type="text"><![CDATA[主要记录日常电脑收集的小技巧。 DNS问题windows 系统正确配置了 IP/DNS、代理等信息，网络仍不可用，除了禁用网络连接（本地网络或无线网络），还可以使用 cmd 刷新重置DNS服务器： 12345678910C:\Windows\system32&gt;ipconfig/flushdnsWindows IP 配置已成功刷新 DNS 解析缓存。C:\Windows\system32&gt;ipconfig/registerdnsWindows IP 配置已经初始化了注册此计算机的所有适配器的 DNS 资源记录。任何错误都将在 15 分钟内在事件查看器中报告。]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>电脑</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES 加密算法]]></title>
    <url>%2F2018%2F05%2F15%2Fsecurity_AES%2F</url>
    <content type="text"><![CDATA[AES 加密算法是密码学中的高级加密标准（Advanced Encryption Standard，AES），又称Rijndael 加密法(严格地说，AES 和 Rijndael 加密法并不完全一样，AES 算是Rijndael 算法的一种特殊实现)，是美国联邦政府采用的一种区块加密标准。2006年，高级加密标准已然成为对称密钥加密中最流行的算法之一。 算法原理AES 的基本要求是，采用对称分组密码体制，密钥的长度最少支持为128、192、256，分组长度必须为128位（16字节，排列：4x4 竖着排的矩阵），算法应易于各种硬件和软件实现。 AES 算法基于排列和置换运算。排列是对数据重新进行安排，置换是将一个数据单元替换为另一个。AES 使用几种不同的方法来执行排列和置换运算。 Java 应用AES 密钥的创建以下为三种密钥生成方式Demo.可根据实际情况选用或拓展： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * * @author deng */class AesStu &#123; /** * 根据原始密钥创建AES密钥 * @param key */ public static void aesStu(String key)&#123; try &#123; // 创建一个 DESKeySpec 对象，使用 key中的前 8个字节，这里采用base64加密后的数组，作为 DES 密钥的密钥内容 DESKeySpec dks = new DESKeySpec(new Base64().decode(key)); SecretKeyFactory keyFactory = SecretKeyFactory.getInstance("DES"); SecretKey sk = keyFactory.generateSecret(dks); // 将密钥的基本编码格式(二进制)转化成16进制的字符串 System.out.println(byteToHexString(sk.getEncoded())); &#125; catch (Exception e) &#123;//直接省略其他Exception，代码规范... e.printStackTrace(); &#125; &#125; /** * 随机生成秘钥 */ public static void getKey() &#123; try &#123; // KeyGenerator 提供（对称）密钥生成器的功能，可重复使用 KeyGenerator kg = KeyGenerator.getInstance("AES"); // 要生成多少位，只需要修改这里即可128, 192或256 kg.init(128); SecretKey sk = kg.generateKey(); System.out.println(byteToHexString(sk.getEncoded())); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; &#125; /** * 使用指定的字符串生成秘钥 * @param pwd */ public static void getKeyByPass(String pwd) &#123; // 生成秘钥 try &#123; KeyGenerator kg = KeyGenerator.getInstance("AES"); // SecureRandom是生成安全随机数序列，password.getBytes()是种子，只要种子相同，序列就一样，所以生成的秘钥就一样。 kg.init(128, new SecureRandom(pwd.getBytes())); SecretKey sk = kg.generateKey(); System.out.println(byteToHexString(sk.getEncoded())); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; &#125; /** * byte数组转化为16进制字符串 * @param bytes * @return */ private static String byteToHexString(byte[] bytes) &#123; StringBuffer sb = new StringBuffer(); for (int i = 0; i &lt; bytes.length; i++) &#123; String strHex = Integer.toHexString(bytes[i]); if (strHex.length() &gt; 3) &#123; sb.append(strHex.substring(6)); &#125; else &#123; if (strHex.length() &lt; 2) &#123; sb.append("0" + strHex); &#125; else &#123; sb.append(strHex); &#125; &#125; &#125; return sb.toString(); &#125;&#125; 参考AES加密算法 百度百科 AES(Rijndael算法) CSDN AES自动生成base64密钥加密解密 CSDN]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Security</tag>
        <tag>AES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自由的含义]]></title>
    <url>%2F2018%2F05%2F14%2Fcollected_free%2F</url>
    <content type="text"><![CDATA[学会用自己的脑袋思考，按自己的意志行事，并为自己的行为负责。这时自由的本意… 自由即责任，世人多畏之。 若无力驾驭，自由便是负担。 毕马隆效应：他人的期望会在一定程度上影响我们的行为。]]></content>
      <categories>
        <category>mind</category>
      </categories>
      <tags>
        <tag>mind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之网络编程]]></title>
    <url>%2F2018%2F05%2F13%2Fjava_net%2F</url>
    <content type="text"><![CDATA[网络编程是指编写运行在多个设备（计算机）的程序，这些设备都通过网络连接起来。Java 语言可编写低层的网络应用。例如，传输文件，建立邮件控制器，处理网络数据等。Java 语言支持的 Internet 协议有 ftp、telnet、www 等，支持网络通信的软件都在 java.net 包中，例如，java.net.ftp、java.net.www等。 TCP/IP，HTTP,FTP等协议名词解释请参考网络基础教程或网上搜索 基础知识TCP/IPTCP/IP分为四个层次： 网络接口层：负责接收和发送物理帧； 网络层：负责相邻节点之间的通信； 传输层：负责起点到终点的通信； 应用层：提供诸如文件传输、电子邮件等应用程序。 TCP/IP 协议是一个协议族，由一组协议组成，主要包含以下更具体的协议： Telnet（远程登录）：允许一台计算机用户登录到另一台远程计算机上，使远程操作如同在本地计算机上操作一样； FTP（File Transfer protocol，文件传输协议）：允许用户将远程主机上的文件复制到自己的计算机上； SMTP（simple Mail Transfer Protocol，简单邮件传输协议）：用于传输电子邮件； NFS（Network file Server，网络文件服务器）：使多台计算机透明地访问彼此的目录； HTTP：一种超文本传输协议，它是基于 TCP/IP 协议的，是 WWW 浏览器和服务器之间应用层的通信协议。HTTP 是一种通用、无状态、面向对象的协议。HTTP 会话（事务）包括四个步骤：连接（Connection）、请求（Request）、应答（Response）和关闭（Close）； IP 地址用于指明因特网上的一台计算机在网络中的地址，用32 位二进制代码表示一个网络地址。地址分A、B、C、D、E五类，常用的是A、B、C三类： A（1.0.0.0-126.255.255.255）：0,7位网络号，后24位为主机号； B（128.0.0.0-191.255.255.255）：10,14位网络号,后16位为主机号； C（192.0.0.0-223.255.255.255）：110,21位网络号,后8位为主机号； D（224.0.0.0-239.255.255.255）：1110,28位多点广播组标号； E（240.0.0.0-254.255.255.255）：1111,保留试验使用。 java.net 包中提供了两种常见的网络协议的支持： TCP：TCP 是传输控制协议的缩写，它保障了两个应用程序之间的可靠通信。通常用于互联网协议，被称 TCP / IP。 UDP：UDP 是用户数据报协议的缩写，一个无连接的协议。提供了应用程序之间要发送的数据的数据包。 URL统一资源定位符 URL(Uniform Resource Locator) 是 www 客户机访问 Internet 时用来标识资源的名字和地址。超文本链路由统一资源定位符 URL 维持。格式是： &lt;METHOD&gt;://&lt;HOSTNAME:PORT&gt;/&lt;PATH&gt;/&lt;FILE&gt; Method 是传输协议; HOSTNAME 是文档和服务器所在的 Internet 主机名（域名系统中 DNS 中的点地址）; PORT 是服务端口号（可省略）; PATH 是路径名; FILE 是文件名。 InetAddressJava.net 包中有 InetAddress 类的定义，InetAddress 类的对象用于 IP 地址和域名，该类提供以下方法： getByName(String s)：获得一个 InetAddress 类的对象，该对象中含有主机的 IP 地址和域名，该对象用如下格式表示它包含的信息：www.sina.com.cn/202.108.37.40； getHostName()：获取 InetAddress 对象的域名； getHostAddress()：获取 InetAddress 对象的 IP 地址； getLocalHost()：获得一个 InetAddress 对象，该对象含有本地机的域名和 IP 地址； 123456789try &#123; // 以下代码通过域名建立InetAddress对象： InetAddress addr = InetAddress.getByName("www.baidu.com"); String domainName = addr.getHostName();// 获得主机名 String IPName = addr.getHostAddress();// 获得IP地址 System.out.println(domainName); System.out.println(IPName); &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; URL 与 URLConnectionURLJava.net 包有 URL 类，一个 URL 对象可以表示一个网络资源。程序利用 URL 对象能实现Internet 寻址、网络资源的定位连接、在客户机与服务器之间直接访问等。 URLConnection要接收和发关信息还要用 URLConnection 类，程序获得一个 URLConnection 对象，相当于完成对指定URL的一个HTTP连接。URLConnection类提供的以下方法: getOutputStream()：获得向远程主机发送信息的 OutputStream 流对象； getInputStream()：获得从远程主机获取信息的 InputStream 流对象。有了网络连接的输入和输出流，程序就可实现远程通信； connect()：设置网络连接 123456789101112131415try &#123; String urlName = "https://www.baidu.com"; URL url = new URL(urlName);// 由网址创建URL对象 URLConnection tc = url.openConnection();// 获得URLConnection对象 tc.connect();// 设置网络连接 InputStreamReader in = new InputStreamReader(tc.getInputStream()); BufferedReader dis = new BufferedReader(in);// 采用缓冲式输入 String inline; while ((inline = dis.readLine()) != null) &#123; System.out.println(inline + "\n"); &#125; dis.close();// 网上资源使用结束后，数据流及时关闭&#125; catch (IOException e) &#123; e.printStackTrace();&#125; Socket 套接字Java 语言在实现 C/S 模式中，套接字分为两类： Server端，ServerSocket 类支持底层的网络通信； Client端，Socket 类支持网络的底层通信; 双方实现通信有流式socket和数据报式socket两种可选方式： 流式 socket 是有连接的通信，即 TCP(Transmission Control Protocol)：每次通信前建立连接，通信结束后断开连接。特点是可以保证传输的正确性、可靠性。 数据报式 socke t是无连接的通信，即 UDP(User Datagram Protocol)：将欲传输的数据分成 小包，直接上网发送。无需建立连接和拆除连接，速度快，但无可靠保证。当 Client 程序和 Server 程序需要通信时，可以用 Socket 类建立套接字连接。 以下步骤在两台计算机之间使用套接字建立TCP连接时会出现： 服务器实例化一个 ServerSocket 对象，表示通过服务器上的端口通信。 服务器调用 ServerSocket 类的 accept() 方法，该方法将一直等待，直到客户端连接到服务器上给定的端口。 服务器正在等待时，一个客户端实例化一个 Socket 对象，指定服务器名称和端口号来请求连接。 Socket 类的构造函数试图将客户端连接到指定的服务器和端口号。如果通信被建立，则在客户端创建一个 Socket 对象能够与服务器进行通信。 在服务器端，accept() 方法返回服务器上一个新的 socket 引用，该 socket 连接到客户端的 socket。 ServerSocket服务器应用程序通过使用 java.net.ServerSocket 类以获取一个端口,并且侦听客户端请求。常用方法： getLocalPort()：返回此套接字在其上侦听的端口； accept()：侦听并接受到此套接字的连接； setSoTimeout(int timeout)：通过指定超时值启用/禁用 SO_TIMEOUT，以毫秒为单位； bind(SocketAddress host, int backlog)：将 ServerSocket 绑定到特定地址（IP 地址和端口号）； Socketjava.net.Socket 类代表客户端和服务器都用来互相沟通的套接字。客户端要获取一个 Socket 对象通过实例化 ，而 服务器获得一个 Socket 对象则通过 accept() 方法的返回值。 注意客户端和服务器端都有一个 Socket 对象，所以无论客户端还是服务端都能够调用这些方法： connect(SocketAddress host, int timeout)：将此套接字连接到服务器，并指定一个超时值； getInetAddress()：返回套接字连接的地址； getPort()：返回此套接字连接到的远程端口； getLocalPort()：返回此套接字绑定到的本地端口。 getRemoteSocketAddress()：返回此套接字连接的端点的地址，如果未连接则返回 null； getInputStream()：返回此套接字的输入流； getOutputStream()：返回此套接字的输出流； close()：关闭此套接字； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class SocketStu&#123; /** * 服务段练习 */ public void server() &#123; ServerSocket server = null; Socket you = null; String s = null; DataOutputStream out = null; DataInputStream in = null; try &#123; server = new ServerSocket(4441); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; you = server.accept(); in = new DataInputStream(you.getInputStream()); out = new DataOutputStream(you.getOutputStream()); while (true) &#123; s = in.readUTF(); System.out.println(s); if (s.equals("#"))&#123; break; &#125; &#125; out.writeUTF("hello, this is server!"); out.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 客户端练习 */ public void client() &#123; String s = null; Socket mySocket; DataInputStream in = null; DataOutputStream out = null; try &#123; mySocket = new Socket("localhost", 4441); in = new DataInputStream(mySocket.getInputStream()); out = new DataOutputStream(mySocket.getOutputStream()); out.writeUTF("hello server,I'm client!"); while (true) &#123; s = in.readUTF(); if (s == null)&#123; break; &#125; else &#123; System.out.println(s); &#125; &#125; mySocket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考Java 网络编程(runoob)Java网络与数据库编程基础]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之多线程]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_thread%2F</url>
    <content type="text"><![CDATA[Java内置支持多线程编程（multithreaded programming）。对于多线程与多进程的区别建议参考《计算机编程原理》。 进程：正在进行中的程序，就是一个应用程序运行时的内存分配空间 线程：进程中一个程序的执行控制单元，一条执行路径。 进程负责的是应用程序的空间的标示，线程负责的是应用程序的执行顺序。CPU 随机性原理：因为 CPU 的快速切换造成，那个线程获取到了 CPU 的执行权，那个线程就执行。 线程的几种状态 被创建：start(); 运行：具备执行资格，同时具备执行权; 冻结：sleep（time）,wait()—–notify()唤醒，线程冻结（沉睡），释放了执行权，同时释放了执行资格; 临时阻塞状态：线程具备cpu的执行资格，没有cpu的执行权; 消亡：stop(); Java 中线程方法与状态 新建状态：线程对象创建(new)时，仅为对象，可设置属性，没有分配资源；setName() 设置线程名;setPriority() 设置优先级;setDaemon() 设置线程类型(用户/守护线程)； 就绪状态：线程对象执行 start()，系统分配除 CPU 外所有资源；yield()：放弃 CPU 资源，重新进入就绪状态； 运行状态：java 通过系统调度，使其占有 CPU 资源，系统真正被执行；isAlive()：判断线程(true)就绪/运行状态，(false)阻塞/停止状态； 阻塞状态：线程不能继续运行；sleep() 等阻塞方法时，线程置入阻塞集里面，等待超时或自动苏醒；多个线程进入同步区域，没能进入该区域的线程被置入锁定集，直到获得同步区域锁，进入就绪状态；wait() 时，线程置入等待集，直到该线程的 notify()。wait()/notify() 执行要求线程首先获得该对象的锁； 死亡状态：线程执行结束后进入死亡状态，此外interrupt()/stop()以异常方式退出进入死亡状态；线程执行完毕正常退出，如设定结束标识，while(flag)，设定flag=false；stop() 强制终止线程（已过时，不建议使用，可能引起不可预料的结果）；interrupt() 强制中断线程，不推荐使用； 线程的创建方式继承 Thread 定义类继承 Thread 类; 重写 run() 方法; 通过创建 Thread 类的子类对象，创建线程对象; 调用线程的 start 方法，开启线程，并执行 run 方法; 12345678910111213141516171819202122232425262728293031323334/** * 线程学习 * @author deng */public class ThreadStu&#123; public static void main(String[] args) &#123; MyThread myThread1 = new MyThread(); myThread1.start(); MyThread myThread2 = new MyThread(); myThread2.start(); MyThread myThread3 = new MyThread(); myThread3.start(); &#125;&#125;class MyThread extends Thread&#123; private static int i = 0; @Override public void run() &#123; while(i&lt;10)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.getName()+":"+i ++); &#125; &#125;&#125; 实现 Runable 定义类实现 Runnable 接口。 覆盖接口中的 run 方法。（用于封装线程要运行的代码）。 通过 Thread 类创建线程对象。 将实现了 Runnable 接口的子类作为实际参数传递给 Thread 类中的构造函数。（为什么要这么做？是为了让线程对象明确要运行的 run 方法所属的对象）。 调用 Thread 对象的 start 方法。开启线程，并运行 Runnable 接口子类中的 run 方法。 1234567891011121314151617181920212223242526272829303132333435/** * 线程学习 * @author deng */public class ThreadStu&#123; public static void main(String[] args) &#123; MyRunnable runnabel1 = new MyRunnable(); //可以设置线程名 new Thread(runnabel1,"runnabel1").start(); //new Thread(runnabel1).start(); MyRunnable runnabel2 = new MyRunnable(); new Thread(runnabel2).start(); MyRunnable runnabel3 = new MyRunnable(); new Thread(runnabel3).start(); &#125;&#125;class MyRunnable implements Runnable &#123; private static int i = 0; public void run() &#123; while(i&lt;10)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // Runnable需使用Thread.currentThread()获取当前线程来获取当前线程信息 System.out.println(Thread.currentThread().getName()+":"+i ++); &#125; &#125;&#125; Thread 类定义了多种方法可以被派生类重载。对于所有的方法，惟一的必须被重载的是 run() 方法。当然实现 Runnable 接口所需的同样的方法。很多 Java 程序员认为类仅在它们被加强或修改时应该被扩展。因此，如果你不重载 Thread 的其他方法时，最好只实现 Runnable 接口。 线程同步防止多个线程访问同一个资源，对资源破坏。线程同步是保证线程安全访问竞争的手段。 java 对象都有一个内置锁； 当程序运行 synchronized 同步方法时，自动获得代码当前实例有关的锁；当程序运行 synchronized 同步代码快时，自动获得锁定对象的锁； 一个对象只有一个锁； 1234567891011121314public void run() &#123; // 同步锁 synchronized (this) &#123; while(i&lt;10)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // Runnable需使用Thread.currentThread()获取当前线程来获取当前线程信息 System.out.println(Thread.currentThread().getName()+":"+i ++); &#125; &#125;&#125; 死锁当一个线程获取对象1的锁时又想获取对象2的锁，而另一个线程有对象2的锁，获取对象1的锁，这种互相等待对方释放锁的过程，导致死锁。 参考Java 多线程编程(runoob)多线程编程]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之输入输出]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_io%2F</url>
    <content type="text"><![CDATA[输入输出（I/O）是指程序与外部设备或其他计算机进行交互的操作。几乎所有的程序都具有输入与输出操作，如从键盘上读取数据，从本地或网络上的文件读取数据或写入数据等。通过输入和输出操作可以从外界接收信息，或者是把信息传递给外界。 流的概念流（Stream）是指在计算机的输入输出操作中各部件之间的数据流动。Java 中一个流被定义为一个数据序列。 流的分类 输入流与输出流流按照数据的传输方向，可分为输入流与输出流（针对内存输入或输出）。一般来说关于流的特性有下面几点： 先进先出，最先写入输出流的数据最先被输入流读取到； 顺序存取，可以一个接一个地往流中写入一串字节，读出时也将按写入顺序读取一串字节，不能随机访问中间的数据； 只读或只写，每个流只能是输入流或输出流的一种，不能同时具备两个功能，在一个数据传输通道中，如果既要写入数据，又要读取数据，则要分别提供两个流； 字节流与字符流流按照数据的传输单位，可分为字节流与字符流。 字节流：读写8位二进制的字节； 字符流：读写16位二进制的字符； 节点流与过滤流（待争琢）流按照功能，可分为节点流与过滤流。 节点流：直接操作目标设备的流，eg：磁盘或内存； 过滤流：对一个已存在的流的封装，对数据进行处理，为程序提供功能强大、灵活的读写功能，不直接操作数据源；eg:缓冲流：为了提高数据的传输效率，引入了缓冲流（Buffered Stream）的概念，即为一个流配备一个缓冲区（Buffer），一个缓冲区就是专门用于传送数据的一块内存。 字节流字节流以字节为传输单位，用来读写8位的数据，除了能够处理纯文本文件之外，还能用来处理二进制文件的数据。由于 InputStream 和 OutputStream 都是抽象类，是所有字节流的父类。所以在程序中创建的输入流对象一般是它们某个子类的对象，通过调用对象继承的 read() 和 write() 方法就可实现对相应外设的输入输出操作。 InputStream面向字节的输入流都是 InputStream 类的子类，其类层次结构如图: InputStream 流类中包含一套所有输入都需要的方法，可以完成最基本的从输入流读入数据的功能，常用方法： read()：从输入流中当前位置读入一个字节的二进制数据，以此数据为低位字节，补足16位的整型量（0~255）后返回，若输入流中当前位置没有数据，则返回-1； read(byte b[])：从输入流中的当前位置连续读入多个字节保存在数组中，并返回所读取的字节数； int read(byte b[], int off, int len)：从输入流中当前位置连续读 len 长的字节，从数组第off+1个元素位置处开始存放，并返回所读取的字节数； available()：返回输入流中可以读取的字节数； skip(long n)：跳过流内的n个字符； close()：关闭输入流； FileInputStreamFileInputStream 是用于从文件读取字节数据的 InputStream 子类。 123456789101112131415161718FileInputStream fis = null;try &#123; fis = new FileInputStream("G:/hexoWorkspace/ioStu.txt"); int date = -1; while ((date = fis.read()) != -1) &#123; System.out.println((char)date); &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125;finally &#123; if(null != fis)&#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; OutputStream面向字节的输出流都是 OutputStream 类的子类，其类层次结构如图： OutputStream 流类中包含一套所有输出都需要的方法，可以完成最基本的向输出流写入数据的功能，常用方法： write(int b)：将参数b的低位字节写入到输出流； write(byte b[])：按顺序将数组b[]中的全部字节写入到输出流； write(byte b[], int off, int len)：按顺序将数组b[]中第off+1个元素开始的len个数据写入到输出流； flush()：强制清空缓冲区并执行向外设输出数据； close()：关闭输出流； FileOutputStreamFileOutputStream 是用于向文件写入字节数据的 OutputStream 子类。 1234567891011121314151617FileOutputStream fos = null;try &#123; fos = new FileOutputStream("G:/hexoWorkspace/ioStu.txt"); String str = "hello,FileOutputStream 练习"; str.getBytes(); fos.write(str.getBytes());&#125; catch (IOException e) &#123; e.printStackTrace();&#125;finally &#123; if(null != fos)&#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 字符流字符流是针对字符数据的特点进行过优化的，因而提供一些面向字符的有用特性，字符流的源或目标通常是文本文件。由于 Reader 和 Writer 都是抽象类，是 java.io 包中所有字符流的父类，所以应使用它们的子类来创建实体对象，利用对象来处理相关的读写操作。Reader 和 Writer 的子类又可以分为两大类：一类用来从数据源读入数据或往目的地写出数据（称为节点流），另一类对数据执行某种处理（称为处理流）。 Reader面向字符的输入流类都是 Reader 的子类，其类层次结构如图： 可以利用 Reader 所提供的这些方法来获得流内的位数据，常用方法： read()：从输入流中读取一个字符； read(char[] ch)：从输入流中读取字符数组； read(char[] ch, int off, int len)：从输入流中读 len 长的字符到 ch 内； ready()：测试流是否可以读取； reset()：重定位输入流； skip(long n)：跳过流内的 n 个字符； close()：关闭输入流； FileReaderFileReader 类是 Reader 子类 InputStreamReader 类的子类，因此 FileReader 类既可以使用Reader 类的方法也可以使用 InputStreamReader 类的方法来创建对象。 需要注意的是，Java 把一个汉字或英文字母作为一个字符对待，回车或换行作为两个字符对待。 1234567891011char a[] = new char[100]; // 创建可容纳 100 个字符的数组 try &#123; FileReader fb = new FileReader("G:/hexoWorkspace/ioStu.txt"); int num = fb.read(a); // 将数据读入到数组 a 中，并返回字符数 String str = new String(a, 0, num); // 将字符串数组转换成字符串 System.out.println("读取的字符个数为：" + num + ",内容为：\n"); System.out.println(str); fb.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; BufferedReaderBufferedReader 类是用来读取缓冲区中的数据。使用时必须创建 FileReader 类对象，再以该对象为参数创建 BufferedReader 类的对象。 1234567891011String readline;try &#123; FileReader a = new FileReader("G:/hexoWorkspace/ioStu.txt"); BufferedReader b = new BufferedReader(a); while ((readline = b.readLine()) != null) &#123; // 每次读取 1 行 System.out.println(readline); &#125; b.close();&#125; catch (IOException io) &#123; e.printStackTrace();&#125; Writer面向字符的输出流都是类 Writer 的子类，其类层次结构如图： Writer 的常用方法： close()：关闭输出流； flush()：将缓冲区中的数据写到文件中； writer(int c)：将单一字符 c 输出到流中； writer(String str)：将字符串 str 输出到流中； writer(char[] ch)：将字符数组 ch 输出到流； writer(char[] ch, int offset, int length)：将一个数组内自 offset 起到 length 长的字符输出到流； FileWriterFileWriter 类是 Writer 子类 OutputStreamWriter 类的子类，因此 FileWriter 类既可以使用 Writer类的方法也可以使用 OutputStreamWriter 类的方法来创建对象。 12345678try &#123; FileWriter fw = new FileWriter("G:/hexoWorkspace/ioStu.txt"); String str = "hello,fileWriter."; fw.write(str); fw.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; BufferedWriterBufferedWriter 类是用来将数据写入到缓冲区。使用时必须创建 FileWriter 类对象，再以该对象为参数创建。 12345678910try&#123; BufferedWriter bw = new BufferedWriter(new FileWriter("G:/hexoWorkspace/ioStu.txt")); String str = "hello,bufferedWriter."; bw.write(str); bw.flush(); bw.close();&#125;catch(IOException e)&#123; e.printStackTrace();&#125; RandomAccessFileJava.io 包提供了 RandomAccessFile 类用于随机文件的创建和访问。使用这个类，可以跳转到文件的任意位置读写数据。程序可以在随机文件中插入数据，而不会破坏该文件的其他数据。此外，程序也可以更新或删除先前存储的数据，而不用重写整个文件。 是JAVA I/O流体系中功能最丰富的文件内容访问类，它提供了众多方法来访问文件内容； 由于可以自由访问文件的任意位置，所以如果需要访问文件的部分内容，RandomAccessFile将是更好的选择； 可以用来访问保存数据记录的文件，文件的记录的大小不必相同，但是其大小和位置必须是可知的； 123456789101112131415161718192021222324BufferedReader br = null;RandomAccessFile raf = null;try &#123; br = new BufferedReader(new InputStreamReader(System.in)); String s = br.readLine(); raf = new RandomAccessFile("G:/hexoWorkspace/ioStu.txt", "rws");// 第二个参数为模式，包括(只读:"r",读写:"rw",读写加内容更新同步写入底层存储设备:"rws","rwd"?) // 移动到文件结尾 raf.seek(raf.length()); // 写入数据，任意访问文件的指针在文件的结尾 raf.writeBytes(s + "\n"); // 读取时，将指针重置到文件的开始位置 raf.seek(0); // 跳过8字节 // raf.skipBytes(8) ; // TODO:有问题 System.out.println(raf.readLine()); br.close(); raf.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 标准输入输出System.in、System.out、System.err 这 3 个标准输入输流对象定义在 java.lang.System 包中，这 3 个对象在 Java 源程序编译时会被自动加载。 标准输入：标准输入 System.in 是 BufferedInputStream 类的对象，当程序需要从键盘上读入数据时，只需要调用 System.in 的 read()方法即可，该方法从键盘缓冲区读入一个字节的二进制数据，返回以此字节为低位字节，高位字节为 0 的整型数据。 标准输出：标准输出 System.out 是打印输出流 PrintStream 类的对象。PrintStream 类是过滤输出流类 FilterOutputStream 的一个子类，其中定义了向屏幕输出不同类型数据的方法print()和 println()。 标准错误输出：System.err 用于为用户显示错误信息，也是由 PrintStream 类派生出来的错误流。Err 流的作用是使 print()和 println()将信息输出到 err 流并显示在屏幕上，以方便用户使用和调试程序。 流的应用场景 FileInputStream/FileOutputStream：需要逐个字节处理原始二进制流的时候使用，效率低下 FileReader/FileWriter：需要组个字符处理的时候使用 StringReader/StringWriter：需要处理字符串的时候，可以将字符串保存为字符数组 PrintStream/PrintWriter：用来包装F ileOutputStream 对象，方便直接将String字符串写入文件 Scanner：用来包装System.in流，很方便地将输入的 String 字符串转换成需要的数据类型 InputStreamReader/OutputStreamReader：字节和字符的转换桥梁，在网络通信或者处理键盘输入的时候用 BufferedReader/BufferedWriter：BufferedInputStream/BufferedOutputStream ， 缓冲流用来包装字节流后者字符流，提升 IO 性能，BufferedReader 还可以方便地读取一行，简化编程。 File文件类请参考API，eg: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * File练习 * @author deng */class FileStu &#123; private int fileLevels = 0; /** * 遍历File * @param file */ public void iteratorDir(File file)&#123; if(null!=file)&#123; // 文件或空目录返回 if(file.isFile() || file.listFiles().length == 0)&#123; return; &#125;else&#123; File[] files = sort(file.listFiles()); for(File f:files)&#123; StringBuilder sb = new StringBuilder(); if(f.isFile())&#123; sb.append(getTab(fileLevels)); sb.append(f.getName()); &#125;else&#123; sb.append(getTab(fileLevels)); sb.append(f.getName()); sb.append("/"); &#125; System.out.println(sb.toString()); if(f.isDirectory())&#123; fileLevels ++;//进入目录 iteratorDir(f);//递归遍历文件夹 fileLevels --;//退出目录 &#125; &#125; &#125; &#125; &#125; /** * 对File列表排序，先文件夹后文件 * @param files * @return */ private File[] sort(File[] files)&#123; List&lt;File&gt; fileList = new ArrayList&lt;File&gt;(); /* 先目录，再文件 */ for(File file:files)&#123; if(file.isDirectory())&#123; fileList.add(file); &#125; &#125; for(File file:files)&#123; if(file.isFile())&#123; fileList.add(file); &#125; &#125; // list 转换数组 return fileList.toArray(new File[fileList.size()]); &#125; /** * 根据文件目录级别返回制表符 * @param level 文件目录级别 * @return */ private String getTab(int level)&#123; StringBuilder sb = new StringBuilder(); for(int i = 0; i&lt;level;i++)&#123; sb.append("\t"); &#125; return sb.toString(); &#125;&#125; 参考Java 流(Stream)、文件(File)和IO(runoob) Java输入输出(IO)操作 Java IO完全总结 JAVA基础知识之IO]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习]]></title>
    <url>%2F2018%2F05%2F12%2FMybatis%2F</url>
    <content type="text"><![CDATA[Mybatis与hibernate的区别： hibernate是全自动，而mybatis是半自动。hibernate完全可以通过对象关系模型实现对数据库的操作，拥有完整的JavaBean对象与数据库的映射结构来自动生成sql。而mybatis仅有基本的字段映射，对象数据以及对象实际关系仍然需要通过手写sql来实现和管理。 hibernate数据库移植性远大于mybatis。hibernate通过它强大的映射结构和hql语言，大大降低了对象与数据库（oracle、mysql等）的耦合性，而mybatis由于需要手写sql，因此与数据库的耦合性直接取决于程序员写sql的方法，如果sql不具通用性而用了很多某数据库特性的sql语句的话，移植性也会随之降低很多，成本很高。 hibernate拥有完整的日志系统，mybatis则欠缺一些。hibernate日志系统非常健全，涉及广泛，包括：sql记录、关系异常、优化警告、缓存提示、脏数据警告等；而mybatis则除了基本记录功能外，功能薄弱很多。 mybatis相比hibernate需要关心很多细节。hibernate配置要比mybatis复杂的多，学习成本也比mybatis高。但也正因为mybatis使用简单，才导致它要比hibernate关心很多技术细节。mybatis由于不用考虑很多细节，开发模式上与传统jdbc区别很小，因此很容易上手并开发项目，但忽略细节会导致项目前期bug较多，因而开发出相对稳定的软件很慢，而开发出软件却很快。hibernate则正好与之相反。但是如果使用hibernate很熟练的话，实际上开发效率丝毫不差于甚至超越mybatis。 sql直接优化上，mybatis要比hibernate方便很多。由于mybatis的sql都是写在xml里，因此优化sql比hibernate方便很多。而hibernate的sql很多都是自动生成的，无法直接维护sql；虽有hql，但功能还是不及sql强大，见到报表等变态需求时，hql也歇菜，也就是说hql是有局限的；hibernate虽然也支持原生sql，但开发模式上却与orm不同，需要转换思维，因此使用上不是非常方便。总之写sql的灵活度上hibernate不及mybatis。]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础重点]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_base_sum%2F</url>
    <content type="text"><![CDATA[常见的 java 基础收集记录，好记性不如烂笔头。本文收录记录常见的Java基础知识。 String相关String，StringBuffer,StringBuilder 区别 String 为强不可变对象（每次操作返回创建的新对象，不改变原 String 对象）Java中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程； StringBuffer 是线程安全的； StringBuilder 是非线程安全； 运行速度快慢为：StringBuilder &gt; StringBuffer &gt; String； String 类的常用方法String 类的常用方法：length(),concat(String),startsWith(),endsWith(),indexOf(),lastIndexOf(),substring(),repalce(),matches(),replaceAll(),split(); new String(“xyz”)创建了几个对象首先得了解String作为最基础的引用数据类型，Java 设计者为 String 提供了字符串常量池以提高其性能（基于不可变的）。 JVM读到”xyz”的时候，首先去字符串常量池检查是否存在，如果不存在创建 “xyz” 字符串对象; JVM读到关键字 new 的时候，JVM会在堆中为其创建一个String对象； 综上，如果字符串常量池存在 “xyz” 常量则创建了1个对象；如果不存在，则创建2个对象。 字符串找数字给你一组字符串如：7i8hy4jjnb2.让你编程输出里面的数字：7842。123456String str = "iu7i8hy4jnb2";Pattern p = Pattern.compile( "\\d" );/*"\[0-9]\"*/Matcher matcher = p.matcher(str);while ( matcher.find() ) &#123; System.out.print(matcher.group());&#125; interface 和 abstractabstract class 和interface 是支持抽象类定义的两种机制，都不能实例化。 interface 实现类及 abstrctclass 的继承类(非 abstrctclass)子类都必须要实现已经声明的抽象方法。 interface 修饰类，abstrctclass 能修饰类，能修饰方法； interface 需要实现，要用 implements，而abstract class 需要继承，要用 extends； 一个类可以实现多个 interface，但一个类只能继承一个 abstract class； 尽管 interface 实现类及 abstrct class 的子类都必须要实现相应的抽象方法，但实现的形式不同。interface 中的每一个方法都是抽象方法，都只是声明的(declaration,没有方法体)，实现类必须要实现。而 abstractclass 的子类可以有选择地实现； interface 是完全抽象的，声明 pulic 的方法，不能声明 private 及 protected 的方法，不能定义方法体，也不能声明实例变量。然而，interface 却可以声明常量变量；… overload 和 overrideoverride（重写） 方法名、参数、返回值相同； 存在于父类和子类之间； 子类方法不能缩小父类方法的访问权限； 子类方法不能抛出比父类方法更多的异常(但子类方法可以不抛出异常)； 方法被定义为final不能被重写； overload（重载） 方法名相同，参数类型、个数、顺序至少有一个不相同； 不能重载只有返回值不同的方法名。 方法的修饰符可以不相同 存在于父类和子类、同类中。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之集合]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_collention%2F</url>
    <content type="text"><![CDATA[java.util包中就包含了一系列重要的集合类，而对于集合类，主要需要掌握的就是它的内部结构，以及遍历集合的迭代模式。Collection 是所有单一对象的集合类的根接口，没有提供直接的实现类；有三个子接口：List(有序可重复),set(无序不可重复),queue(队列)。 Map 是 Java.util 包中的另一个接口，它和 Collection 接口没有关系，是相互独立的，但是都属于集合类的一部分。Map 包含了 key-value（Entity） 对。Map 不能包含重复的 key，但是可以包含相同的 value。 Iterator 迭代器Iterator 是 Java 集合的顶层接口（不包括 map 系列的集合，Map 接口是 map 系列集合的顶层接口），用于遍历集合中元素的接口。 Iterator 和 Iterable 接口 Iterable(存在于 java.lang 包中),封装了 Iterator 接口。所以只要实现了只要实现了Iterable接口的类，就可以使用Iterator迭代器了; Iterator(存在于 java.util 包中),核心的方法 next(),hasnext(),remove() Iterator：只能正向遍历集合，适用于获取移除元素。ListIerator：继承Iterator，可以双向列表的遍历，同样支持元素的修改 1234567Iterator it = list.iterator();//通过迭代器遍历元素Iterator it = collection.iterator();while(it.hasNext()) &#123; // 得到下一个元素 Object obj = it.next();&#125; List 接口List 是有序，可以重复的集合。List 关注的是索引，拥有一系列和索引相关的方法，查询速度快。因为往 List 集合里插入或删除数据时，会伴随着后面数据的移动，所有插入删除数据速度慢。List 接口提供了特殊的迭代器，称为 ListIterator，除了允许 Iterator 接口提供的正常操作外，该迭代器还允许元素插入和替换，以及双向访问。还提供了一个方法来获取从列表中指定位置开始的列表迭代器。 List 接口的三个典型实现： ArrayList：底层数据结构是数组(允许所有元素，包括 null)，查询快，增删慢;线程不安全，效率高; LinkedList：底层数据结构是链表(允许所有元素，包括 null)，查询慢，增删快; 线程不安全，效率高（LinkedList 不但继承了 List 接口，还继承了 Deque（double-ended queue），所以既拥有List的特征（快速查询、迭代等），又有双端队列的特征（用作队列、栈等））;实现线程安全的一种解决方法：List list = Collections.synchronizedList(new LinkedList(...)); Vector：同 ArrayList，但它是线程安全的，几乎已经淘汰了这个集合(Vector同步了每个方法); Set 接口Set 是无序（LinkedHashSet、TreeSet是有序…），不可重复的集合。检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。 Set 接口的三个典型实现： HashSet：底层采用 哈希表算法，不保证元素的添加顺序，查询效率高；不可重复；线程不安全; LinkedHashSet：底层采用 链表 和 哈希表的算法。既保证了元素的添加顺序，也保证了查询效率。但是整体性能要低于 HashSet；线程不安全; TreeSet：底层使用 红黑树算法，擅长于范围查询；不保证元素的添加顺序，但是会对集合中的元素进行排序；线程不安全; Queue 接口方便的实现队列和栈及其相关的数据结构，实现类之一PriorityQueue优先队列。TODO:未接触 Map 接口Map 接口不是 Collection 接口的继承。而是从自己的用于维护键-值关联的接口层次结构入手。按定义，该接口描述了从不重复的键到值的映射。 Map 接口的五个典型实现： HashMap：是最常用的 Map，数据结构是数组+单链表的组合；它根据键的 HashCode 值存储数据，根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的。因为键对象不可以重复，所以 HashMap 最多只允许一条记录的键为 Null，允许多条记录的值为 Null，线程不安全；HashMap可以通过 Map m = Collections.synchronizedMap(hashMap)来达到同步的效果; Hashtable：与 HashMap 类似，是 HashMap 的线程安全版，它支持线程的同步，即任一时刻只有一个线程能写 Hashtable，因此也导致了 Hashtale 在写入时会比较慢，它继承自 Dictionary 类，不同的是它不允许记录的键或者值为null，同时效率较低。 ConcurrentHashMap：线程安全，并且锁分离。ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的hash table，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。 LinkedHashMap：保存了记录的插入顺序，在用 Iteraor 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的，在遍历的时候会比 HashMap 慢，有 HashMap 的全部特性； TreeMap：实现 SortMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序（自然顺序），也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。不允许 key 值为空，线程不安全； Map 遍历 KeySet()：将 Map 中所有的键存入到set集合中。因为set具备迭代器。所有可以迭代方式取出所有的键，再根据get方法。获取每一个键对应的值。 123456Iterator it = map.keySet().iterator(); //获取迭代器while(it.hasNext())&#123; Object key = it.next(); System.out.println(map.get(key));&#125; entrySet()：方法返回一个实现Map.Entry 接口的对象集合。如果底层 Map 在Map.Entry 接口的setValue() 方法外部被修改，此条目集就会变得无效，并导致迭代器行为未定义; 123456//将map集合中的映射关系取出，存入到set集合Iterator it = map.entrySet().iterator();while(it.hasNext())&#123; Entry e =(Entry) it.next(); System.out.println(e.getKey () + ":" + e.getValue());&#125; 推荐使用第二种方式，即entrySet()方法，效率较高。 其他hashCode() 与 equal()hashCode()方法和equal()方法的作用其实一样，但是重写的 equal() 里一般比较复杂，效率低;利用 hashCode() 进行对比，因为只要生成一个 hash 值进行比较就可以了，效率很高。但hashCode()并不是完全可靠，生成hash值得公式可能存在的问题。 所以一般比较用 hashCode() 方法，当 hashCode 一致时，再用 equal() 去再对比，如果再一致，则说明两个对象相等。 Comparable 与 ComparatorComparable &amp; Comparator 都是用来实现集合中元素的比较、排序的。只是 Comparable 是在集合内部定义的方法实现的排序，Comparator 是在集合外部实现的排序，所以，如想实现排序，就需要在集合外定义 Comparator 接口的方法或在集合内实现 Comparable 接口的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Compara学习 * @author deng */public class ComparaStu &#123; public static void main(String[] args) &#123; List&lt;Person&gt; persons = new ArrayList&lt;Person&gt;(); persons.add(new Person(1)); persons.add(new Person(5)); persons.add(new Person(2)); persons.add(new Person(4)); persons.add(new Person(3)); Collections.sort(persons); for (int i = 0; i &lt; persons.size(); i++) &#123; System.out.println(persons.get(i).getId()); &#125; &#125;&#125;/** * 实现Comparable,重写compareTo * @author deng * @param &lt;T&gt; */class Person implements Comparable&lt;Person&gt; &#123; private int id; public Person() &#123; super(); &#125; public Person(int id) &#123; super(); this.id = id; &#125; @Override public int compareTo(Person person) &#123; return this.getId() - person.getId(); &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Compara学习 * @author deng */public class ComparaStu &#123; public static void main(String[] args) &#123; List&lt;Car&gt; cars = new ArrayList&lt;Car&gt;(); cars.add(new Car("B1")); cars.add(new Car("A3")); cars.add(new Car("C1")); cars.add(new Car("A1")); cars.add(new Car("B2")); //没有实现Comparable重写compareTo方法不能直接排序，报错 //Collections.sort(cars); Collections.sort(cars,new Comparator&lt;Car&gt;()&#123; @Override public int compare(Car o1, Car o2) &#123; return o1.getNum().compareTo(o2.getNum()); &#125; &#125;); for (int i = 0; i &lt; cars.size(); i++) &#123; System.out.println(cars.get(i).getNum()); &#125; &#125;&#125;final class Car &#123; private String num; public Car() &#123; super(); &#125; public Car(String num) &#123; super(); this.num = num; &#125; public String getNum() &#123; return num; &#125; public void setNum(String num) &#123; this.num = num; &#125;&#125; 集合与数组 数组（可以存储基本数据类型）是用来存现对象的一种容器，但是数组的长度固定，不适合在对象数量未知的情况下使用； 集合（只能存储对象，对象类型可以不一样）的长度可变，可在多数情况下使用； 参考Java 集合框架 (runoob)JAVA集合类汇总]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之异常]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_throwable%2F</url>
    <content type="text"><![CDATA[异常是程序中的一些错误，但并不是所有的错误都是异常，并且错误有时候是可以避免的。本文为 Java 异常的学习整理。Throwable为异常类的最终父类，有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Throwable中常用方法有： getCause()：返回抛出异常的原因。如果 cause 不存在或未知，则返回 null； getMeage()：返回异常的消息信息； printStackTrace()：对象的堆栈跟踪输出至错误输出流，作为字段 System.err 的值； Error（错误）Error 是程序无法处理的错误，表示运行应用程序中较严重问题。 大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError 。 这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 Exception（异常）Exception 是程序本身可以处理的异常。异常可分为运行时异常跟编译异常。 运行时异常：即RuntimeException及其之类的异常。这类异常在代码编写的时候不会被编译器所检测出来，是可以不需要被捕获，但是程序员也可以根据需要进行捕获抛出。常见的RUNtimeException有：NullpointException（空指针异常），ClassCastException（类型转换异常），IndexOutOfBoundsException（数组越界异常）等; 编译异常：RuntimeException以外的异常。这类异常在编译时编译器会提示需要捕获，如果不进行捕获则编译错误。常见编译异常有：IOException（流传输异常），SQLException（数据库操作异常）等; finally 不一定被执行，例如 catch 块中有退出系统的语句 System.exit(-1); finally就不会被执行 参考Java 异常处理 (runoob)深入理解java异常处理机制]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础学习]]></title>
    <url>%2F2018%2F05%2F12%2Fjava_base%2F</url>
    <content type="text"><![CDATA[Java 是由Sun Microsystems公司于1995年5月推出的高级程序设计语言。Java可运行于多个平台，如Windows, Mac OS，及其他多种UNIX版本的系统。 JDK、JRE、JVM 区别JDK = JRE + JVM + 其它。 JDK(Java Development Kit) 是 Java 开发工具包，针对Java开发员。JDK 中包含 JRE，在 JDK 的安装目录下有一个名为 jre 的目录，里面有两个文件夹 bin 和 lib，在这里可以认为 bin 里的就是 jvm，lib 中则是 jvm 工作所需要的类库，而 jvm 和 lib 和起来就称为 jre。 JRE(Java Runtime Environment)是 Java 运行环境，针对 Java 程序的用户。JRE 中包含了 JVM，runtime class libraries 和 Java application launcher ，这些是运行Java程序的必要组件。 JVM(java virtual machine)是 Java 虚拟机，针对 Java 程序。jre包含lib类库，所有的 Java 程序会首先被编译为 .class 的类文件，这种类文件可以在虚拟机上执行。class 并不直接与机器的操作系统相对应，而是经过虚拟机间接与操作系统交互，由虚拟机将程序解释给本地系统执行。 学习参考Java 教程(runoob) Java 入门教程]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse设置打开文件目录]]></title>
    <url>%2F2018%2F05%2F04%2Feclipse_openMDR%2F</url>
    <content type="text"><![CDATA[在使用 eclispe 开发中打开选中文件，默认情况为：选中文件/文件夹/包名–右击–Properties–Location–复制路径–打开我的电脑–粘贴地址–回车。相比 myeclispe 中快捷按钮打开文件所在目录不方便了很多。下面介绍 eclipse 中设置 myeclispe 中那种效果： 找到eclipse工具栏中 External Tools: 新建拓展工具配置： 配置工具配置：Name：C:/WINDOWS/explorer.exeArguments：${container_loc} 勾选展示以及配置确认完成： 配置完成后，选中相应的文件或文件夹点击External Tools位置即可打开文件或文件夹所在目录。]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
      <tags>
        <tag>eclipse</tag>
        <tag>快捷设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[收集励志]]></title>
    <url>%2F2018%2F04%2F26%2Fcollected_chickenSoup%2F</url>
    <content type="text"><![CDATA[有时，情绪低落，你需要’鸡汤’。它可以是鸡汤，也可以不是鸡汤。君子务本,本立而道生。 《论语 学而》 做一件事儿需注意根本要素。 岁月已往者不可复，未来者不可期，见在者不可失。 《省心录》 过去的就让它过去，往事随风；未来是多变的，不能期待；现在的是你拥有的，活在当下。 我荒废的今日，正是昨日殒身之人祈求的明日。 佚名 以前网上传的哈佛图书馆名言之一，后面进一步查询，发现是网上的鸡汤文，不知出处。以前大三的时候有感，或许当时的我很适合这句，过着忙碌却又荒废的日子，如浮动的青萍，找不到落脚点。每天都应珍惜。 抱歉，应该把后面的尾巴（打赏以及版本脚注）去掉，没时间去研究改造优化hexo的next主题了，你就当没看见，空闲了看见了再改改~]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logback学习笔记]]></title>
    <url>%2F2018%2F04%2F19%2Flog_logback%2F</url>
    <content type="text"><![CDATA[logback 是一个开源的日志框架组件，与 log4j 出自同一个开发者(Ceki Gülcü)之手。logback 常被认为是用来替代 log4j 的，与 log4j 相比其拥有更多的特性，同时也带来很大性能提升。 模块概念模块组成Logback 主要由三个模块组成： logback-core，提供了 logback 的核心功能，是另外两个组件的基础。 logback-classic，实现了 slf4j 的 API，所以当想配合 Slf4j 使用时，需要将 logback-classic 加入 classpath。 logback-access，为集成 servlet 环境而准备的，可提供 HTTP-access 的日志接口，比如说 tomcat 或者 jettyjetty。 重要概念在 logback 里，最重要的三个类分别是： Logger，位于 logback-classic 模块中。日志记录器，把它关联到应用对应的context上后，主要用于存放日志对象，定义日志类型，级别。 Appender，位于 logback-core 模块中。指定日志输出的目的地，目的地可以是控制台，文件，或者数据库等 Layout，位于 logback-core 模块中。负责把事件转换成字符串，格式化日志信息的输出 配置基础配置方式logback 提供的配置方式有以下几种： 编程式配置 xml 格式 groovy 格式 配置优先级顺序 在 classpath 中寻找 logback.groovy 文件， 如果没找到，则在 classpath 寻找继续寻找 logback-test.xml 文件 如果没找到，继续寻找 logback.xml 文件 如果没找到, jdk6+ 会调用 ServiceLoader 查找 com.qos.logback.classic.spi.Configurator 接口的第一个实现类 如果仍然没找到，则使用默认配置（打印到控制台） 不使用配置情况下：12345678910111213private static final Logger logger = LoggerFactory.getLogger(LogbackMain.class);public static void main(String[] args) &#123; logger.trace("logback的trace"); logger.debug("logback的debug"); logger.info("logback的info"); logger.warn("logback的warn"); logger.error("logback的error"); //打印 Logback 内部状态 LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); StatusPrinter.print(lc); &#125; 输出结果:1234567814:43:48.275 [main] DEBUG cn.cq.nocoder.log.LogbackMain - logback的debug14:43:48.278 [main] INFO cn.cq.nocoder.log.LogbackMain - logback的info14:43:48.278 [main] WARN cn.cq.nocoder.log.LogbackMain - logback的warn14:43:48.278 [main] ERROR cn.cq.nocoder.log.LogbackMain - logback的error14:43:48,236 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]14:43:48,236 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]14:43:48,236 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.xml]14:43:48,240 |-INFO in ch.qos.logback.classic.BasicConfigurator@68a750a - Setting up default configuration. 配置详解根节点根节点也就是&lt;configuration&gt;，属性如下： scan：当此属性设置为 true 时，配置文件如果发生改变，将会被重新加载，默认值为 true。 scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当 scan 为 true 时，此属性生效。默认的时间间隔为1分钟。 debug：当此属性设置为 true 时，将打印出 logback 内部日志信息，实时查看 logback 运行状态。默认值为 false。 例如：123&lt;configuration scan="true" scanPeriod="20 second" debug="false"&gt; &lt;!-- 其他配置省略--&gt; &lt;/configuration&gt; 主子节点根节点 &lt;configuration&gt; 的子节点大概如下： appender： 负责写日志的组件。 logger： 设置某一个包或者具体的某一个类的日志打印级别、以及指定 &lt;appender&gt;。 root：根 logger。 因配置逻辑关系先介绍 logger 与 root，后介绍 appender。 logger 子节点用来设置某一个包或者具体的某一个类的日志打印级别、以及指定 &lt;appender&gt;。&lt;logger&gt; 仅有一个 name 属性，一个可选的 level 和一个可选的 additivity 属性。 name：用来指定受此 logger 约束的某一个包或者具体的某一个类。 level：用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF（定义类：ch.qos.logback.classic.Level）。还有一个特殊值 INHERITED 或者同义词 NULL，代表强制执行上级的级别。如果未设置此属性，那么当前 logger 将会继承上级的级别。 additivity：是否向上级 logger 传递打印信息。默认是 true。 &lt;logger&gt; 可以包含零个或多个 &lt;appender-ref&gt; 元素，标识这个 appender 将会添加到这个 logger。 日志打印级别logger 有日志打印级别，可以为一个 logger 指定它的日志打印级别。如果不为一个 logger 指定打印级别，那么它将继承离他最近的一个有指定打印级别的祖先的打印级别。如果 logger 先找它的父亲，而它的父亲没有指定打印级别，那么它会立即忽略它的父亲，往上继续寻找它爷爷，直到它找到 root logger。因此，也能看出来，要使用 logback， 必须为 root logger 指定日志打印级别。日志打印级别从低级到高级排序的顺序是：TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR。 获取 logger在 logback 中，每个 logger 都是一个单例，调用 LoggerFactory.getLogger 方法时，如果传入的 logger name 相同，获取到的 logger 都是同一个实例。在为 logger 命名时，用类的全限定类名作为 logger name 是最好的策略，这样能够追踪到每一条日志消息的来源。 root 子节点也是 &lt;logger&gt; 元素，但是它是根 logger。只有一个 level 属性，应为已经被命名为 ”root”. level：用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，不能设置为 INHERITED 或者同义词 NULL。默认是 DEBUG。 &lt;root&gt; 可以包含零个或多个 &lt;appender-ref&gt; 元素，标识这个 appender 将会添加到这个 logger。12345678910111213&lt;configuration&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!-- encoder 默认配置为PatternLayoutEncoder --&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- logback为java中的包 --&gt; &lt;logger name="cn.cq.deng.logback"/&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;/root&gt; &lt;/configuration&gt; 其中 appender 的配置表示打印到控制台 (稍后详细讲解 appender)。&lt;logger name=&quot;cn.cq.deng.logback&quot;/&gt; 将控制 logback 包下的所有类的日志的打印，但是并没有设置打印级别，所以继承他的上级 &lt;root&gt; 的日志级别 “DEBUG”。没有设置 additivity，默认为 true，将此 logger 的打印信息向上级传递。没有设置 appender，此 logger 本身不打印任何信息。&lt;root level=&quot;INFO&quot;&gt; 将 root 的打印级别设置为 “INFO” ，指定了名字为 “STDOUT” 的 appender。首先执行 &lt;logger name=&quot;cn.cq.deng.logback&quot;/&gt;，将级别为 “DEBUG” 及大于 “DEBUG” 的日志信息传递给 root，本身并不打印。root 接到下级传递的信息，交给已经配置好的名为 “STDOUT” 的 appender 处理， “STDOUT”appender 将信息打印到控制台。 appender 子节点&lt;appender&gt; 是 &lt;configuration&gt; 的子节点，是负责写日志的组件。&lt;appender&gt; 有两个必要属性name和class。name指定appender名称，class指定appender的全限定名。 ConsoleAppender把日志添加到控制台，有以下子节点： &lt;encoder&gt;：对日志进行格式化。（具体参数稍后讲解 ） &lt;target&gt;：字符串 System.out 或者 System.err ，默认 System.out. 例如：1234567891011&lt;configuration&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg %n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;/root&gt; &lt;/configuration&gt; FileAppender把日志添加到文件，有以下子节点: &lt;file&gt;：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 &lt;append&gt;：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。 &lt;encoder&gt;：对记录事件进行格式化。详见下文 &lt;prudent&gt;：如果是 true，日志会被安全的写入文件，即使其他的FileAppender也在向此文件做写入操作，效率低，默认是 false。 例如：12345678910111213&lt;configuration&gt; &lt;appender name="FILE" class="ch.qos.logback.core.FileAppender"&gt; &lt;file&gt;testFile.log&lt;/file&gt; &lt;append&gt;true&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt; &lt;/configuration&gt; RollingFIleAppender滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。有以下子节点： &lt;file&gt;：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 &lt;append&gt;：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。 &lt;encoder&gt;：对记录事件进行格式化。（具体参数稍后讲解 ） &lt;rollingPolicy&gt;:当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。 &lt;triggeringPolicy&gt;: 告知 RollingFileAppender 何时激活滚动。 &lt;prudent&gt;：当为true时，不支持 FixedWindowRollingPolicy。支持 TimeBasedRollingPolicy，但是有两个限制: 不支持也不允许文件压缩 不能设置file属性，必须留空 另外还有SocketAppender、SMTPAppender、DBAppender、SyslogAppender、SiftingAppender，并不常用，这些就不在这里讲解了，大家可以参考官方文档。当然大家可以编写自己的Appender。 rollingPolicyTimeBasedRollingPolicy最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责触发滚动。有以下子节点： &lt;fileNamePattern&gt;: 必要节点，包含文件名及“%d”转换符，%d”可以包含一个 Java.text.SimpleDateFormat 指定的时间格式，如：%d{yyyy-MM}。如果直接使用 %d，默认格式是 yyyy-MM-dd。RollingFileAppender 的file字节点可有可无，通过设置file，可以为活动文件和归档文件指定不同位置，当前日志总是记录到file指定的文件（活动文件），活动文件的名字不会改变；如果没设置file，活动文件的名字会根据fileNamePattern 的值，每隔一段时间改变一次。“/”或者“\”会被当做目录分隔符。 &lt;maxHistory&gt;: 可选节点，控制保留的归档文件的最大数量，超出数量就删除旧文件。假设设置每个月滚动，且&lt;maxHistory&gt;是6，则只保存最近6个月的文件，删除之前的旧文件。注意，删除旧文件是，那些为了归档而创建的目录也会被删除。 FixedWindowRollingPolicy根据固定窗口算法重命名文件的滚动策略。有以下子节点: &lt;minIndex&gt;:窗口索引最小值。 &lt;maxIndex&gt;:窗口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12。 &lt;fileNamePattern &gt;: 必须包含“%i”例如，假设最小值和最大值分别为1和2，命名模式为 mylog%i.log,会产生归档文件mylog1.log和mylog2.log。还可以指定文件压缩选项，例如，mylog%i.log.gz 或者 没有log%i.log.zip triggeringPolicySizeBasedTriggeringPolicy查看当前活动文件的大小，如果超过指定大小会告知 RollingFileAppender 触发当前活动文件滚动。只有一个节点: &lt;maxFileSize&gt;:这是活动文件的大小，默认值是10MB。 例如：每天生产一个日志文件，保存30天的日志文件12345678910111213141516&lt;configuration&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;logFile.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt; &lt;/configuration&gt; 又例如：按照固定窗口模式生成日志文件，当文件大于20MB时，生成新的日志文件。窗口大小是1到3，当保存了3个归档文件后，将覆盖最早的日志12345678910111213141516171819202122&lt;configuration&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;test.log&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt; &lt;fileNamePattern&gt;tests.%i.log.zip&lt;/fileNamePattern&gt; &lt;minIndex&gt;1&lt;/minIndex&gt; &lt;maxIndex&gt;3&lt;/maxIndex&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;5MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt; &lt;/configuration&gt; encoder负责两件事，一是把日志信息转换成字节数组，二是把字节数组写入到输出流。目前 PatternLayoutEncoder 是唯一有用的且默认的 encoder ，有一个 &lt;pattern&gt; 节点，用来设置日志的输入格式。使用“%”加“转换符”方式，如果要输出“%”，则必须用“\”对“\%”进行转义。例如：123&lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; 格式修饰符，与转换符共同使用：可选的格式修饰符位于“%”和转换符之间。第一个可选修饰符是左对齐 标志，符号是减号“-”；接着是可选的最小宽度 修饰符，用十进制数表示。如果字符小于最小宽度，则左填充或右填充，默认是左填充（即右对齐），填充符为空格。如果字符大于最小宽度，字符永远不会被截断。最大宽度 修饰符，符号是点号”.”后面加十进制数。如果字符大于最大宽度，则从前面截断。点符号“.”后面加减号“-”在加数字，表示从尾部截断。例如：%-4relative 表示，将输出从程序启动到创建日志记录的时间 进行左对齐 且最小宽度为4。 其他子节点上下文名称 &lt;contextName&gt;设置上下文名称 &lt;contextName&gt;：(默认上下文名称为“default”。可以使用设置成其他名字，用于区分不同应用程序的记录)1234&lt;configuration scan="true" scanPeriod="60 second" debug="false"&gt; &lt;contextName&gt;myAppName&lt;/contextName&gt; &lt;!-- 其他配置省略--&gt; &lt;/configuration&gt; 变量 &lt;property&gt;设置变量 &lt;property&gt;：(用来定义变量值的标签，&lt;property&gt; 有两个属性，name和value；其中name的值是变量的名称，value的值时变量定义的值。通过&lt;property&gt;定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量)12345&lt;configuration scan="true" scanPeriod="60 second" debug="false"&gt; &lt;property name="APP_Name" value="myAppName" /&gt; &lt;contextName&gt;$&#123;APP_Name&#125;&lt;/contextName&gt; &lt;!-- 其他配置省略--&gt; &lt;/configuration&gt; 时间戳字符串 &lt;timestamp&gt;获取时间戳字符串 &lt;timestamp&gt;:(两个属性 key:标识此 的名字；datePattern：设置将当前时间（解析配置文件的时间）转换为字符串的模式，遵循 Java.txt.SimpleDateFormat 的格式。)例如将解析配置文件的时间作为上下文名称：12345&lt;configuration scan="true" scanPeriod="60 second" debug="false"&gt; &lt;timestamp key="bySecond" datePattern="yyyyMMdd'T'HHmmss"/&gt; &lt;contextName&gt;$&#123;bySecond&#125;&lt;/contextName&gt; &lt;!-- 其他配置省略--&gt; &lt;/configuration&gt; 条件化处理配置文件logback 允许在配置文件中定义条件语句，以决定配置的不同行为。需引用：12345&lt;!-- 可用于在xml中写条件语句 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.codehaus.janino&lt;/groupId&gt; &lt;artifactId&gt;janino&lt;/artifactId&gt;&lt;/dependency&gt; 具体语法格式如下：123456789101112131415&lt;!-- if-then form --&gt; &lt;if condition="some conditional expression"&gt; &lt;then&gt; ... &lt;/then&gt; &lt;/if&gt; &lt;!-- if-then-else form --&gt; &lt;if condition="some conditional expression"&gt; &lt;then&gt; ... &lt;/then&gt; &lt;else&gt; ... &lt;/else&gt; &lt;/if&gt; 以下示例为操作系统判断：1234567891011121314151617181920212223242526272829&lt;!-- Windows操作系统 --&gt;&lt;if condition='property("os.name").contains("Windows")'&gt; &lt;then&gt; &lt;property name="LOG_FILE_ROOT" value="D://opt/applog/logbak-logs" /&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;%date&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;[%level][%thread][%logger.java:%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;/then&gt;&lt;/if&gt;&lt;!-- Linux操作系统 --&gt;&lt;if condition='property("os.name").contains("Linux")'&gt; &lt;then&gt; &lt;property name="LOG_FILE_ROOT" value="/opt/applog/logbak-logs" /&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%date&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;[%level][%thread][%logger.java:%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;/then&gt;&lt;/if&gt; 文件包含可以使用 ≶include&gt; 标签在一个配置文件中包含另外一个配置文件，如下所示:123456&lt;configuration&gt; &lt;include file="src/main/java/cn/cq/deng/includedConfig.xml"/&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="includedConsole" /&gt; &lt;/root&gt;&lt;/configuration&gt; 被包含的文件必须有以下格式:1234567&lt;included&gt; &lt;appender name="includedConsole" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;"%d - %m%n"&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt;&lt;/included&gt; 支持从多种源头包含: 从文件中包含,&lt;include file=&quot;src/main/java/chapters/configuration/includedConfig.xml&quot;/&gt; 从 classpath 中包含,&lt;include resource=&quot;includedConfig.xml&quot;/&gt; 从 URL 中包含, &lt;include url=&quot;http://some.host.com/includedConfig.xml&quot;/&gt; 如果包含不成功，那么 logback 会打印出一条警告信息，如果不希望 logback 抱怨，只需这样做：&lt;include optional=&quot;true&quot; ..../&gt; 完整配置示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!---scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true-scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。- 当scan为true时，此属性生效。默认的时间间隔为1分钟-debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。-- configuration 子节点为 appender、logger、root--&gt;&lt;configuration scan="true" scanPeriod="60 second" debug="false"&gt; &lt;!-- 负责写日志,控制台日志 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!-- 一是把日志信息转换成字节数组,二是把字节数组写入到输出流 --&gt; &lt;encoder&gt; &lt;Pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%5level] [%thread] %logger&#123;0&#125; %msg%n&lt;/Pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 文件日志 --&gt; &lt;appender name="DEBUG" class="ch.qos.logback.core.FileAppender"&gt; &lt;file&gt;debug.log&lt;/file&gt; &lt;!-- append: true,日志被追加到文件结尾; false,清空现存文件;默认是true --&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;!-- LevelFilter: 级别过滤器，根据日志级别进行过滤 --&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;Pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%5level] [%thread] %logger&#123;0&#125; %msg%n&lt;/Pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt; &lt;appender name="INFO" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;File&gt;info.log&lt;/File&gt; &lt;!-- ThresholdFilter:临界值过滤器，过滤掉 TRACE 和 DEBUG 级别的日志 --&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;Pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%5level] [%thread] %logger&#123;0&#125; %msg%n&lt;/Pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!-- 每天生成一个日志文件，保存30天的日志文件 - 如果隔一段时间没有输出日志，前面过期的日志不会被删除，只有再重新打印日志的时候，会触发删除过期日志的操作。 --&gt; &lt;fileNamePattern&gt;info.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;TimeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/TimeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;/appender &gt; &lt;!--&lt;!– 异常日志输出 –&gt;--&gt; &lt;!--&lt;appender name="EXCEPTION" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;--&gt; &lt;!--&lt;file&gt;exception.log&lt;/file&gt;--&gt; &lt;!--&lt;!– 求值过滤器，评估、鉴别日志是否符合指定条件. 需要额外的两个JAR包，commons-compiler.jar和janino.jar –&gt;--&gt; &lt;!--&lt;filter class="ch.qos.logback.core.filter.EvaluatorFilter"&gt;--&gt; &lt;!--&lt;!– 默认为 ch.qos.logback.classic.boolex.JaninoEventEvaluator –&gt;--&gt; &lt;!--&lt;evaluator&gt;--&gt; &lt;!--&lt;!– 过滤掉所有日志消息中不包含"Exception"字符串的日志 –&gt;--&gt; &lt;!--&lt;expression&gt;return message.contains("Exception");&lt;/expression&gt;--&gt; &lt;!--&lt;/evaluator&gt;--&gt; &lt;!--&lt;OnMatch&gt;ACCEPT&lt;/OnMatch&gt;--&gt; &lt;!--&lt;OnMismatch&gt;DENY&lt;/OnMismatch&gt;--&gt; &lt;!--&lt;/filter&gt;--&gt; &lt;!--&lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt;--&gt; &lt;!--&lt;!– 触发节点，按固定文件大小生成，超过5M，生成新的日志文件 –&gt;--&gt; &lt;!--&lt;maxFileSize&gt;5MB&lt;/maxFileSize&gt;--&gt; &lt;!--&lt;/triggeringPolicy&gt;--&gt; &lt;!--&lt;/appender&gt;--&gt; &lt;appender name="ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;error.log&lt;/file&gt; &lt;encoder&gt; &lt;Pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%5level] [%thread] %logger&#123;0&#125; %msg%n&lt;/Pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 按照固定窗口模式生成日志文件，当文件大于20MB时，生成新的日志文件。 - 窗口大小是1到3，当保存了3个归档文件后，将覆盖最早的日志。 - 可以指定文件压缩选项 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt; &lt;fileNamePattern&gt;error.%d&#123;yyyy-MM&#125;(%i).log.zip&lt;/fileNamePattern&gt; &lt;minIndex&gt;1&lt;/minIndex&gt; &lt;maxIndex&gt;3&lt;/maxIndex&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;!-- 异步输出 --&gt; &lt;appender name ="ASYNC" class= "ch.qos.logback.classic.AsyncAppender"&gt; &lt;!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold &gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为256 --&gt; &lt;queueSize&gt;512&lt;/queueSize&gt; &lt;!-- 添加附加的appender,最多只能添加一个 --&gt; &lt;appender-ref ref ="ERROR"/&gt; &lt;/appender&gt; &lt;!-- - 1.name：包名或类名，用来指定受此logger约束的某一个包或者具体的某一个类 - 2.未设置打印级别，所以继承他的上级&lt;root&gt;的日志级别“DEBUG” - 3.未设置additivity，默认为true，将此logger的打印信息向上级传递； - 4.未设置appender，此logger本身不打印任何信息，级别为“DEBUG”及大于“DEBUG”的日志信息传递给root， - root接到下级传递的信息，交给已经配置好的名为“STDOUT”的appender处理，“STDOUT”appender将信息打印到控制台； --&gt; &lt;logger name="ch.qos.logback" /&gt; &lt;!-- - 1.将级别为“INFO”及大于“INFO”的日志信息交给此logger指定的名为“STDOUT”的appender处理，在控制台中打出日志， - 不再向次logger的上级 &lt;logger name="logback"/&gt; 传递打印信息 - 2.level：设置打印级别（TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF），还有一个特殊值INHERITED或者同义词NULL，代表强制执行上级的级别。 - 如果未设置此属性，那么当前logger将会继承上级的级别。 - 3.additivity：为false，表示此logger的打印信息不再向上级传递,如果设置为true，会打印两次 - 4.appender-ref：指定了名字为"STDOUT"的appender。 --&gt; &lt;logger name="com.weizhi.common.LogMain" level="INFO" additivity="false"&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;!--&lt;appender-ref ref="DEBUG"/&gt;--&gt; &lt;!--&lt;appender-ref ref="EXCEPTION"/&gt;--&gt; &lt;!--&lt;appender-ref ref="INFO"/&gt;--&gt; &lt;!--&lt;appender-ref ref="ERROR"/&gt;--&gt; &lt;appender-ref ref="ASYNC"/&gt; &lt;/logger&gt; &lt;!-- - 根logger - level:设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，不能设置为INHERITED或者同义词NULL。 - 默认是DEBUG。 -appender-ref:可以包含零个或多个&lt;appender-ref&gt;元素，标识这个appender将会添加到这个logger --&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;!--&lt;appender-ref ref="DEBUG"/&gt;--&gt; &lt;!--&lt;appender-ref ref="EXCEPTION"/&gt;--&gt; &lt;!--&lt;appender-ref ref="INFO"/&gt;--&gt; &lt;appender-ref ref="ASYNC"/&gt; &lt;/root&gt;&lt;/configuration&gt; 资源获取 jar包，logback官网下载页链接 maven，pom文件配置如下(${logback-core.version}为版本号，如1.1.7)：12345678910111213141516&lt;!-- logbakc.xml jar --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;$&#123;logback-core.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-access&lt;/artifactId&gt; &lt;version&gt;$&#123;logback-access.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;$&#123;logback-classic.version&#125;&lt;/version&gt;&lt;/dependency&gt; 参考资料logback官网从零开始玩转logbacklogback教程logback 配置详解]]></content>
      <categories>
        <category>日志框架</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>logback</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客语法]]></title>
    <url>%2F2018%2F04%2F18%2Fblog_grammar%2F</url>
    <content type="text"><![CDATA[Hexo博客框架使用Markdown语法解析文章。本文主要介绍本网站博客的写作语法：markdown标记语言语法，hexo标签以及next主题的内置标签。 Markdown语法Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。 代码高亮采用三个反引号包裹代码。如下： 三个反引号(因解析问题，暂时如此写) js 代码 三个反引号 样例：123function()&#123; console.log('hello,code');&#125; 图片很明显地，要在纯文字应用中设计一个「自然」的语法来插入图片是有一定难度的。Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。 行内式的图片语法看起来像是：123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式的图片语法则长得像这样：1![Alt text][id] id」是图片参考的名称，图片参考的定义方式则和连结参考一样： 参考：Markdown 语法说明 (简体中文版) hexo框架标签插件hexo 是一个快速、简洁且高效的博客框架，本网站采用此框架搭建。下面为hexo博客框架的标签插件。 引用块在文章中插入引言，可包含作者、来源、链接以及标题。PS:注意空格123&#123;% blockquote [author[, source]] [link] [source_link_title] %&#125;引用内容&#123;% endblockquote %&#125; 简单样例：123&#123;% blockquote %&#125;没有提供参数，则只输出普通的 blockquote&#123;% endblockquote %&#125; 没有提供参数，则只输出普通的 blockquote 复杂样例：123&#123;% blockquote 搜索引擎, baidu https://www.baidu.com/ 百度搜索 %&#125;提供参数:作者, 来源 链接 链接名称&#123;% endblockquote %&#125; 提供参数:作者, 来源 链接 链接名称 搜索引擎, baidu百度搜索 代码块在文章中插入代码。码农enjoy~123&#123;% codeblock [title] [lang:language] [url] [link text] %&#125;code snippet&#123;% endcodeblock %&#125; 简单样例123&#123;% codeblock %&#125;alert(&apos;Hello World!&apos;);&#123;% endcodeblock %&#125; 1alert(&apos;Hello World!&apos;); 复杂样例：12345&#123;% codeblock alert lang:js http://www.runoob.com/jquery/jquery-tutorial.html jquery.js%&#125;$(document).ready(function()&#123; alert(&apos;Hello jquery!&apos;);&#125;);&#123;% endcodeblock %&#125; jquery学习jquery123$(document).ready(function()&#123; alert('Hello jquery!');&#125;); 反引号代码块另一种形式的代码块，不同的是它使用三个反引号来包裹。格式如下： 三个反引号(因解析问题，暂时如此写) [language] [title] [url] [link text] code snippet 三个反引号 没找到比markdown语法中代码高亮更好的写作体验，我还是采用markdown代码语法。 参考：hexo标签插件（Tag Plugins） next主题内置标签文本居中的引用此标签将生成一个带上下分割线的引用，同时引用内文本将自动居中。 文本居中时，多行文本若长度不等，视觉上会显得不对称，因此建议在引用单行文本的场景下使用。 例如作为文章开篇引用 或者 结束语之前的总结引用。 使用方式(此标签要求 NexT 的版本在 0.4.5 或以上。 若你正在使用的版本比较低，可以选择使用 HTML 方式)。 HTML方式：使用这种方式时，给 img 添加属性 class=”blockquote-center” 即可。 标签方式：使用 centerquote 或者 简写 cq。123456789&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;&lt;!-- 其中 class="blockquote-center" 是必须的 --&gt;&lt;blockquote class="blockquote-center"&gt;blah blah blah&lt;/blockquote&gt;&lt;!-- 标签 方式，要求版本在0.4.5或以上 --&gt;&#123;% centerquote %&#125;blah blah blah&#123;% endcenterquote %&#125;&lt;!-- 标签别名 --&gt;&#123;% cq %&#125; blah blah blah &#123;% endcq %&#125; blah blah blah nocoder 突破容器宽度限制的图片当使用此标签引用图片时，图片将自动扩大 26%，并突破文章容器的宽度。 此标签使用于需要突出显示的图片, 图片的扩大与容器的偏差从视觉上提升图片的吸引力。 此标签有两种调用方式（详细参看底下示例）。 使用方式(此标签要求 NexT 的版本在 0.4.5 或以上。 若你正在使用的版本比较低，可以选择使用 HTML 方式)。 HTML方式：使用这种方式时，为 img 添加属性 class=”full-image”即可。 标签方式：使用 fullimage 或者 简写 fi， 并传递图片地址、 alt 和 title 属性即可。 属性之间以逗号分隔。123456789&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;&lt;!-- 其中 class="full-image" 是必须的 --&gt;&lt;img src="/image-url" class="full-image" /&gt;&lt;!-- 标签 方式，要求版本在0.4.5或以上 --&gt;&#123;% fullimage /image-url, alt, title %&#125;&lt;!-- 别名 --&gt;&#123;% fi /image-url, alt, title %&#125; Bootstrap Callout使用方式1&#123;% note class_name %&#125; Content (md partial supported) &#123;% endnote %&#125; 其中，class_name 可以是以下列表中的一个值： default primary success info warning danger default(不需设置class_name) (md partial supported) primary (md partial supported) success (md partial supported) info (md partial supported) warning (md partial supported) danger (md partial supported) 参考：next主题内置标签]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle服务脚本]]></title>
    <url>%2F2018%2F03%2F21%2Fbat_oracle_script%2F</url>
    <content type="text"><![CDATA[安装了oracle数据库后，由于oracle很占系统资源，而安装后此数据库的很多服务都是默认开机启动的，所以正常情况下我们需要关闭服务，需要使用时打开服务。 Oracle服务介绍 Oracle ORCL VSS Writer Service：Oracle卷映射拷贝写入服务，VSS（Volume Shadow Copy Service）能够让存储基础设备（比如磁盘，阵列等）创建高保真的时间点映像，即映射拷贝（shadow copy）。它可以在多卷或者单个卷上创建映射拷贝，同时不会影响到系统的系统能。（非必须启动） OracleDBConsoleorcl：Oracle数据库控制台服务，orcl是Oracle的实例标识，默认的实例为orcl。在运行Enterprise Manager（企业管理器OEM）的时候，需要启动这个服务。（非必须启动） OracleJobSchedulerORCL：Oracle作业调度（定时器）服务，ORCL是Oracle实例标识。（非必须启动） OracleMTSRecoveryService：服务端控制。该服务允许数据库充当一个微软事务服务器MTS、COM/COM+对象和分布式环境下的事务的资源管理器。（非必须启动） OracleOraDb11g_home1ClrAgent：Oracle数据库.NET扩展服务的一部分。 （非必须启动） OracleOraDb11g_home1TNSListener：监听器服务，服务只有在数据库需要远程访问的时候才需要。（非必须启动，下面会有详细详解）。 OracleServiceORCL：数据库服务(数据库实例)，是Oracle核心服务该服务，是数据库启动的基础， 只有该服务启动，Oracle数据库才能正常启动。(必须启动) 解决方案在window下将代码贴入记事本中，保存为 *.bat 文件。 关闭服务12345@echo offnet stop OracleServiceORCLnet stop OracleOraDb11g_home1TNSListenernet stop OracleOraDb11g_home1ClrAgentnet stop OracleMTSRecoveryService 启动服务12345@echo offnet start OracleMTSRecoveryServicenet start OracleOraDb11g_home1ClrAgentnet start OracleOraDb11g_home1TNSListenernet start OracleServiceORCL]]></content>
      <categories>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除SVN版本控制目录]]></title>
    <url>%2F2018%2F03%2F21%2Fbat_SVN_delete%2F</url>
    <content type="text"><![CDATA[svn导出的文件包含svn元信息（版本控制目录），若导出后进行测试或者转移git等，手动删除相关信息是很麻烦的事儿。 解决方案在window下将代码贴入记事本中，保存为 *.bat 文件。在要删除svn信息的文件夹（项目）根目录中复制一份，执行脚本。等待出现结束code:completed，然后按任意键结束即可。123456789@echo on@rem 删除SVN版本控制目录@rem for /r . %%a in (.) do @if exist "%%a\.svn" @echo "%%a\.svn"@for /r . %%a in (.) do @if exist "%%a\.svn" rd /s /q "%%a\.svn"@echo completed@pause]]></content>
      <categories>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>SVN</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo,欢迎!]]></title>
    <url>%2F2018%2F03%2F20%2Fhexo_welcome%2F</url>
    <content type="text"><![CDATA[历时三个工作日的休闲时间，搭建完成个人博客网站，以下为搭建历程。具体步骤，请自行baidu,google等~那些大神写的详细的很… before申请github 账户，新建项目(项目必须要遵守格式：账户名.github.io 。不造为撒，但我的没有这样采用…网上以及官方教程都这样写，但是没人解释…) 2018-03-19申请域名，可以在 阿里云 申请配置域名解析 2018-03-20下载安装 node.js使用node.js的npm下载 hexo配置hexo环境,github关联 2018-03-21更换博客主题 themes初步调试以及部署 2018-03-22更换博客主题 next主题优化next主题进阶优化next主题个性化教程:打造炫酷网站 环境变量配置因电脑系统更换，导致个人电脑环境配置出错。因为有安装程序，执行命令时失败。出现类是情况添加环境变量即可（安装上述软件时会默认配置环境变量，故以下供系统重装而文件系统未损坏时时可采纳）： node.j 环境变量：path中添加你的nodejs安装路径，例如：D:\Program Files\nodejs\node-v8.9.3-win-x64; node_modules 环境变量：新建NODE_PATH系统变量，变量值为你的node.js的mode_modules路径：例如：D:\Program Files\nodejs\node-v8.9.3-win-x64\node_modules hexo 环境配置：path中添加你的hexo安装路径下的.bin，例如：G:\hexoWorkspace\node_modules\.bin; next本地搜索更换域名：https://www.jianshu.com/p/fd3accaa2ae0 渲染MathJax数学公式https://www.jianshu.com/p/7ab21c7f0674https://www.jianshu.com/p/a0aa94ef8ab2 参考使用Hexo+Github一步步搭建属于自己的博客（基础）]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java序列化轶事]]></title>
    <url>%2F2018%2F02%2F23%2Fjava_serializable%2F</url>
    <content type="text"><![CDATA[Java 对象序列化是 JDK 1.1 中引入的一组开创性特性之一，用于作为一种将 Java 对象的状态转换为字节数组，以便存储或传输的机制，以后仍可以将字节数组转换回 Java 对象原有的状态。实际上，序列化的思想是“冻结”对象状态，传输对象状态（写到磁盘、通过网络传输等等），然后“解冻”状态，重新获得可用的 Java 对象。这要归功于 ObjectInputStream/ObjectOutputStream 类、完全保真的元数据以及程序员愿意Serializable标识接口标记他们的类，从而“参与”这个过程。 下面为简单的序列化示例： 将对象序列化后，很容易将对象状态写到磁盘，然后重新读出它，如下： 到现在为止，还没有看到什么新鲜的或令人兴奋的事情。但可以以此来探讨 Java 序列化大多数程序员不知道的的事儿。 序列化允许重构序列化允许一定数量的类变种，甚至重构之后也是如此，ObjectInputStream 仍可以很好地将其读出来。 Java Object Serialization 规范可以自动管理的关键任务是： 将新字段添加到类中 将字段从 static 改为非 static 将字段从 transient 改为非 transient 取决于所需的向后兼容程度，转换字段形式（从非 static 转换为 static 或从非 transient 转换为 transient ）或者删除字段需要额外的消息传递。 重构序列化类：序列化使用一个 hash，该 hash 是根据给定源文件中几乎所有东西：方法名称、字段名称、字段类型、访问修改方法等计算出来的，序列化将该 hash 值与序列化流中的 hash 值相比较。 为了使 Java 运行时相信两种类型实际上是一样的，第二版和随后版本的对象必须与第一版有相同的序列化版本 hash（存储为private static final serialVersionUID字段）。因此我们需要 serialVersionUID 字段，它是通过对原始（或V1）版本的类运行 JDK serialver 命令计算出的。 序列化并不安全序列化二进制格式完全编写在文档中，并且完全可逆。实际上，只需将二进制序列化流的内容转储到控制台，就足以看清类是什么样子，以及它包含什么内容。这对于安全性有着不良影响。例如，当通过RMI进行远程方法调用时，通过连接发送的对象中的任何 private 字段几乎都是以明文的方式出现在套接字流中，这显然容易招致哪怕最简单的安全问题。 幸运的是，序列化允许 “hook” 序列化过程，并在序列化之前和反序列化之后保护（或模糊化）字段数据。可以通过在 Serializable 对象上提供一个 writeObject 方法来做到这一点。模糊化序列化数据（可以自行更改算法，示例仅供参考）： 如果需要查看被模糊化的数据，总是可以查看序列化数据流/文件。而且，由于该格式被完全文档化，即使不能访问类本身，也仍可以读取序列化流中的内容。 序列化的数据可以被签名和密封上一个技巧假设您想模糊化序列化数据，而不是对其加密或者确保它不被修改。当然，通过使用 writeObject 和 readObject 可以实现密码加密和签名管理，但其实还有更好的方式。 如果需要对整个对象进行加密和签名，最简单的是将它放在一个 javax.crypto.SealedObject 或 java.security.SignedObject 包装器中。两者都是可序列化的，所以将对象包装在 SealedObject 中可以围绕原对象创建一种“包装盒”。必须有对称密钥才能解密，而且密钥必须单独管理。同样，也可以将 SignedObject 用于数据验证，并且对称密钥也必须单独管理。结合使用这两种对象，便可以轻松地对序列化数据进行密封和签名，而不必强调关于数字签名验证或加密的细节。 序列化允许将代理放在流中很多情况下，类中包含一个核心数据元素，通过它可以派生或找到类中的其他字段。在此情况下，没有必要序列化整个对象。可以将字段标记为 transient，但是每当有方法访问一个字段时，类仍然必须显式地产生代码来检查它是否被初始化。 如果首要问题是序列化，那么最好指定一个或代理放在流中。为原始类提供一个 writeReplace 方法，可以序列化不同类型的对象来代替它。类似地，如果反序列化期间发现一个 readResolve 方法，那么将调用该方法，将替代对象提供给调用者。 打包和解包代理：writeReplace和readResolve 方法使类可以将它的所有数据（或其中的核心数据）打包到一个 PersonProxy 中，将它放入到一个流中，然后在反序列化时再进行解包。 信任，但要验证认为序列化流中的数据总是与最初写到流中的数据一致，这没有问题。但是，信任，但要验证。对于序列化的对象，这意味着验证字段，以确保在反序列化之后它们仍具有正确的值，“以防万一”。为此，可以实现 ObjectInputValidatio 接口，并覆盖 validateObject() 方法。如果调用该方法时发现某处有错误，则抛出一个 InvalidObjectException。 参考深入分析Java的序列化与反序列化 Java Serialization（Java 序列化）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven profile多资源分离打包方案]]></title>
    <url>%2F2017%2F11%2F17%2Fmaven_profile%2F</url>
    <content type="text"><![CDATA[一个项目通常要在多个环境部署，如开发环境、测试环境、生产环境。每种环境都有各自的配置参数，比如：数据库连接、远程调用的 ws 地址等等。如果每个环境都需要人工处理这些配置文件，这显然麻烦且易错。而 maven 提供了一种方便的解决这种问题的方案，就是 profile 功能profile 可以让我们定义一系列的配置信息，然后指定其激活条件。这样我们就可以定义多个 profile，然后每个 profile 对应不同的激活条件和配置信息，从而达到不同环境使用不同配置信息的效果。profile 定义的位置： 针对于特定项目的 profile 配置我们可以定义在该项目的 pom.xml 中。 针对于特定用户的 profile 配置，我们可以在用户的 settings.xml 文件中定义 profile 。该文件一般在用户家目录下的 “.m2” 目录下。 全局的 profile 配置。全局的 profile 是定义在 Maven 安装目录下的 “conf/settings.xml” 文件中的。 由于本人所在项目配置文件数目较多（43个资源配置文件），且有些部分资源配置文件开发、测试、生产内容一致，有部分则需区分。(单个资源文件分离较为简单，且网上实施方案较多，本次以所在普通web项目转换为Maven项目遇到问题的实际的解决方案为例)。 实施方案配置资源文件目录在根目录下，创建路径 src/main/profiles 目录(也可以取其他名，与pom文件配置中对应)，该目录下创建 dev、pro 目录（还可以添加测试、本地等目录，但要与 pom.xml 中 profile 配置的目录名相同）。将整理好的开发、生产配置资源文件分别放入对应目录中，maven项目原有 src\main\resources 目录下放置通用的配置资源文件。 项目pom文件配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&lt;properties&gt; &lt;!-- 配置资源相对路径--&gt; &lt;profiles.dir&gt;src/main/profiles&lt;/profiles.dir&gt;&lt;/properties&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;!-- 配置资源相对路径下开发目录 --&gt; &lt;profile.dir&gt;$&#123;profiles.dir&#125;/dev&lt;/profile.dir&gt; &lt;profile.xml&gt;$&#123;profiles.dir&#125;/devXml&lt;/profile.xml&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;pro&lt;/id&gt; &lt;properties&gt; &lt;!-- 配置资源相对路径下生产目录 --&gt; &lt;profile.dir&gt;$&#123;profiles.dir&#125;/pro&lt;/profile.dir&gt; &lt;profile.xml&gt;$&#123;profiles.dir&#125;/devXml&lt;/profile.xml&gt; &lt;/properties&gt;&lt;/profile&gt;&lt;/profiles&gt;&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;!-- 打包时包含resources文件 --&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;!-- 开启过滤,（对配置资源路径dev\pro等生效） --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;directory&gt;$&#123;profile.dir&#125;&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 数据库源配置信息打包 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;configuration&gt; &lt;webResources&gt; &lt;resource&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;directory&gt;$&#123;profile.xml&#125;&lt;/directory&gt; &lt;targetPath&gt;WEB-INF&lt;/targetPath&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.md&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/webResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包时候跳过测试 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;build&gt; 打包操作确认以上无误后进行打包操作，打包命令（根据打包类型在命令后跟上配置的profile的id，如下打包生产配置）：mvn clean install -P pro 可在用户或全局 setting.xml 文件中配置：1234&lt;!--在settings.xml中使用activeProfiles来指定需要激活的profile，这种方式激活的profile将所有情况下都处于激活状态 --&gt;&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt; &lt;/activeProfiles&gt; 这样可以在 Eclipse 下直接运行 run as-&gt;maven install 打包成日常测试或开发包。]]></content>
      <categories>
        <category>MAVEN</category>
      </categories>
      <tags>
        <tag>MAVEN</tag>
        <tag>profile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析HTTP]]></title>
    <url>%2F2016%2F12%2F12%2Fhttp_study%2F</url>
    <content type="text"><![CDATA[作为WEB网站开发人员，理解 HTTP 协议不可少。HTTP(HyperText Transfer Protocol：超文本传输协议)是互联网上应用最为广泛的一种网络协议。 HTTP 之初识篇HTTP 简介 HTTP 是一个基于 TCP/IP 通信协议来传递数据； HTTP 是一个客户端和服务器端请求和应答的标准（ TCP ）； HTTP 是一个属于应用层的面向对象的协议； HTTP 是一个无状态的协议； HTTP 协议通常承载于 TCP 协议之上，有时也承载于 TLS 或 SSL 协议层之上，这个时候，就成了我们常说的 HTTPS HTTP 流程一次 HTTP 操作称为一个事务，基于 HTTP 协议的客户/服务器模式的信息交换过程，它分四个过程：建立连接、发送请求信息、发送响应信息、关闭连接。 首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP 的工作开始。 建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是 MIME 信息包括请求修饰符、客户机信息和可能的内容。 服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是 MIME 信息包括服务器信息、实体信息和可能的内容。 客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由 HTTP 自己完成的，用户只要用鼠标点击，等待信息显示就可以了 HTTP 之请求篇HTTP请求由三部分组成，分别是：请求行、消息报头、请求正文。 请求行请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本，格式如:Method Request-URI HTTP-Version CRLF其中 Method表示请求方法；Request-URI是一个统一资源标识符；HTTP-Version表示请求的HTTP协议版本；CRLF表示回车和换行（除了作为结尾的CRLF外，不允许出现单独的CR或LF字符）。 请求方法请求方法有多种，各个方法的解释如下： GET 请求获取Request-URI所标识的资源 POST 在Request-URI所标识的资源后附加新的数据 HEAD 请求获取由Request-URI所标识的资源的响应消息报头 PUT 请求服务器存储一个资源，并用Request-URI作为其标识 DELETE 请求服务器删除Request-URI所标识的资源 TRACE 请求服务器回送收到的请求信息，主要用于测试或诊断 CONNECT 保留将来使用 OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求 消息报头详见后文。 请求正文一般为请求表单或者其他类容，例如GET请求： HTTP之响应篇在接收和解释请求消息后，服务器返回一个HTTP响应消息。HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文。 状态行状态行格式如下：HTTP-Version Status-Code Reason-Phrase CRLF其中，HTTP-Version 表示服务器 HTTP 协议的版本；Status-Code 表示服务器发回的响应状态代码；Reason-Phrase 表示状态代码的文本描述。 状态码状态码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值： 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 常见状态码、状态描述以及说明： 200 OK //客户端请求成功 400 Bad Request //客户端请求有语法错误，不能被服务器所理解 401 Unauthorized //请求未经授权，这个状态代码必须和 WWW-Authenticate 报头域一起使用 403 Forbidden //服务器收到请求，但是拒绝提供服务 404 Not Found //请求资源不存在 500 Internal Server Error //服务器发生不可预期的错误 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常 消息报头详见后文。 响应正文服务器根据请求给出的实际响应结果，一般被客户端浏览器所读取展示，例如网页： HTTP之报头篇何为消息报头HTTP 消息由客户端到服务器的请求消息和服务器到客户端的响应消息组成。这两种类型的消息由一个开始行（对于请求消息，开始行就是请求行，对于响应消息，开始行就是状态行），一个或者多个消息报头（可选），一个指示报头结束的空行和消息正文（可选）组成。HTTP 消息报头包括普通报头、请求报头、响应报头、实体报头，每一个报头域都是由名字+“：”+空格+值 组成，消息报头域的名字是大小写无关的，头域可以被扩展为多行，在每行开始处，使用至少一个空格或制表符。 通用报头通用报头包含请求和响应消息都支持的报头，通用报头包含 Cache-Control、Connection、Date、Pragma、Transfer-Encoding 等。简单描述两个例子（详细了解请自行搜索）： Cache-Control 报头：指定请求和响应遵循的缓存机制。在请求消息或响应消息中设置 Cache-Control 并不会修改另一个消息处理过程中的缓存处理过程。请求时的缓存指令包括 no-cache、no-store、max-age、max-stale、min-fresh、only-if-cached，响应消息中的指令包括 public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age。 Date 报头：表示消息发送的时间，时间的描述格式由rfc822定义。例如，Date:Mon,31Dec200104:25:57GMT。Date描述的时间表示世界标准时，换算成本地时间，需要知道用户所在的时区。 请求报头请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息。 Host报头：指定请求资源的Intenet主机和端口号，必须表示请求url的原始服务器或网关的位置。HTTP/1.1请求必须包含主机信息，否则系统会以400状态码返回。 Accept报头：用于指定客户端接受哪些类型的信息。例如：Accept：image/gif，表明客户端希望接受GIF图象格式的资源；Accept：text/html，表明客户端希望接受html文本。 User-Agent报头：允许客户端将它的操作系统、浏览器和其它属性告诉服务器。 Referer报头：允许客户端指定请求uri的源资源地址，这可以允许服务器生成回退链表，可用来登陆、优化cache等。他也允许废除的或错误的连接由于维护的目的被追踪。如果请求的uri没有自己的uri地址，Referer不能被发送。如果指定的是部分uri地址，则此地址应该是一个相对地址。 Accept-Encoding报头：类似于Accept，但是它是用于指定可接受的内容编码。eg：Accept-Encoding:gzip.deflate。如果请求消息中没有设置这个报头，服务器假定客户端对各种内容编码都可以接受。 Accept-Charset报头：用于指定客户端接受的字符集。例如：Accept-Charset:zh-CN,zh.如果在请求消息中没有设置这个报头，缺省是任何字符集都可以接受。 响应报头响应报头允许服务器传递不能放在状态行中的附加响应信息，以及关于服务器的信息和对Request-URI所标识的资源进行下一步访问的信息。 Server报头：包含处理请求的原始服务器的软件信息。此域能包含多个产品标识和注释，产品标识一般按照重要性排序。 实体报头请求消息和响应消息都可以包含实体信息，实体信息一般由实体头域和实体组成。实体头域包含关于实体的原信息，实体头包括Allow、Content-Base、Content-Encoding、Content-Language、Content-Length、Content-Location、Content-MD5、Content-Range、Content-Type、Etag、Expires、Last-Modified等。 Content-Type报头：用于指明发送给接收者的实体正文的媒体类型。 Content-Length报头：用于指明实体正文的长度，以字节方式存储的十进制数字来表示。 Last-Modified报头：用于指示资源的最后修改日期和时间]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat下OutOfMemoryError问题解决]]></title>
    <url>%2F2015%2F08%2F09%2Ftomcat_OutOfMemoryError%2F</url>
    <content type="text"><![CDATA[在eclipse运行tomcat服务器，经常遇到反应慢，时不时出现 OutOfMemoryError: PermGen space 问题。简单来说，项目运行耗费内存资源，内存溢出了。 问题原因当 JVM 需要加载一个新类的定义的却发现在 PermGen 没有足够的空间时，”java.lang.OutOfMemoryError: PermGen Space” 错误便发生了。有两个可能原因会导致内存溢出： 已有的内存空间不足以容纳所有的类； 内存泄漏，GC(Garbage Collection) 不会在主程序运行期对 PermGen space 进行清理，所以如果你的项目会加载很多类的话,就很可能出现 PermGen space 错误。 解决方案通过配置Tomcat的配置文件 catalina.bat（在 Tomcat 安装路径下bin目录，Windows下为catalina.bat而Linux下为catalina.sh文件)可以解决这种问题。 在文件中echo Using CATALINA_BASE: &quot;%CATALINA_BASE%&quot;前加入：1set JAVA_OPTS=%JAVA_OPTS%-server -Xms256m -Xmx512m -XX:PermSize=128m -XX:MaxPermSize=256m 参数的含义 -vmargs -Xms128M -Xmx512M -XX:PermSize=64M -XX:MaxPermSize=128M -vmargs 说明后面是VM的参数，所以后面的其实都是JVM的参数了 -Xms128m JVM初始分配的堆内存 -Xmx512m JVM最大允许分配的堆内存，按需分配 -XX:PermSize=64M JVM初始分配的非堆内存 -XX:MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配 在重启你的Tomcat服务器之后，这些配置的更改才会有效。 拓展学习下面解释下堆(Heap)和非堆(Non-heap)内存代表的含义。按照官方的说法： “Java 虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在 Java 虚拟机启动时创建的。” “在JVM中堆之外的内存称为非堆内存(Non-heap memory)”。 可以看出 JVM 主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给自己用的，所以方法区、JVM内部处理或优化所需的内存(如JIT编译后的代码缓存)、每个类结构(如运行时常数池、字段和方法数据)以及方法和构造方法的代码都在非堆内存中。 堆内存分配JVM 初始分配的堆内存由-Xms指定，默认是物理内存的1/64；JVM 最大分配的堆内存由 -Xmx 指定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM 就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM 会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx 相等以避免在每次GC 后调整堆的大小。说明：如果 -Xmx 不指定或者指定偏小，应用可能会导致 java.lang.OutOfMemory 错误，此错误来自 JVM，不是 Throwable 的，无法用 try…catch 捕捉。 非堆内存分配JVM 使用 -XX:PermSize 设置非堆内存初始值，默认是物理内存的1/64；由 XX:MaxPermSize 设置最大非堆内存的大小，默认是物理内存的1/4。 上面错误信息中的 PermGen space 的全称是 Permanent Generation space ，是指内存的永久保存区域。还没有弄明白 PermGen space 是属于堆内存，还是属于非堆内存。但 XX:MaxPermSize 设置过小会导致 java.lang.OutOfMemoryError: PermGen space 就是内存益出。 JVM内存限制(最大值)JVM内存限制于实际的最大物理内存，所以设置JVM参数导致程序要注意： 参数中 -Xms 的值不能大于 -Xmx ，或者-XX:PermSize的值不能大于-XX:MaxPermSize； -Xmx 的值和 -XX:MaxPermSize 的总和超过了JVM内存的最大限制；比如当前操作系统最大内存限制，或者实际的物理内存等等。（说到实际物理内存这里需要说明一点的是，如果你的内存是1024MB，但实际系统中并没有这么多，因为有一部分被硬件占用了。）]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
</search>
